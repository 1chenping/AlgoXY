\ifx\wholebook\relax \else
% ------------------------

\documentclass[UTF8]{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
\input{../../common-zh-cn.tex}

\setcounter{page}{1}

\begin{document}

%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{分而治之，快速排序和归并排序}

\author{刘新宇
\thanks{{\bfseries 刘新宇 } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\maketitle
\fi

\markboth{快速排序和归并排序}{初等算法}

\ifx\wholebook\relax
\chapter{分而治之，快速排序和归并排序}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{简介}
\label{introduction}

人们已经证明，基于比较的排序算法的最佳性能为$O(n \lg n)$\cite{TAOCP}。本章中，我们将要介绍两种分而治之的排序算法。它们的性能都可达到$O(n \lg n)$。一种是快速排序，是最常用的排序算法。快速排序被广泛研究，很多编程环境的标准库中，排序工具都是基于快速排序算法的。

在本章中，我们首先介绍快速排序的基本思想，它是一种典型的分而治之策略。我们会解释若干变形形式，并分析在一些特殊情况下，快速排序为什么无法均衡地分割序列，因而表现不佳。

为了解决不均衡分割的问题，我们接着会介绍归并排序，它能保证在任何情况下序列都被均分。我们还会介绍归并排序的若干变形形式，包括自然归并排序，和自底向上的归并排序。

我们会给出所有算法的命令式实现和函数式实现。

% ================================================================
% Quick sort
% ================================================================
\section{快速排序}
\index{快速排序}

考虑幼儿园的老师安排小朋友们按照身高站成一队。最矮的小朋友站在最左侧，最高的小朋友站在最右侧。老师要如何给出指示，使得小朋友们能自己站好呢?

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/kids-inline.eps}
 \caption{安排小朋友们站成一队。}
 \label{fig:knuth-ssort}
\end{figure}

有很多方法可以做到，其中就包括快速排序的方法：

\begin{enumerate}
  \item 第一个小朋友举起手。所有比这个小朋友矮的都站到他的左侧去；所有比他高的站到他的右侧去；
  \item 所有站到左侧的小朋友重复这一步骤；所有站到右侧的小朋友也重复这一步骤。
\end{enumerate}

假设一组小朋友的身高为（单位是厘米）：$\{102, 100, 98, 95, 96, 99, 101, 97\}$。下表描述了它们按照上述方法站队的过程。

\begin{tabular}{ | c c c c c c c c |}
\hline
\underline{102} & 100 & 98 & 95 & 96 & 99 & 101 & 97 \\
\underline{100} & 98 & 95 & 96 & 99 & 101 & 97 & `102' \\
\underline{98} & 95 & 96 & 99 & 97 & `100' & 101 & `102' \\
\underline{95} & 96 & 97 & `98' & 99 & `100' & `101' & `102' \\
`95' & \underline{96} & 97 & `98' & `99' & `100' & `101' & `102' \\
`95' & `96' & 97 & `98' & `99' & `100' & `101' & `102' \\
`95' & `96' & `97' & `98' & `99' & `100' & `101' & `102' \\
\hline
\end{tabular}

最开始的时候，身高为102厘米的第一个小朋友举手。我们称这个小朋友为pivot，并用下划线标记他。恰巧这个小朋友的身高是最高的。因此所有其他人都站到他的左侧，如表中第二行所示。此时，身高为102里面的小朋友站到了最终应站的位置，所以我们用引号把他括起来。接下来身高为100里面的小朋友举手，因此，身高为98、95、96、和99里面的小朋友站到了他的左侧，而只有一名身高为101厘米的小朋友身高比pivot高，所以他站到了右侧。表中的第三行给出了此时的状态。然后，身高为98厘米的小朋友成为了左侧的pivot；而身高为101厘米的小朋友成为了右侧的pivot。但是身高101厘米为pivot的那组小朋友只有他一个人，因此无需继续排序了。他站立的位置就是最终的位置。我们重复同样的方法，指导所有人都站到最终的位置。

\subsection{基本形式}
\index{快速排序!基本形式}

将上述步骤归纳可以得到快速排序的递归描述。对序列$L$进行排序时：

\begin{itemize}
\item 若$L$为空，则排序结果明显为空。这是边界情况；
\item 否则，在$L$中任选一个元素作为pivot，然后递归地将$L$中不大于pivot的元素排序，将结果置于pivot的左侧，\underline{同时}递归地将所有大于pivot的元素排序，将结果置于pivot的右侧。
\end{itemize}

这里我们强调了“同时”，而不是“然后”。也就是说，左右两侧的递归排序是可以同时并行进行的。我们后面会再次讨论有关并行的内容。

快速排序由C. A. R. Hoare在1960年提出\cite{TAOCP}、\cite{wiki-qs}。这里给出的描述是最基本的一种。它并没有明确解释如何选择pivot。我们稍后会看到pivot的选取会直接影响到排序的性能。

最简单的方法是总选择第一个元素作为pivot。这样就可以将快速排序形式化为下面的公式：

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  sort(\{ x | x \in L', x \leq l_1 \}) \cup \{ l_1 \} \cup sort(\{ x | x \in L', l_1 < x \}) & otherwise \\
  \end{array}
\right.
\ee

其中$l_1$是非空序列$L$中的第一个元素，而$L'$包含除$l_1$外的剩余部分$\{l_2, l_3, ...\}$。这里我们使用了Zermelo Frankel表达式（简称为ZF表达式）\footnote{以纪念对现代集合论贡献巨大的两位数学家。中文译作：策梅罗、弗兰克尔。}，也称为list comprehension。一个ZF表达式$\{ a | a \in S, p_1(a), p_2(a), ... \}$表示从集合$S$中选取使得断言$p_1, p_2, ...$都为真的元素。ZF表达式原本用于表示\underline{集合}，我们将其扩展以简短地表示列表。因此允许存在重复的元素，并且不同的排列代表不同的列表。详细信息请参考本书的附录。

在支持list comprehension的编程环境中，上述公式可以直接翻译为代码。如下面的Haskell例子程序：

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort [y | y<-xs, y <= x] ++ [x] ++ sort [y | y<-xs, x < y]
\end{lstlisting}

迄今为止，这可能是最短的快速排序程序。即使引入一些中间变量，程序也仍然简洁：

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = as ++ [x] ++ bs where
    as = sort [ a | a <- xs, a <= x]
    bs = sort [ b | b <- xs, x < b]
\end{lstlisting}

这一基本的快速排序程序还有一些变形，例如明确使用filter，而不是list comprehension。如下面的Python例子所示：

\lstset{language=Python}
\begin{lstlisting}
def sort(xs):
    if xs == []:
        return []
    pivot = xs[0]
    as = sort(filter(lambda x : x <= pivot, xs[1:]))
    bs = sort(filter(lambda x : pivot < x, xs[1:]))
    return as + [pivot] + bs
\end{lstlisting}

\subsection{Strict weak ordering}
\index{快速排序!Strict weak ordering}

我们假设元素按照单调非递减的顺序排序。我们也可以改变算法，按照其他条件排序。这样就可以适用更多场景，在实际中，待排序的元素可能是数字、字符串、或者其他更复杂的内容（例如对一组列表排序）。

典型的方法，是把比较条件抽象成一个参数，如同此前在插入排序和选择排序的章节中所描述的。我们并不要求比较条件一定要遵从total ordering，但是至少要满足\underline{strict weak ordering}\cite{wiki-total-order}、\cite{wiki-sweak-order}。

简单起见，我们仅仅考虑使用小于等于（不大于）作为比较条件来进行排序。

\subsection{划分（Partition）}
\index{快速排序!partition}
观察前面的基本快速排序算法，会发现遍历了两次：第一次遍历获得了所有不大于pivot的元素，第二次遍历获得了所有大于pivot的元素。我们可以将他们合并成只遍历一次的划分过程。定义如下：

\be
partition(p, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, \Phi) & L = \Phi \\
  (\{ l_1 \} \cup A, B) & p(l_1), (A, B) = partition(p, L') \\
  (A, \{ l_1 \} \cup B) & \lnot p(l_1)
  \end{array}
\right.
\ee

这里的$\{x\} \cup L$仅仅是一个“cons”操作（将元素链接到表头），它只需要常数时间。使用partition，快速排序可以定义为：

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  sort(A) \cup \{l_1\} \cup sort(B) & otherwise, (A, B) = partition(\lambda_x x \leq l_1, L')
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort as ++ [x] ++ sort bs where
    (as, bs) = partition (<= x) xs

partition _ [] = ([], [])
partition p (x:xs) = let (as, bs) = partition p xs in
    if p x then (x:as, bs) else (as, x:bs)
\end{lstlisting}

划分（partition）的概念对于快速排序至关重要。划分在其很多其他排序排序算法中也很关键。本章最后我们会解释它如何在普遍地影响排序的思想方法。在进一步改进快速排序的划分算法前，我们先来考虑如何用命令式的方法实现in-place的快速排序。

在诸多的划分方法中，Nico Lomuto\cite{pearls}、\cite{CLRS}给出的方法是最简单易懂的。我们稍后还会介绍其他划分方法，并展示不同的方法是如何影响性能的。

\begin{figure}[htbp]
   \centering
   \subfloat[Partition invariant]{\includegraphics[scale=0.5]{img/partition-1-way.ps}} \\
   \subfloat[开始]{\includegraphics[scale=0.5]{img/partition-1-way-start.ps}} \\
   \subfloat[结束]{\includegraphics[scale=0.5]{img/partition-1-way-finish.ps}}
   \caption{使用最左边的元素作pivot划分一段数组。}
   \label{fig:partition-1-way}
\end{figure}

图\ref{fig:partition-1-way}描述了这种一次遍历进行划分的方法。我们从左向右逐一处理数组中的元素。任何时候，数组都由图\ref{fig:partition-1-way} (a)所示的几部分组成：

\begin{itemize}
\item 最左侧为pivot，当划分过程结束时，pivot会被移动到最终的位置；
\item 一段只包含不大于pivot的元素的部分。这一段的右侧边界被标记为“left”；
\item 一段只包含大于pivot的元素的部分。这一段的右侧边界被标记为“right”。也就是说，left标记和right标记之间的元素都大于pivot；
\item right标记后面的元素尚未被处理。这部分的元素可能大于，也可能不大于pivot。
\end{itemize}

在划分过程开始的时候，left标记指向pivot，righ标记指向pivot后的下一个元素，如图\ref{fig:partition-1-way} (b)所示。然后算法不断地向右侧移动right标记进行处理直到right标记越过数组的右侧边界。

每次迭代，都比较right标记指向的元素和pivot的大小。若大于pivot，这一元素应该位于left和right标记之间，算法继续向前移动right标记以检查下一个元素；否则，说明right标记指向的元素小于或者等于pivot（不大于），它应该位于left标记的左侧。为此，我们将left标记向前移动一步，然后交换left和right标记指向的元素。

当right标记越过最后一个元素时，所有的元素都已处理完毕。大于pivot的元素都被移动到了left标记的右侧，而其他元素位于left标记的左侧。此时我们需要移动pivot元素，使得它位于这两段的中间。为此，我们可以交换pivot和left标记指向的元素。如图\ref{fig:partition-1-way} (c)中的双向箭头所示。

left标记最终指向pivot，它将整个的数组分成了两部分。我们将left标记作为划分过程的结果返回。实际中，为了方便后继处理，我们通常将left标记增加1，使得它指向第一个大于pivot的元素。整个划分过程中，我们in-place地修改了数组中的内容。

划分算法可以描述如下。它接受三个参数：一个数组$A$，待划分区间的上下界\footnote{这里描述的算法和\cite{CLRS}中的略有不同，后者用待划分区间的最后一个元素作为pivot。}

\begin{algorithmic}[1]
\Function{Partition}{A, l, u}
  \State $p \gets A[l]$  \Comment{the pivot}
  \State $L \gets l$ \Comment{左侧标记}
  \For{$R \in [l+1, u]$} \Comment{对右侧标记进行迭代}
    \If{$\lnot (p < A[R])$} \Comment{对于strict weak order，定义$<$比较就足够了}
      \State $L \gets L + 1$
      \State \textproc{Exchange} $A[L] \leftrightarrow A[R]$
    \EndIf
  \EndFor
  \State \textproc{Exchange} $A[L] \leftrightarrow p$
  \State \Return $L + 1$ \Comment{The partition position}
\EndFunction
\end{algorithmic}

下表给出了划分数组$\{ 3, 2, 5, 4, 0, 1, 6, 7\}$的步骤。

\begin{tabular}{|llllllll|l|}
\hline
\underline{3}(l)  & 2(r) & 5 & 4 & 0 & 1 & 6 & 7 & 开始，$pivot = 3$、$l = 1$、$r = 2$ \\
\underline{3} & 2(l)(r) & 5 & 4 & 0 & 1 & 6 & 7 & $2 < 3$，移动$l$（$r=l$）\\
\underline{3} & 2(l) & 5(r) & 4 & 0 & 1 & 6 & 7 & $5 > 3$, 继续 \\
\underline{3} & 2(l) & 5 & 4(r) & 0 & 1 & 6 & 7 & $4 > 3$, 继续 \\
\underline{3} & 2(l) & 5 & 4 & 0(r) & 1 & 6 & 7 & $0 < 3$ \\
\underline{3} & 2 & 0(l) & 4 & 5(r) & 1 & 6 & 7 & 移动$l$，然后和$r$交换 \\
\underline{3} & 2 & 0(l) & 4 & 5 & 1(r) & 6 & 7 & $1 < 3$ \\
\underline{3} & 2 & 0 & 1(l) & 5 & 4(r) & 6 & 7 & 移动$l$，然后和$r$交换 \\
\underline{3} & 2 & 0 & 1(l) & 5 & 4 & 6(r) & 7 & $6 > 3$，继续 \\
\underline{3} & 2 & 0 & 1(l) & 5 & 4 & 6 & 7(r) & $7 > 3$，继续 \\
1 & 2 & 0 & 3 & 5(l+1) & 4 & 6 & 7 & $r$越过了边界，交换pivot和$l$ \\
\hline
\end{tabular}

下面的ANSI C例子程序实现了这一划分算法。

\lstset{language=C}
\begin{lstlisting}
int partition(Key* xs, int l, int u) {
    int pivot, r;
    for (pivot = l, r = l + 1; r < u; ++r)
        if (!(xs[pivot] < xs[r])) {
            ++l;
            swap(xs[l], xs[r]);
        }
    swap(xs[pivot], xs[l]);
    return l + 1;
}
\end{lstlisting}

其中\texttt{swap(a, b)}可以定义为函数或者宏。ISO C++中\texttt{swap(a, b)}在标准库中以函数模板的形式提供。被交换的元素类型通过模板进行推导。我们此后不再详细解释这些语言细节。

使用这一划分算法，命令式的in-place快速排序可以实现如下：

\begin{algorithmic}[1]
\Procedure{Quick-Sort}{$A, l, u$}
  \If{$l < u$}
    \State $m \gets$ \Call{Partition}{$A, l, u$}
    \State \Call{Quick-Sort}{$A, l, m - 1$}
    \State \Call{Quick-Sort}{$A, m, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

对数组进行排序时，我们传入数组的上下界，如：\textproc{Quick-Sort}($A, 1, |A|$)。其中$l \geq u$用以判断数组片段为空或者只含有一个元素，这两种情况下我们都认为数组是已序的，算法直接返回而无需做任何处理。

下面的ANSI C例子程序给出了in-place快速排序的实现。

\lstset{language=C}
\begin{lstlisting}
void quicksort(Key* xs, int l, int u) {
    int m;
    if (l < u) {
        m = partition(xs, l, u);
        quicksort(xs, l, m - 1);
        quicksort(xs, m, u);
    }
}
\end{lstlisting}

\subsection{函数式划分算法的小改进}
\index{快速排序!One pass functional partition}

在深入分析快速排序的划分算法前，我们首先可以用fold来实现一个小改进：只需要遍历一遍就可以完成划分的算法。读者可以参考本书附录A来了解fold的详细内容。

\be
partition(p, L) = fold(f(p), (\Phi, \Phi), L)
\ee

其中函数$f$使用断言$p$来对元素和pivot进行比较。断言作为一个参数传入函数$f$，我们称之为$f$的“curried”形式，参见附录A。另外，$f$可以是$partition$函数作用域内的一个lexical闭包（lexical closure），它可以访问这一作用域内的断言。函数$f$不断更新划分结果内的一对列表。

\be
f(p, x, (A, B)) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\{ x \} \cup A, B) & p(x) \\
  (A, \{ x \} \cup B) & otherwise(\lnot p(x))
  \end{array}
\right.
\ee

我们这里使用了模式匹配（pattern-matching）形式的定义。在不支持模式匹配的环境中，需要使用一个变量，如$P$来代表列表对$(A, B)$，并使用函数来获取$P$中的两个值。

下面的Haskell例子程序实现了这一改进的快速排序，每次划分只需要遍历一次。

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort small ++ [x] ++ sort big where
  (small, big) = foldr f ([], []) xs
  f a (as, bs) = if a <= x then (a:as, bs) else (as, a:bs)
\end{lstlisting}

\subsubsection{累积划分（Accumulated partition）}
\index{快速排序!Accmulated partition}

使用fold进行划分的过程，实际上是向结果列表对$(A, B)$累积的过程。若元素不大于pivot，则它被累积到$A$，否则累积到$B$。我们可以将这一累积过程明确定义出来，相对于最初的基本快速排序算法，这样既可以节省空间，又利于进行尾递归优化（参见附录A）。

\be
partition(p, L, A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (A, B) & L = \Phi \\
  partition(p, L', \{ l_1 \} \cup A, B) & p(l_1) \\
  partition(p, L', A, \{ l_1 \} \cup B) & otherwise
  \end{array}
\right.
\ee

其中，若列表$L$不空，则$l_1$代表其中的第一个元素，$L'$代表除第一元素外的剩余部分，形如：$L' = \{ l_2, l_3, ...\}$。通过向划分函数传入比较参数，如：$\lambda_x x \leq pivot$即可以实现升序的排序算法。

\be
sort(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  sort(A) \cup \{ l_1 \} \cup sort(B) & otherwise
  \end{array}
\right.
\ee

其中$A$、$B$是通过上述划分函数计算出的结果。

\[
(A, B) = partition(\lambda_x x \leq l_1, L', \Phi, \Phi)
\]

\subsubsection{累积式快速排序}
\index{快速排序!Accumulated quick sort}

观察前面快速排序定义中的递归部分可以发现，列表的连接操作$sort(A) \cup \{l_1\} \cup sort(B)$需要的时间和列表的长度成比例。可以使用附录A中介绍的一些方法提高性能，另外，也可以将排序算法转换为累积形式。

\[
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \Phi \\
  ... & otherwise
  \end{array}
\right.
\]

其中$S$为累积结果。我们传入一个空的起始值来启动排序：$sort(L) = sort'(L, \Phi)$。当划分完成时，需要递归地对两个字列表进行排序。我们可以先递归地将大于pivot的元素排序，然后将pivot链接到这一结果的前面。然后将链接结果作为新的“累积结果”传入后续的排序过程中。

根据这一思路，上述算法中的省略号部分可以实现如下：

\[
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \Phi \\
  sort(A, \{l_1\} \cup sort(B, ?)) & otherwise
  \end{array}
\right.
\]

当开始对$B$排序时，累积结果应该是什么呢？这里有一个很重要的invariant：任何时候，累积结果$S$中总保存了迄今为止已经排序好的元素。因此，我们通过向$S$累积来对$B$排序。

\be
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \Phi \\
  sort(A, \{l_1\} \cup sort(B, S)) & otherwise
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了累积式快速排序算法。

\lstset{language=Haskell}
\begin{lstlisting}
asort xs = asort' xs []

asort' [] acc = acc
asort' (x:xs) acc = asort' as (x:asort' bs acc) where
  (as, bs) = part xs [] []
  part [] as bs = (as, bs)
  part (y:ys) as bs | y <= x = part ys (y:as) bs
                    | otherwise = part ys as (y:bs)
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item 选择一门命令式语言，实现递归的基本快速排序算法。
\item 和命令式快速排序算法类似，除了列表为空的情况外，如果列表只含有一个元素，也可以作为边界情况处理。修改函数式算法，处理这一边界情况。
\item 在累积式快速排序算法的实现中，使用了中间变量$A$、$B$。我们可以通过重新定义划分函数，通过递归调用sort函数来消除中间变量。选择一门函数式编程语言，实现这一改动。
\end{itemize}
\end{Exercise}

\section{快速排序的性能分析}
\index{快速排序!性能分析}

快速排序在实际应用中性能良好，但是给出严格的分析却并不容易。我们需要使用统计学工具来证明平均情况下的性能。

尽管如此，我们可以很直观地计算出最好情况和最坏情况下的性能。显然，最好情况发生在每次划分都能将序列均分成两段长度相同子序列时。如图\ref{fig:qsort-best}所示，共需要$O(\lg n)$次递归调用。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.6]{img/qsort-best.ps}
 \caption{最好情况下，快速排序每次将序列划分成长度相等的两部分。}
 \label{fig:qsort-best}
\end{figure}

总共有$O(\lg n)$层递归。在第一层，进行一次划分，处理$n$个元素；在第二层，进行两次划分，每次划分处理$n/$个元素，第二层的总体执行时间为$2 O(n/2) = O(n)$。在第三层，执行划分四次，每次处理$n/4$个元素，第三层的总体执行时间也是$O(n)$……在最后一层，总共有$n$个片段，每个片段只含有一个元素，总处理时间也是$O(n)$。将上述所有层的执行时间相加，得到快速排序在最好情况下的性能为$O(n \lg n)$。

但是在最坏情况下，划分过程大部分时间都把序列分成两个很不平衡的部分。其中一部分的长度为$O(1)$，另一部分的长度为$O(n)$。因此递归的深度退化为$O(n)$。如果我们用图样的图来描述，最好情况下，快速排序过程形成一棵平衡二叉树；而最坏情况下，会形成一棵很不平衡的树，每个节点都只有一棵子树，而另外一棵子树为空。二叉树退化成了一个长度为$O(n)$的链表。而在每一层中，所有的元素都被处理，因此醉话情况下的性能为$O(n^2)$，这和插入排序、选择排序的性能相当。

最坏情况在何时发生的？其中一个特殊情况是所有的元素（或大部分元素）都相同。Nico Lumuto的划分方法此时的表现很差。我们在下一节会介绍另外一种划分方法，可以有效解决这一问题。

另外有两种明显的序列类型可以导致最坏情况：即序列已序（升序或降序）。划分升序序列时pivot前的部分总是空的，而pivot后的部分包含所有剩余元素。划分降序序列的结果与此相反。

还有其他一些情况可以导致快速排序的性能很差。不存在一种方法可以完全避免最坏情况。我们下一节会给出一些工程方法可以把最坏情况发生的可能性降低。

\subsection{平均情况的分析 $\star$}
\index{快速排序!平均情况分析}

快速排序在平均情况下性能良好。甚至在每次划分时，总得到长度比为1:9的两部分，总体性能仍然为$O(n \lg n)$\cite{CLRS}。

本节需要一些额外的数学知识，读者可以选择跳过。

有两种方法可以证明快速排序在平均情况下的性能。其中一种方法利用了快速排序中的比较操作的次数来考量性能\cite{CLRS}。作为对比，在选择排序中，任何两个元素都进行了比较。与此不同，快速排序避免了很多不必要的比较。考虑划分列表$\{ a_1, a_2, a_3, ..., a_n\}$，选择$a_1$作为pivot，划分结果产生两个子列表$A = \{x_1, x_2, ..., x_k\}$和$B = \{ y_1, y_2, ..., y_{n-k-1} \}$。在接下来的快速排序过程中，$A$中的任何元素，都不再和$B$中的任何元素进行比较。

记最终排序的结果为$\{ a_1, a_2, ..., a_n \}$，我们有这样的结果：若$a_i < a_j$，我们将不再对它们进行比较当且仅当存在某一元素$a_k$满足$a_i < a_k < a_j$，并且$a_k$在$a_i$或$a_j$之前被选为pivot。

也就是说，$a_i$与$a_j$进行比较的唯一可能是要么$a_i$，要么$a_j$在所有$a_{i+1} < a_{i+2} < ... < a_{j-1}$之前被选为pivot。

令$P(i, j)$代表$a_i$和$a_j$进行比较的概率，我们有：

\be
P(i, j) = \frac{2}{j - i + 1}
\ee

全部比较操作的总数可以这样得到：

\be
C(n) = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n} P(i, j)
\ee

如果我们比较了$a_i$和$a_j$，在接下来的快速排序中，就不再比较$a_j$和$a_i$，并且元素$a_i$永远不会和自己进行比较。因此在上式中，$i$的上限为$n-1$，$j$的下限为$i+1$。

将概率代入，得：

\be
\begin{array}{rl}
C(n) & = \displaystyle \sum_{i=1}^{n-1}\sum_{j = i+1}^{n} \frac{2}{j - i + 1} \\
     & = \displaystyle \sum_{i=1}^{n-1}\sum_{k=1}^{n-i} \frac{2}{k+1} \\
\end{array}
\ee

使用调和级数\cite{wiki-harmonic}。

\[
H_n = 1 + \frac{1}{2} + \frac{1}{3} + .... = \ln n + \gamma + \epsilon_n
\]

因此：

\be
C(n) = \sum_{i=1}^{n-1} O(\lg n) = O(n \lg n)
\ee

我们还可以用另外一种方法证明快速排序在平均情况下的性能。考虑递归，当待排序的列表长度为$n$时，划分过程将列表分成两个部分，一部分长度为$i$，另一部分长度为$n-i-1$。划分过程需要比较pivot和每个元素，它自身用时$cn$。因此我们有如下递归关系：

\be
T(n) = T(i) + T(n-i-1) + c n
\ee

其中$T(n)$是对长度为$n$的列表进行快速排序所用的时间。由于$i$以相同的概率在$0, 1, ..., n-1$中取值，通过使用数学期望，可以得到如下结果：

\be
\renewcommand*{\arraystretch}{1.5}
\begin{array}{rl}
T(n) & = E(T(i)) + E(T(n-i-1)) + c n \\
     & = \displaystyle \frac{1}{n} \sum_{i=0}^{n-1}T(i) + \frac{1}{n} \sum_{i=0}^{n-1}T(n-i-1) + cn \\
     & = \displaystyle \frac{1}{n} \sum_{i=0}^{n-1}T(i) + \frac{1}{n} \sum_{j=0}^{n-1}T(j) + cn \\
     & = \displaystyle \frac{2}{n} \sum_{i=0}^{b-1}T(i) + cn
\end{array}
\ee

两边同时乘以$n$：

\be
n T(n) = 2 \sum_{i=0}^{n-1} T(i) + c n^2
\label{eq:ntn}
\ee

将$n$用$n-1$替换，可以得到另外一个等式：

\be
(n-1) T(n-1) = 2 \sum_{i=0}^{n-2} T(i) + c (n-1)^2
\label{eq:n1tn1}
\ee

用式(\ref{eq:ntn})减去式(\ref{eq:n1tn1})可以消去所有的$T(i)$，其中$0 \leq i < n-1$。

\be
n T(n) = (n + 1) T(n-1) + 2cn - c
\ee

在计算性能时，我们可以忽略掉常数时间$c$。因此上式进一步变化为：

\be
\frac{T(n)}{n+1} = \frac{T(n-1)}{n} + \frac{2c}{n+1}
\ee

我们依次用$n-1$、$n-2$……代入$n$，可以得到$n-1$个等式。

\[
\frac{T(n-1)}{n} = \frac{T(n-2)}{n-1} + \frac{2c}{n}
\]

\[
\frac{T(n-2)}{n-1} = \frac{T(n-3)}{n-2} + \frac{2c}{n-1}
\]

\[
...
\]

\[
\frac{T(2)}{3} = \frac{T(1)}{2} + \frac{2c}{3}
\]

将所有等式相加，消去左右两侧相同的变量，可以化简得到一个关于$n$的函数。

\be
\frac{T(n)}{n+1} = \frac{T(1)}{2} + 2c \sum_{k=3}^{n+1} \frac{1}{k}
\ee

使用上面提到的调和级数，最终的结果为：

\be
O(\frac{T(n)}{n+1}) = O(\frac{T(1)}{2} + 2c \ln n + \gamma + \epsilon_n) = O(\lg n)
\ee

因此

\be
O(T(n)) = O(n \lg n)
\ee

\begin{Exercise}
\begin{itemize}
\item 当有很多重复元素时，为什么Lomuto的方法性能会变差？
\end{itemize}
\end{Exercise}

% ================================================================
% Minor Improvement for quick sort
% ================================================================

\section{工程实践中的改进}
\index{快速排序!Engineering improvement}

大多数情况下快速排序性能优异。但是在最差的情况下，性能会下降到平方级别。如果待排序的数据是随机分布的，这样的情况比较罕见，但是某些常见的特殊序列却会引发最差情况。

本节我们介绍一些工程上常用的方法，它们或者针对某些特殊的输入数据改进划分算法来避免性能下降，或者通过改变概率分布来减小出现最差情况的可能。

\subsection{处理重复元素的工程方法}
\index{快速排序!处理重复元素}

如上一节的练习中所示，N. Lomuto的划分算法不擅长处理含有很多重复元素的序列。考虑含有$n$个相等元素的特殊序列$\{x, x, ..., x\}$，我们有两种方案来进行排序。

\begin{enumerate}
\item 普通的基本快速排序法：我们任意选择一个元素作为pivot，其值为$x$，这样分割后得到两个子序列，一个是$\{x, x, ..., x \}$，包含$n-1$个元素，另外一个子序列为空。接下来递归地对第一个子序列排序；这明显是一个$O(n^2)$的解决方法。
\item 另外一个方法是只挑选严格小于$x$的元素，和严格大于$x$的元素进行划分。这样得到的结果是两个空序列，和$n$个等于pivot的元素。接下来我们递归地对只含有小于pivot的元素子序列和只含有大于pivot的元素的子序列进行排序，由于它们都为空，因此递归调用立即结束。剩下要做的就是将比pivot小的元素的排序结果，全部等于pivot的元素，和比pivot大的元素的排序结果连接起来。
\end{enumerate}

如果所有元素都相等，第二种方法只需要$O(n)$时间。这给出了划分算法的一个重要改进：相对于二分划分(binary partition，划分成两个子序列和一个pivot)，三分划分（ternary partition，划分成三个子序列）能更好地处理重复元素。

我们可以这样来定义三分划分快速排序（ternary quick sort）：

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  sort(S) \cup sort(E) \cup sort(G) & otherwise
  \end{array}
\right.
\ee

其中$S, E, G$分别是所有小于、等于、和大于pivot的元素组成的列表。

\[
\begin{array}{l}
S = \{ x | x \in L, x < l_1 \} \\
E = \{ x | x \in L, x = l_1 \} \\
G = \{ x | x \in L, l_1 < x \}
\end{array}
\]

下面的Haskell例子程序实现了基本的三分快速排序算法。

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort [a | a<-xs, a<x] ++
              x:[b | b<-xs, b==x] ++ sort [c | c<-xs, c>x]
\end{lstlisting}

注意，元素间的比较必须支持“小于”和“等于”操作。基本的三分快速排序需要线性时间$O(n)$将三个子列表连接起来。可以使用一个累积变量（accumulator）来改善这一性能。

令函数$sort'(L, A)$表示带有累积变量的三分快速排序定义，其中$L$为待排序序列，累积变量$A$包含已排好序的部分。它最开始时为空：$sort(L) = sort'(L, \Phi)$。我们可以先定义好边界条件：

\[
sort'(L, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \Phi \\
  ... & otherwise
  \end{array}
\right.
\]

对于递归情况，三分划分将序列分为三个子序列$S, E, G$，其中只有$S$和$G$需要递归排序，而$E$包含全部等于pivot的元素，无需进一步排序了。我们可以先使用累积变量$A$对$G$进行排序，然后将排序结果连接到$E$的后面，作为新的累积变量对$S$进行排序。

\be
sort'(L, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \Phi \\
  sort(S, E \cup sort(G, A)) & otherwise
  \end{array}
\right.
\ee

划分算法也可以使用累积变量来实现。这和基本的快速排序类似。注意这里我们不能只传入一个和pivot进行比较的断言，而需要传入两个：一个用于“小于”比较，另外一个用于“等于”判断。简单起见，这里我们传入pivot元素。

\be
partition(p, L, S, E, G) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (S, E, G) & L = \Phi \\
  partition(p, L', \{l_1\} \cup S, E, G) & l_1 < p \\
  partition(p, L', S, \{l_1\} \cup E, G) & l_1 = p \\
  partition(p, L', S, E, \{ l_1 \} \cup G) & p < l_1
  \end{array}
\right.
\ee

其中，若$L$不为空，$l_1$为$L$中的第一个元素，$L'$包含除$l_1$外的剩余部分。下面的Haskell例子程序实现了这一算法。它在划分算法的边界情况中启动递归排序。

\lstset{language=Haskell}
\begin{lstlisting}
sort xs = sort' xs []

sort' []     r = r
sort' (x:xs) r = part xs [] [x] [] r where
    part [] as bs cs r = sort' as (bs ++ sort' cs r)
    part (x':xs') as bs cs r | x' <  x = part xs' (x':as) bs cs r
                             | x' == x = part xs' as (x':bs) cs r
                             | x' >  x = part xs' as bs (x':cs) r
\end{lstlisting}

Richard Bird给出了另外一个改进\cite{fp-pearls}，它不对递归排序的结果立即执行连接操作，而是把排好的子列表放入一个列表中保存。最终再将这些子列表连接在一起。

\lstset{language=Haskell}
\begin{lstlisting}
sort xs = concat $ pass xs []

pass [] xss = xss
pass (x:xs) xss = step xs [] [x] [] xss where
    step [] as bs cs xss = pass as (bs:pass cs xss)
    step (x':xs') as bs cs xss | x' <  x = step xs' (x':as) bs cs xss
                               | x' == x = step xs' as (x':bs) cs xss
                               | x' >  x = step xs' as bs (x':cs) xss
\end{lstlisting} %$

\subsubsection{双向划分（2-way partition）}
\index{快速排序!双向划分}

也可以用命令式的方法解决大量重复元素的问题。Robert Sedgewick给出了一个划分方法\cite{qsort-impl}、\cite{pearls}，使用两个指针，一个从左向右移动，另一个从右向左移动。开始的时候两个指针指向数组的左右边界。

划分开始时，选择最左侧的元素作为pivot。然后左侧指针$i$不断向右前进直到遇到一个不小于pivot的元素；另外\footnote{注意，我们没有使用“然后”一词，因为这两轮扫描可以同时并发进行。}，右侧指针$j$不断向左扫描直到遇到一个不大于pivot的元素。

此时，所有在左侧指针$i$之前的元素都严格小于pivot，而所有在右侧指针$j$之后的元素都严格大于pivot。$i$指向一个大于或等于pivot的元素；而$j$指向一个小于或等于pivot的元素。图\ref{fig:partition-2-way} (a)描述了此时的情形。

为了将全部小于或等于pivot的元素划分到左侧，而其余元素划分到右侧，我们可以交换$i$和$j$指向的两个元素。然后我们恢复扫描，重复上面的步骤直到$i$和$j$相遇或者交错。

在划分的任何时刻，总保持着不变条件（invariant），即所有$i$之前的元素（包括$i$指向的元素）都不大于pivot；而所有$j$之后的元素（包括$j$指向的元素）都不小于pivot。$i$和$j$之间的元素尚未处理。图\ref{fig:partition-2-way} (b)描述了这一不变条件。

\begin{figure}[htbp]
   \centering
   \subfloat[指针$i$和$j$停止前进时]{\includegraphics[scale=0.5]{img/partition-2-way-inner.ps}} \\
   \subfloat[划分不变条件（invariant）]{\includegraphics[scale=0.5]{img/partition-2-way.ps}} \\
   \caption{选择最左侧的元素作为pivot进行划分。}
   \label{fig:partition-2-way}
\end{figure}

当左侧指针$i$和右侧指针$j$相遇或交错时，我们需要进行一次额外的交换操作，将最左侧的pivot元素交换到$j$指向的位置上。然后，我们对划分区间下届和$j$之间的数组片段，以及$i$和划分区间上界之间的片段进行递归排序。

这一算法可以描述如下。

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$} \Comment{sort range $[l, u)$}
  \If{$u - l > 1$} \Comment{非平凡情况下包含1个以上的元素}
    \State $i \gets l$, $j \gets u$
    \State $pivot \gets A[l]$
    \Loop
      \Repeat
        \State $i \gets i + 1$
      \Until{$A[i] \geq pivot$} \Comment{忽略$i \geq u$的错误处理}
      \Repeat
        \State $j \gets j - 1$
      \Until{$A[j] \leq pivot$} \Comment{忽略$j < l$的错误处理}
      \If{$j < i$}
        \State break
      \EndIf
      \State \textproc{Exchange} $A[i] \leftrightarrow A[j]$
    \EndLoop
    \State \textproc{Exchange} $A[l] \leftrightarrow A[j]$ \Comment{移动pivot}
    \State \Call{Sort}{$A, l, j$}
    \State \Call{Sort}{$A, i, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

考虑所有元素都相等的极端情况，这一in-place的快速排序将数组划分为两段长度相等的子数组，这里发生了$\frac{n}{2}$次不必要的交换操作。由于划分是平衡的，所以总体性能仍然为$O(n \lg n)$，而没有下降到平方级别。下面的C语言例子程序实现了这一算法。

\lstset{language=C}
\begin{lstlisting}
void qsort(Key* xs, int l, int u) {
    int i, j, pivot;
    if (l < u - 1) {
        pivot = i = l; j = u;
        while (1) {
            while (i < u && xs[++i] < xs[pivot]);
            while (j >=l && xs[pivot] < xs[--j]);
            if (j < i) break;
            swap(xs[i], xs[j]);
        }
        swap(xs[pivot], xs[j]);
        qsort(xs, l, j);
        qsort(xs, i, u);
    }
}
\end{lstlisting}

和此前介绍的N. Lumoto的划分算法相比，可以发现这一算法的元素交换操作次数更少。这是因为它跳过了那些最终位置在pivot正确一侧的元素不进行交换。

\subsubsection{三路划分}
\index{快速排序!三路划分}

显然，我们应该避免对重复元素进行不必要的交换操作。进一步，可以利用一种称为“三分排序”（ternary sort，也称作三路划分）的思路来改进算法，所有严格小于pivot的元素被放入左侧的子序列片段，严格大于pivot的元素被放入右侧，而中间部分包含所有等于pivot的元素。使用三路划分，我们只需要对不等于pivot的元素进行递归排序。在上述的特殊情况中，由于所有的元素都相等，我们无需进行进一步的递归排序。因此整体的性能为线性时间$O(n)$。

我们接下来需要考虑如何实现三路划分。Jon Bentley和Douglas McIlroy给出了一个方法：如图\ref{fig:partition-3-way} (a)所示，所有和pivot相等的元素最初保存在最左侧和最右侧\cite{3-way-part}、\cite{opt-qs}。

\begin{figure}[htbp]
   \centering
   \subfloat[三路划分的不变条件（Invariant）。]{\includegraphics[scale=0.5]{img/partition-3-way.ps}} \\
   \subfloat[将和pivot相等的元素交换到中间部分。]{\includegraphics[scale=0.5]{img/partition-3-way-end.ps}} \\
   \caption{三路划分}
   \label{fig:partition-3-way}
\end{figure}

扫描过程大部分和Robert Sedgewick给出的相似，两个指针$i$和$j$相向前进直到$i$遇到任何大于等于pivot的元素，并且$j$遇到任何小于等于pivot的元素。此时，如果$i$和$j$没有相遇或者交错，我们不仅交换它们指向的元素，同时检查被指向的元素是否和pivot相等，如果相等，就交换$i$和$p$指向的元素，以及$j$和$q$指向的元素。

在划分过程结束前，需要把所有等于pivot的元素从左右两侧交换到中间。交换的次数取决于重复元素的个数。如果所有的元素都不等，则交换次数为零，不产生任何额外的性能消耗。划分的最终结果如图\ref{fig:partition-3-way} (b)所示。此后，我们只需要对“严格小于”和“严格大于”部分的子片段进行递归排序。

可以通过修改两路划分的算法进行实现。

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $i \gets l$, $j \gets u$
    \State $p \gets l$, $q \gets u$ \Comment{指向相等元素的边界}
    \State $pivot \gets A[l]$
    \Loop
      \Repeat
        \State $i \gets i + 1$
      \Until{$A[i] \geq pivot$} \Comment{忽略$i \geq u$的错误处理}
      \Repeat
        \State $j \gets j - 1$
      \Until{$A[j] \leq pivot$} \Comment{忽略$j < l$的错误处理}
      \If{$j \leq i$}
        \State break \Comment{注意和此前算法的不同}
      \EndIf
      \State \textproc{Exchange} $A[i] \leftrightarrow A[j]$
      \If{$A[i] = pivot$} \Comment{处理相等的元素}
        \State $p \gets p + 1$
        \State \textproc{Exchange} $A[p] \leftrightarrow A[i]$
      \EndIf
      \If{$A[j] = pivot$}
        \State $q \gets q - 1$
        \State \textproc{Exchange} $A[q] \leftrightarrow A[j]$
      \EndIf
    \EndLoop
    \If{$i = j \land A[i] = pivot$} \Comment{特殊情况}
      \State $j \gets j - 1$, $i \gets i + 1$
    \EndIf
    \For{$k$ from $l$ to $p$} \Comment{将相等的元素交换到中间}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \State $j \gets j - 1$
    \EndFor
    \For{$k$ from $u-1$ down-to $q$}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
      \State $i \gets i + 1$
    \EndFor
    \State \Call{Sort}{$A, l, j + 1$}
    \State \Call{Sort}{$A, i, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

下面的C语言例子程序实现了三路划分快速排序算法。

\lstset{language=C}
\begin{lstlisting}
void qsort2(Key* xs, int l, int u) {
    int i, j, k, p, q, pivot;
    if (l < u - 1) {
        i = p = l; j = q = u; pivot = xs[l];
        while (1) {
            while (i < u && xs[++i] < pivot);
            while (j >= l && pivot < xs[--j]);
            if (j <= i) break;
            swap(xs[i], xs[j]);
            if (xs[i] == pivot) { ++p; swap(xs[p], xs[i]); }
            if (xs[j] == pivot) { --q; swap(xs[q], xs[j]); }
        }
        if (i == j && xs[i] == pivot) { --j, ++i; }
        for (k = l; k <= p; ++k, --j) swap(xs[k], xs[j]);
        for (k = u-1; k >= q; --k, ++i) swap(xs[k], xs[i]);
        qsort2(xs, l, j + 1);
        qsort2(xs, i, u);
    }
}
\end{lstlisting}

引入三路划分后，算法逐渐变得复杂了。各种边界条件都需要进行仔细的处理。回顾此前的N. Lumoto的划分方法，它的优势就是简单直观，我们可以考虑对它加以改进，得到一个简单的三路划分实现。

我们需要调整一下不变条件（invariant）。我们仍然选择第一个元素作为pivot，如图\ref{fig:partition-3-way-lumoto}所示，任何时刻，左侧的片段包含严格小于pivot的元素；接下来的片段包含等于pivot的元素；最右侧的片段包含严格大于pivot的元素。这三个片段的边界分别为$i$、$k$和$j$。剩余在$k$和$j$之间的部分是尚未扫描的元素。

我们从左向右逐一扫描元素，一开始时，严格小于pivot的部分为空；等于pivot的部分只包含一个元素，就是pivot本身。$i$此时指向数组的下界，$k$指向$i$的下一个元素。严格大于pivot的部分也为空，$j$指向数组的上界。

\begin{figure}[htbp]
   \centering
   \includegraphics[scale=0.5]{img/partition-3-way-lumoto.ps}
   \caption{基于N. Lumoto方法的三路划分。}
   \label{fig:partition-3-way-lumoto}
\end{figure}

划分过程开始后，我们逐一检查$k$指向的元素。如果它等于pivot，$k$就移动指向下一个元素；如果它大于pivot，我们将它和未处理区间的最后一个元素交换，这样严格大于的区间长度就增加一。它的边界$j$向左移动一步。由于我们不确定移动到$k$的元素是否仍然大于pivot，我们需要再次进行比较，重复上述过程。否则，如果元素小于pivot，我们将它和等于pivot区间的第一个元素交换。当$k$和$j$相遇时，划分过程结束。

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $i \gets l$, $j \gets u$, $k \gets l + 1$
    \State $pivot \gets A[i]$
    \While{$k < j$}
      \While{$pivot < A[k]$}
        \State $j \gets j - 1$
        \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \EndWhile
      \If{$A[k] < pivot$}
        \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
        \State $i \gets i + 1$
      \EndIf
      \State $k \gets k + 1$
    \EndWhile
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

和前面的三路划分快速排序算法相比，这一算法要相对简单，但是需要更多的交换次数。下面的C语言例子程序实现了这一算法。

\lstset{language=C}
\begin{lstlisting}
void qsort(Key* xs, int l, int u) {
    int i, j, k; Key pivot;
    if (l < u - 1) {
        i = l; j = u; pivot = xs[l];
        for (k = l + 1; k < j; ++k) {
            while (pivot < xs[k]) { --j; swap(xs[j], xs[k]); }
            if (xs[k] < pivot) { swap(xs[i], xs[k]); ++i; }
        }
        qsort(xs, l, i);
        qsort(xs, j, u);
    }
}
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item 我们给出的命令式快速排序算法都使用第一个元素作为pivot，也可以使用最后一个元素作为pivot。请修改快速的排序的基本算法，Sedgewick的改进算法，和三路快速排序算法，使用最后一个元素作为pivot。
\end{itemize}
\end{Exercise}

\section{针对最差情况的工程实践}

虽然三分快速排序（使用三路划分）能处理含有很多重复元素的序列，但是仍然无法有效解决典型的最差情况。例如，如果序列中的大部分元素已序时，无论是升序还是降序，划分的结果将会是两个长度不平衡的子序列，一个包含少量的元素，另一个包含剩余的部分。

考虑两种极端情况：$\{ x_1 < x_2 < ... < x_n\}$和$\{ y_1 > y_2 > ... > y_n\}$。图\ref{fig:worst-cases-1}给出了划分结果。

\begin{figure}[htbp]
   \centering
   \subfloat[序列$\{x_1 < x_2 < ... < x_n\}$的划分树，每次划分时，选择第一个元素为pivot，小于等于pivot的部分总为空。]{\hspace{.3\textwidth} \includegraphics[scale=0.5]{img/unbalanced.ps} \hspace{.3\textwidth}} \\
   \subfloat[序列$\{y_1 > y_2 > ... > y_n\}$的划分树，每次划分时，选择第一个元素为pivot，大于等于pivot的部分总为空。]{\includegraphics[scale=0.5]{img/unbalanced-2.ps}} \\
   \caption{两种最差情况。}
   \label{fig:worst-cases-1}
\end{figure}

很容易给出更多的最差情况，例如$\{ x_m, x_{m-1}, ..., x_2, x_1, x_{m+1}, x_{m+2}, ... x_n\}$，其中$\{ x_1 < x_2 < ... < x_n \}$；另一个例子是$\{x_n, x_1, x_{n-1}, x_2, ... \}$。图\ref{fig:worst-cases-2}给出了它们的划分结果。

\begin{figure}[htbp]
   \centering
   \subfloat[除了第一次划分结果，其他都不平衡。]{\includegraphics[scale=0.4]{img/unbalanced-3.ps}} \\
   \subfloat[一个zig-zag形状的划分树。]{\includegraphics[scale=0.5]{img/unbalanced-zigzag.ps}} \\
   \caption{另两种最差情况。}
   \label{fig:worst-cases-2}
\end{figure}

观察可以发现，仅仅简单地选择第一个元素作为pivot，很容易使得划分的结果不平衡，Robert Sedgwick在\cite{qsort-impl}中给出了一种改进，在实际中得到了广泛的使用。这一改进不是每次在固定的位置上选择一个pivot，而是进行简单的抽样以减小引发不平衡划分的可能性。一种抽样方法是检查第一个元素，中间的元素，和末尾的元素，然后选择这三个元素的中数（median）作为pivot。在最差情况下，他保证划分后较短的序列至少含有一个元素。

在实际实现中还有一个细节需要注意。由于数组的索引在实际中的字长通常是有限的，简单实用\texttt{|(l + u) / 2|}来计算中间元素的索引可能引发溢出错误。正确的做法是使用\texttt{l + (u - l) / 2}来索引中间位置的元素。有两种方法来寻找中数，一种最多需要三次比较操作\cite{3-way-part}；另外一种方法通过交换将三个元素中的最小值移动到第一个元素的位置，将最大值移动到最后一个元素的位置，将中数移动到中间位置。此后选在中间位置的元素作为pivot即可。下面的算法使用第二种方法确定划分的pivot。

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$ \Comment{实际中要处理溢出的情况}
    \If{$A[m] < A[l]$} \Comment{确保$A[l] \leq A[m]$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[m]$
    \EndIf
    \If{$A[u-1] < A[l]$} \Comment{确保$A[l] \leq A[u-1]$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[u-1]$
    \EndIf
    \If{$A[u-1] < A[m]$} \Comment{确保$A[m] \leq A[u-1]$}
      \State \textproc{Exchange} $A[m] \leftrightarrow A[u-1]$
    \EndIf
    \State \textproc{Exchange} $A[l] \leftrightarrow A[m]$
    \State $(i, j) \gets $ \Call{Partition}{$A, l, u$}
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

对上述的4种特殊的最差情况，这一算法显然性能良好。它常常被称为“三者取中数”算法（median-of-three），我们将它的命令式实现留给读者作为练习。

但是，在纯函数式环境中，随机获取中间和最后的元素代价很大，我们不能直接将命令式的中数选择算法翻译过来。为了进行少量抽样，一种替代方案是在前三个元素中获取中数。如下面的Haskell例子程序所示。

\lstset{language=Haskell}
\begin{lstlisting}
qsort [] = []
qsort [x] = [x]
qsort [x, y] = [min x y, max x y]
qsort (x:y:z:rest) = qsort (filter (< m) (s:rest)) ++ [m] ++ qsort (filter (>= m) (l:rest)) where
    xs = [x, y, z]
    [s, m, l] = [minimum xs, median xs, maximum xs]
\end{lstlisting}

但是，对于上述4种特殊的最差情况，这种替代方案都不能良好工作，本质原因是由于抽样的质量很差，我们需要在大范围内（整个列表），而不是在小范围内（前三个）进行抽样。我们稍后会介绍如果用函数式的方法解决这一划分问题。

出了median-of-three方法，另一种流行的工程实践是随机选择元素作为pivot，例如下面的改进：

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State \textproc{Exchange} $A[l] \leftrightarrow A[$ \Call{Random}{$l, u$} $]$
    \State $(i, j) \gets $ \Call{Partition}{$A, l, u$}
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

函数\textproc{Random}($l, u$返回一个在$l$和$u$之间的随机整数$l \leq i < u$。这一位置上的元素被交换到第一位置上作为pivot用以进行划分。这一算法称为\underline{随机快速排序}\cite{CLRS}。

理论上，无论median-of-three还是随机快速排序都不能完全避免最差情况。如果待排序序列是随机分布的，无论选择第一个作为pivot，还是任何其他位置上的元素，在效果上都是相同的。在纯函数式编程环境中，列表的底层数据结构通常是单向链表，没有简单的方法可以实现纯函数式的随机快速排序。

即使在理论上无法避免最差情况，但是这些工程上的时间在实际应用中往往能够取得很好的结果。

\section{其他工程实践}
\index{快速排序!Insertion sort fall-back}

还有一些工程实践，它们不是着眼于解决划分的最差情况。Robert Sedgewick观察到如果待排序的列表较短时，快速排序引入的额外代价比较明显，此时插入排序反而更有优势\cite{pearls}、\cite{3-way-part}。Sedgewick、Bentley和McIlroy尝试了不同的序列长度，称为“Cut-Off”。如果序列中的元素个数少于Cut-Off，就转而使用插入排序。

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > $ \textproc{Cut-Off}}
    \State \Call{Quick-Sort}{$A, l, u$}
  \Else
    \State \Call{Insertion-Sort}{$A, l, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

The implementation of this improvement is left as exercise to the reader.

\begin{Exercise}
\begin{itemize}
\item Can you figure out more quick sort worst cases besides the four given in this section?
\item Implement median-of-three method in your favorite imperative programming language.
\item Implement random quick sort in your favorite imperative programming language.
\item Implement the algorithm which falls back to insertion sort when the length of list is
small in both imperative and functional approach.
\end{itemize}
\end{Exercise}

\section{Side words}
It's sometimes called `true quick sort' if the implementation equipped with most of
the engineering practice we introduced, including insertion sort fall-back with cut-off,
in-place exchanging, choose the pivot by median-of-three method, 3-way-partition.

The purely functional one, which express the idea of quick sort perfect can't
take all of them. Thus someone think the functional quick sort is essentially
tree sort.

Actually, quick sort does have close relationship with tree sort. Richard Bird
shows how to derive quick sort from binary tree sort by deforestation \cite{algo-fp}.

Consider a binary search tree creation algorithm called $unfold$. Which turns a
list of elements into a binary search tree.

\be
unfold(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  tree(T_l, l_1, T_r) & otherwise
  \end{array}
\right.
\ee

Where

\be
\begin{array}{l}
T_l = unfold( \{ a | a \in L', a \leq l_1\} ) \\
T_r = unfold( \{ a | a \in L', l_1 < a \} )
\end{array}
\ee

The interesting point is that, this algorithm creates tree in a different
way as we introduced in the chapter of binary search tree. If the list to be unfold
is empty, the result is obviously an empty tree. This is the trivial edge case;
Otherwise, the algorithm set the first element $l_1$ in the list as the key of the
node, and recursively creates its left and right children. Where the elements
used to form the left child is those which are less than or equal to the
key in $L'$, while the rest elements which are greater than the key are used to form
the right child.

Remind the algorithm which turns a binary search tree to a list by in-order
traversing:

\be
toList(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & T = \Phi \\
  toList(left(T)) \cup \{ key(T) \} \cup toList(right(T)) & otherwise
  \end{array}
\right.
\ee

We can define quick sort algorithm by composing these two functions.

\be
quickSort = toList \cdot unfold
\ee

The binary search tree built in the first step of applying $unfold$ is the intermediate
result. This
result is consumed by $toList$ and dropped after the second step. It's quite possible to
eliminate this intermediate result, which leads to the basic version of quick sort.

The elimination of the intermediate binary search tree is called {\em deforestation}.
This concept is based on Burstle-Darlington's work \cite{slpj}.

% ================================================================
% Merge Sort
% ================================================================

\section{Merge sort}
\index{Merge Sort}
Although quick sort performs perfectly in average cases, it can't avoid the worst case no matter what
engineering practice is applied. Merge sort, on the other kind, ensure the performance is bound to
$O(n \lg n)$ in all the cases. It's particularly useful in theoretical algorithm design and analysis.
Another feature is that merge sort is friendly for linked-space settings, which is suitable for
sorting nonconsecutive stored sequences. Some functional programming and dynamic programming environments
adopt merge sort as the standard library sorting solution, such as Haskel, Python and Java (later than
Java 7).

In this section, we'll first brief the intuitive idea of merge sort, provide a basic version.
After that, some variants of merge sort will be given including nature merge sort, and bottom-up
merge sort.

\subsection{Basic version}
\index{Merge Sort!Basic version}
Same as quick sort, the essential idea behind merge sort is also divide and conquer. Different
from quick sort, merge sort enforces the divide to be strictly balanced, that it always splits the
sequence to be sorted at the middle point. After that, it recursively sort the sub sequences
and merge the sorted two sequences to the final result. The algorithm can be described as the
following.

In order to sort a sequence $L$,
\begin{itemize}
\item Trivial edge case: If the sequence to be sorted is empty, the result is obvious empty;
\item Otherwise, split the sequence at the middle position, recursively sort the two sub sequences
and merge the result.
\end{itemize}

The basic merge sort algorithm can be formalized with the following equation.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  merge(sort(L_1), sort(L_2)) & otherwise, (L_1, L_2) = splitAt(\lfloor \frac{|L|}{2} \rfloor, L)
  \end{array}
\right.
\ee

\subsubsection{Merge}
\index{Merge Sort!Merge}
There are two `black-boxes' in the above merge sort definition, one is the $splitAt$ function,
which splits a list at a given position; the other is the $merge$ function, which can
merge two sorted lists into one.

As presented in the appendix of this book, it's trivial to realize $splitAt$
in imperative settings by using random access. However, in functional settings, it's typically
realized as a linear algorithm:

\be
splitAt(n, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, L) & n = 0 \\
  (\{l_1\} \cup A, B) & otherwise, (A, B) = splitAt(n - 1, L')
  \end{array}
\right.
\ee

Where $l_1$ is the first element of $L$, and $L'$ represents the rest elements except of $l_1$ if $L$
isn't empty.

The idea of merge can be illustrated as in figure \ref{fig:merge}. Consider two lines of kids.
The kids have already stood in order of their heights. that the shortest one stands at the
first, then a taller one, the tallest one stands at the end of the line.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/merge.eps}
 \caption{Two lines of kids pass a door.}
 \label{fig:merge}
\end{figure}

Now let's ask the kids to pass a door one by one, every time there can be at most one kid
pass the door. The kids must pass this door in the order of their height. The one can't
pass the door before all the kids who are shorter than him/her.

Since the two lines of kids have already been `sorted', the solution is to ask the first
two kids, one from each line, compare their height, and let the shorter kid pass the door;
Then they repeat this step until one line is empty, after that, all the rest kids can
pass the door one by one.

This idea can be formalized in the following equation.

\be
merge(A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & B = \Phi \\
  B & A = \Phi \\
  \{a_1\} \cup merge(A', B) & a_1 \leq b_1 \\
  \{b_1\} \cup merge(A, B') & otherwise
  \end{array}
\right.
\ee

Where $a_1$ and $b_1$ are the first elements in list $A$ and $B$; $A'$ and $B'$ are the
rest elements except for the first ones respectively. The first two cases are trivial
edge cases. That merge one sorted list with an empty list results the same sorted list;
Otherwise, if both lists are non-empty, we take the first elements from the two lists,
compare them, and use the minimum as the first one of the result, then recursively
merge the rest.

With $merge$ defined, the basic version of merge sort can be implemented like the following
Haskell example code.

\lstset{language=Haskell}
\begin{lstlisting}
msort [] = []
msort [x] = [x]
msort xs = merge (msort as) (msort bs) where
  (as, bs) = splitAt (length xs `div` 2) xs

merge xs [] = xs
merge [] ys = ys
merge (x:xs) (y:ys) | x <= y = x : merge xs (y:ys)
                    | x >  y = y : merge (x:xs) ys
\end{lstlisting}

Note that, the implementation differs from the algorithm definition that it treats the singleton
list as trivial edge case as well.

Merge sort can also be realized imperatively. The basic version can be developed as the below algorithm.

\begin{algorithmic}[1]
\Procedure{Sort}{$A$}
  \If{$|A| > 1$}
    \State $m \gets \lfloor \frac{|A|}{2} \rfloor$
    \State $X \gets$ \Call{Copy-Array}{$A[1...m]$}
    \State $Y \gets$ \Call{Copy-Array}{$A[m+1...|A|]$}
    \State \Call{Sort}{$X$}
    \State \Call{Sort}{$Y$}
    \State \Call{Merge}{$A, X, Y$}
  \EndIf
\EndProcedure
\end{algorithmic}

When the array to be sorted contains at least two elements, the non-trivial sorting process starts.
It first copy the first half to a new created array $A$, and the second half to a second new array $B$.
Recursively sort them; and finally merge the sorted result back to $A$.

This version uses the same amount of extra spaces of $A$. This is because the \textproc{Merge} algorithm
isn't in-place at the moment. We'll introduce the imperative in-place merge sort in later section.

The merge process almost does the same thing as the functional definition. There is a verbose version
and a simplified version by using sentinel.

The verbose merge algorithm continuously checks the element from the two input arrays, picks the smaller one
and puts it back to the result array $A$, it then advances along the arrays respectively until either
one input array is exhausted. After that, the algorithm appends the rest of the elements in the other
input array to $A$.

\begin{algorithmic}[1]
\Procedure{Merge}{$A, X, Y$}
  \State $i \gets 1, j\gets 1, k\gets 1$
  \State $m \gets |X|, n \gets |Y|$
  \While{$i \leq m \land j \leq n$}
    \If{$X[i] < Y[j]$}
      \State $A[k] \gets X[i]$
      \State $i \gets i + 1$
    \Else
      \State $A[k] \gets Y[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i \leq m$}
    \State $A[k] \gets X[i]$
    \State $k \gets k + 1$
    \State $i \gets i + 1$
  \EndWhile
  \While{$j \leq n$}
    \State $A[k] \gets Y[j]$
    \State $k \gets k + 1$
    \State $j \gets j + 1$
  \EndWhile
\EndProcedure
\end{algorithmic}

Although this algorithm is a bit verbose, it can be short in some programming environment with enough tools
to manipulate array. The following Python program is an example.

\lstset{language=Python}
\begin{lstlisting}
def msort(xs):
    n = len(xs)
    if n > 1:
        ys = [x for x in xs[:n/2]]
        zs = [x for x in xs[n/2:]]
        ys = msort(ys)
        zs = msort(zs)
        xs = merge(xs, ys, zs)
    return xs

def merge(xs, ys, zs):
    i = 0
    while ys != [] and zs != []:
        xs[i] = ys.pop(0) if ys[0] < zs[0] else zs.pop(0)
        i = i + 1
    xs[i:] = ys if ys !=[] else zs
    return xs
\end{lstlisting}

\subsubsection{Performance}
\index{Merge Sort!Performance analysis}
Before dive into the improvement of this basic version, let's analyze the performance of merge sort.
The algorithm contains two steps, divide step, and merge step. In divide step, the sequence to be
sorted is always divided into two sub sequences with the same length. If we draw a similar partition tree
as what we did for quick sort, it can be found this tree is a perfectly balanced binary tree as shown in
figure \ref{fig:qsort-best}. Thus the height of this tree is $O(\lg n)$. It means the recursion depth
of merge sort is bound to $O(\lg n)$. Merge happens in every level. It's intuitive to analyze the
merge algorithm, that it compare elements from two input sequences in pairs, after one sequence is fully examined
the rest one is copied one by one to the result, thus it's a linear algorithm proportion to the length of
the sequence. Based on this facts, denote $T(n)$ the time for sorting the sequence with length $n$,
we can write the recursive time cost as below.

\be
\renewcommand*{\arraystretch}{2}
\begin{array}{rl}
T(n) & = \displaystyle T(\frac{n}{2}) + T(\frac{n}{2}) + c n \\
     & = \displaystyle 2 T(\frac{n}{2}) + c n
\end{array}
\ee

It states that the cost consists of three parts: merge sort the first half takes $T(\frac{n}{2})$,
merge sort the second half takes also $T(\frac{n}{2})$, merge the two results takes $c n$, where $c$
is some constant. Solve this equation gives the result as $O(n \lg n)$.

Note that, this performance doesn't vary in all cases, as merge sort always uniformly divides the input.

Another significant performance indicator is space occupation. However, it varies a lot in different
merge sort implementation. The detail space bounds analysis will be explained in every detailed variants
later.

For the basic imperative merge sort, observe that it demands same amount of spaces as the input array
in every recursion, copies the original elements to them for recursive sort, and these spaces can
be released after this level of recursion. So the peak space requirement happens when the recursion
enters to the deepest level, which is $O(n \lg n)$.

The functional merge sort consume much less than this amount, because the underlying data structure
of the sequence is linked-list. Thus it needn't extra spaces for merge\footnote{The complex effects
caused by lazy evaluation is ignored here, please refer to \cite{algo-fp} for detail}.
The only spaces requirement is for book-keeping the stack for recursive calls. This can be
seen in the later explanation of even-odd split algorithm.

\subsubsection{Minor improvement}
\index{Merge Sort!Work area allocation}
We'll next improve the basic merge sort bit by bit for both the functional and imperative realizations.
The first observation is that the imperative merge algorithm is a bit verbose. \cite{CLRS} presents
an elegant simplification by using positive $\infty$ as the sentinel. That we append $\infty$ as
the last element to the both ordered arrays for merging\footnote{For sorting in monotonic non-increasing order,
$-\infty$ can be used instead}. Thus we needn't test which array is not exhausted. Figure \ref{fig:merge-with-sentinel}
illustrates this idea.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-with-sentinel.ps}
 \caption{Merge with $\infty$ as sentinels.}
 \label{fig:merge-with-sentinel}
\end{figure}

\begin{algorithmic}[1]
\Procedure{Merge}{$A, X, Y$}
  \State \Call{Append}{$X, \infty$}
  \State \Call{Append}{$Y, \infty$}
  \State $i \gets 1, j\gets 1$
  \For{$k \gets$ from 1 to $|A|$}
    \If{$X[i] < Y[j]$}
      \State $A[k] \gets X[i]$
      \State $i \gets i + 1$
    \Else
      \State $A[k] \gets Y[j]$
      \State $j \gets j + 1$
    \EndIf
  \EndFor
\EndProcedure
\end{algorithmic}

The following ANSI C program imlements this idea. It embeds the merge inside. \verb|INF| is defined
as a big constant number with the same type of \verb|Key|. Where the type can either be defined elsewhere
or we can abstract the type information by passing the comparator as parameter. We skip these
implementation and language details here.

\lstset{language=C}
\begin{lstlisting}
void msort(Key* xs, int l, int u) {
    int i, j, m;
    Key *as, *bs;
    if (u - l > 1) {
        m = l + (u - l) / 2;  /* avoid int overflow */
        msort(xs, l, m);
        msort(xs, m, u);
        as = (Key*) malloc(sizeof(Key) * (m - l + 1));
        bs = (Key*) malloc(sizeof(Key) * (u - m + 1));
        memcpy((void*)as, (void*)(xs + l), sizeof(Key) * (m - l));
        memcpy((void*)bs, (void*)(xs + m), sizeof(Key) * (u - m));
        as[m - l] = bs[u - m] = INF;
        for (i = j = 0; l < u; ++l)
            xs[l] = as[i] < bs[j] ? as[i++] : bs[j++];
        free(as);
        free(bs);
    }
}
\end{lstlisting}

Running this program takes much more time than the quick sort. Besides the major reason we'll explain later,
one problem is that this version frequently allocates and releases memories for merging. While memory
allocation is one of the well known bottle-neck in real world as mentioned by Bentley in \cite{pearls}.
One solution to address this issue is to allocate another array with the same size to the original one
as the working area. The recursive sort for the first and second halves needn't allocate any more
extra spaces, but use the working area when merging. Finally, the algorithm copies
the merged result back.

This idea can be expressed as the following modified algorithm.

\begin{algorithmic}[1]
\Procedure{Sort}{A}
  \State $B \gets $ \Call{Create-Array}{$|A|$}
  \State \Call{Sort'}{$A, B, 1, |A|$}
\EndProcedure
\Statex
\Procedure{Sort'}{$A, B, l, u$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$
    \State \Call{Sort'}{$A, B, l, m$}
    \State \Call{Sort'}{$A, B, m + 1, u$}
    \State \Call{Merge'}{$A, B, l, m, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

This algorithm duplicates another array, and pass it along with the original array to be sorted
to \textproc{Sort'} algorithm. In real implementation, this working area should be released
either manually, or by some automatic tool such as GC (Garbage collection).
The modified algorithm \textproc{Merge'} also accepts a working area as parameter.

\begin{algorithmic}[1]
\Procedure{Merge'}{$A, B, l, m, u$}
  \State $i \gets l, j \gets m + 1, k \gets l$
  \While{$i \leq m \land j \leq u$}
    \If{$A[i] < A[j]$}
      \State $B[k] \gets A[i]$
      \State $i \gets i + 1$
    \Else
      \State $B[k] \gets A[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i \leq m$}
    \State $B[k] \gets A[i]$
    \State $k \gets k + 1$
    \State $i \gets i + 1$
  \EndWhile
  \While{$j \leq u$}
    \State $B[k] \gets A[j]$
    \State $k \gets k + 1$
    \State $j \gets j + 1$
  \EndWhile
  \For{$i \gets$ from $l$ to $u$} \Comment{Copy back}
    \State $A[i] \gets B[i]$
  \EndFor
\EndProcedure
\end{algorithmic}

By using this minor improvement, the space requirement reduced to $O(n)$ from $O(n \lg n)$.
The following ANSI C program implements this minor improvement. For illustration purpose,
we manually copy the merged result back to the original array in a loop. This can also
be realized by using standard library provided tool, such as \verb|memcpy|.

\lstset{language=C}
\begin{lstlisting}
void merge(Key* xs, Key* ys, int l, int m, int u) {
    int i, j, k;
    i = k = l; j = m;
    while (i < m && j < u)
        ys[k++] = xs[i] < xs[j] ? xs[i++] : xs[j++];
    while (i < m)
        ys[k++] = xs[i++];
    while (j < u)
        ys[k++] = xs[j++];
    for(; l < u; ++l)
        xs[l] = ys[l];
}

void msort(Key* xs, Key* ys, int l, int u) {
    int m;
    if (u - l > 1) {
        m = l + (u - l) / 2;
        msort(xs, ys, l, m);
        msort(xs, ys, m, u);
        merge(xs, ys, l, m, u);
    }
}

void sort(Key* xs, int l, int u) {
    Key* ys = (Key*) malloc(sizeof(Key) * (u - l));
    kmsort(xs, ys, l, u);
    free(ys);
}
\end{lstlisting}

This new version runs faster than the previous one. In my test machine, it speeds up about 20\% to 25\% when sorting
100,000 randomly generated numbers.

The basic functional merge sort can also be fine tuned. Observe that, it splits the list at the middle point. However,
as the underlying data structure to represent list is singly linked-list, random access at a given position is
a linear operation (refer to appendix A for detail). Alternatively, one can split the list in an even-odd manner.
That all the elements in even position are collected in one sub list, while all the odd elements are collected
in another. As for any lists, there are either same amount of elements in even and odd positions, or they
differ by one. So this divide strategy always leads to well splitting, thus the performance can be ensured
to be $O(n \lg n)$ in all cases.

The even-odd splitting algorithm can be defined as below.

\be
split(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, \Phi) & L = \Phi \\
  (\{ l_1 \}, \Phi) & |L| = 1 \\
  (\{ l_1 \} \cup A, \{ l_2 \} \cup B) & otherwise, (A, B) = split(L'')
  \end{array}
\right.
\ee

When the list is empty, the split result are two empty lists; If there is only one element in the list, we put this
single element, which is at position 1, to the odd sub list, the even sub list is empty; Otherwise, it means
there are at least two elements in the list, We pick the first one to the odd sub list, the second one to the
even sub list, and recursively split the rest elements.

All the other functions are kept same, the modified Haskell program is given as the following.

\lstset{language=Haskell}
\begin{lstlisting}
split [] = ([], [])
split [x] = ([x], [])
split (x:y:xs) = (x:xs', y:ys') where (xs', ys') = split xs
\end{lstlisting}

\section{In-place merge sort}
\index{Merge Sort!In-place merge sort}
One drawback for the imperative merge sort is that it requires extra spaces for merging, the basic version without
any optimization needs $O(n \lg n)$ in peak time, and the one by allocating a working area needs $O(n)$.

It's nature for people to seek the in-place version merge sort, which can reuse the original array without allocating
any extra spaces. In this section, we'll introduce some solutions to realize imperative in-place merge sort.

\subsection{Naive in-place merge}
\index{Merge Sort!Naive in-place merge}
The first idea is straightforward. As illustrated in figure \ref{fig:merge-in-place-naive}, sub list $A$, and $B$
are sorted, when performs in-place merge, the variant ensures that all elements before $i$ are merged, so that
they are in non-decreasing order; every time we compare the $i$-th and the $j$-th elements. If the $i$-th is less
than the $j$-th, the marker $i$ just advances one step to the next. This is the easy case. Otherwise, it
means that the $j$-th element is the next merge result, which should be put in front of $i$. In order
to achieve this, all elements between $i$ and $j$, including the $i$-th should be shift to the end by one cell.
We repeat this process till all the elements in $A$ and $B$ are put to the correct positions.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-in-place-naive.ps}
 \caption{Naive in-place merge}
 \label{fig:merge-in-place-naive}
\end{figure}

\begin{algorithmic}[1]
\Procedure{Merge}{$A, l, m, u$}
  \While{$l \leq m \land m \leq u$}
    \If{$A[l] < A[m]$}
      \State $l \gets l + 1$
    \Else
      \State $x \gets A[m]$
      \For{$i \gets m $ down-to $l+1$} \Comment{Shift}
        \State $A[i] \gets A[i-1]$
      \EndFor
      \State $A[l] \gets x$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}

However, this naive solution downgrades merge sort overall performance to quadratic $O(n^2)$! This is because
that array shifting is a linear operation. It is proportion to the length of elements in
the first sorted sub array which haven't been compared so far.

The following ANSI C program based on this algorithm runs very slow, that it takes about 12 times slower than
the previous version when sorting 10,000 random numbers.

\lstset{language=C}
\begin{lstlisting}
void naive_merge(Key* xs, int l, int m, int u) {
    int i; Key y;
    for(; l < m && m < u; ++l)
        if (!(xs[l] < xs[m])) {
            y = xs[m++];
            for (i = m - 1; i > l; --i) /* shift */
                xs[i] = xs[i-1];
            xs[l] = y;
        }
}

void msort3(Key* xs, int l, int u) {
    int m;
    if (u - l > 1) {
        m = l + (u - l) / 2;
        msort3(xs, l, m);
        msort3(xs, m, u);
        naive_merge(xs, l, m, u);
    }
}
\end{lstlisting}

\subsection{in-place working area}
\index{Merge Sort!In-place working area}
In order to implement the in-place merge sort in $O(n \lg n)$ time, when sorting a sub array, the rest part of
the array must be reused as working area for merging. As the elements stored in the working area, will be sorted
later, they can't be overwritten. We can modify the previous algorithm, which duplicates extra spaces for merging,
a bit to achieve this. The idea is that, every time when we compare the first elements in the two sorted sub
arrays, if we want to put the less element to the target position in the working area, we in-turn exchange what
sored in the working area with this element. Thus after merging the two sub arrays store what the working area
previously contains. This idea can be illustrated in figure \ref{fig:merge-workarea}.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-workarea.ps}
 \caption{Merge without overwriting working area.}
 \label{fig:merge-workarea}
\end{figure}

In our algorithm, both the two sorted sub arrays, and the working area for merging are parts of the
original array to be sorted. we need supply the following arguments when merging: the start points and end
points of the sorted sub arrays, which can be represented as ranges; and the start point of the working
area. The following algorithm for example, uses $[a, b)$ to indicate the range include $a$,
exclude $b$. It merges sorted range $[i, m)$ and range $[j, n)$ to the working area starts from $k$.

\begin{algorithmic}[1]
\Procedure{Merge}{$A, [i, m), [j, n), k$}
  \While{$i < m \land j < n$}
    \If{$A[i] < A[j]$}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
      \State $i \gets i + 1$
    \Else
      \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i < m$}
    \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
    \State $i \gets i + 1$
    \State $k \gets k + 1$
  \EndWhile
  \While{$j < m$}
    \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
    \State $j \gets j + 1$
    \State $k \gets k + 1$
  \EndWhile
\EndProcedure
\end{algorithmic}

Note that, the following two constraints must be satisfied when merging:

\begin{enumerate}
\item The working area should be within the bounds of the array. In other words, it should be big
enough to hold elements exchanged in without causing any out-of-bound error;
\item The working area can be overlapped with either of the two sorted arrays, however, it should
be ensured that there are not any unmerged elements being overwritten;
\end{enumerate}

This algorithm can be implemented in ANSI C as the following example.

\lstset{language=C}
\begin{lstlisting}
void wmerge(Key* xs, int i, int m, int j, int n, int w) {
    while (i < m && j < n)
        swap(xs, w++, xs[i] < xs[j] ? i++ : j++);
    while (i < m)
        swap(xs, w++, i++);
    while (j < n)
        swap(xs, w++, j++);
}
\end{lstlisting}

With this merging algorithm defined, it's easy to imagine a solution, which can sort
half of the array; The next question is, how to deal with the rest of the unsorted part
stored in the working area as shown in figure \ref{fig:merge-in-place-start}?

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-in-place-start.ps}
 \caption{Half of the array is sorted.}
 \label{fig:merge-in-place-start}
\end{figure}

One intuitive idea is to recursively sort another half of the working area, thus there are
only $\frac{1}{4}$ elements haven't been sorted yet. Which is shown in figure \ref{fig:merge-in-place-quater}.
The key point at this stage is that we must merge the sorted $\frac{1}{4}$ elements $B$
with the sorted $\frac{1}{2}$ elements $A$ sooner or later.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-in-place-quater.ps}
 \caption{$A$ and $B$ must be merged at sometime.}
 \label{fig:merge-in-place-quater}
\end{figure}

Is the working area left, which only holds $\frac{1}{4}$ elements, big enough for merging
$A$ and $B$? Unfortunately, it isn't in the settings shown in figure \ref{fig:merge-in-place-quater}.

However, the second constraint mentioned before gives us a hint, that we can exploit
it by arranging the working area to overlap with either sub array if we can ensure
the unmerged elements won't be overwritten under some well designed merging schema.

Actually, instead of sorting the second half of the working area, we can sort the first
half, and put the working area between the two sorted arrays as shown in figure \ref{fig:merge-in-place-setup} (a).
This setup effects arranging the working area to overlap with the sub array $A$. This idea
is proposed in \cite{msort-in-place}.

\begin{figure}[htbp]
 \centering
 \subfloat[]{\includegraphics[scale=0.8]{img/merge-in-place-setup.ps}} \\
 \subfloat[]{\includegraphics[scale=0.8]{img/merge-in-place-merged-quater.ps}}
 \caption{Merge $A$ and $B$ with the working area.}
 \label{fig:merge-in-place-setup}
\end{figure}

Let's consider two extreme cases:

\begin{enumerate}
\item All the elements in $B$ are less than any element in $A$. In this case, the merge algorithm
finally moves the whole contents of $B$ to the working area; the cells of $B$ holds what previously
stored in the working area; As the size of area is as same as $B$, it's OK to exchange their contents;
\item All the elements in $A$ are less than any element in $B$. In this case, the merge algorithm
continuously exchanges elements between $A$ and the working area. After all the previous $\frac{1}{4}$
cells in the working area are filled with elements from $A$, the algorithm starts to overwrite the
first half of $A$. Fortunately, the contents being overwritten are not those unmerged elements.
The working area is in effect advances toward the end of the array, and finally moves to the right
side; From this time point, the merge algorithm starts exchanging contents in $B$ with the working area.
The result is that the working area moves to the left most side which is shown in figure \ref{fig:merge-in-place-setup} (b).
\end{enumerate}

We can repeat this step, that always sort the second half of the unsorted part, and exchange
the sorted sub array to the first half as working area. Thus we keep reducing the working area
from $\frac{1}{2}$ of the array, $\frac{1}{4}$
of the array, $\frac{1}{8}$ of the array, ... The scale of the merge problem keeps reducing.
When there is only one element left in the working area, we needn't sort it any more since
the singleton array is sorted by nature. Merging a singleton array to the other is equivalent
to insert the element. In practice, the algorithm can finalize the last few
elements by switching to insertion sort.

The whole algorithm can be described as the following.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$
    \State $w \gets l + u - m$
    \State \Call{Sort'}{$A, l, m, w$} \Comment{The second half contains sorted elements}
    \While{$w - l > 1$}
      \State $u' \gets w$
      \State $w \gets \lceil \frac{l + u'}{2} \rceil$ \Comment{Ensure the working area is big enough}
      \State \Call{Sort'}{$A, w, u', l$} \Comment{The first half holds the sorted elements}
      \State \Call{Merge}{$A, [l, l + u' - w], [u', u], w$}
    \EndWhile
    \For{$i \gets w$ down-to $l$} \Comment{Switch to insertion sort}
      \State $j \gets i$
      \While{$j \leq u \land A[j] < A[j-1]$}
        \State \textproc{Exchange} $A[j] \leftrightarrow A[j-1]$
        \State $j \gets j + 1$
      \EndWhile
    \EndFor
  \EndIf
\EndProcedure
\end{algorithmic}

Note that in order to satisfy the first constraint, we must ensure the working area is big enough to hold
all exchanged in elements, that's way we round it by ceiling when sort the second half of the working area.
Note that we actually pass the ranges including the end points to the algorithm \textproc{Merge}.

Next, we develop a \text{Sort'} algorithm, which mutually recursive call \text{Sort} and exchange the result
to the working area.

\begin{algorithmic}[1]
\Procedure{Sort'}{$A, l, u, w$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$
    \State \Call{Sort}{$A, l, m$}
    \State \Call{Sort}{$A, m+1, u$}
    \State \Call{Merge}{$A, [l, m], [m+1, u], w$}
  \Else \Comment{Exchange all elements to the working area}
    \While{$l \leq u$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[w]$
      \State $l \gets l + 1$
      \State $w \gets w + 1$
    \EndWhile
  \EndIf
\EndProcedure
\end{algorithmic}

Different from the naive in-place sort, this algorithm doesn't shift the array during merging. The main
algorithm reduces the unsorted part in sequence of $\frac{n}{2}, \frac{n}{4}, \frac{n}{8}, ...$, it takes $O(\lg n)$ steps to complete
sorting. In every step, It recursively sorts half of the rest elements, and performs linear time merging.

Denote the time cost of sorting $n$ elements as $T(n)$, we have the following equation.

\be
T(n) = T(\frac{n}{2}) + c \frac{n}{2} + T(\frac{n}{4}) + c \frac{3n}{4} + T(\frac{n}{8}) + c \frac{7n}{8} + ...
\ee

Solving this equation by using telescope method, gets the result $O(n \lg n)$. The detailed process is
left as exercise to the reader.

The following ANSI C code completes the implementation by using the example \verb|wmerge| program given
above.

\lstset{language=C}
\begin{lstlisting}
void imsort(Key* xs, int l, int u);

void wsort(Key* xs, int l, int u, int w) {
    int m;
    if (u - l > 1) {
        m = l + (u - l) / 2;
        imsort(xs, l, m);
        imsort(xs, m, u);
        wmerge(xs, l, m, m, u, w);
    }
    else
        while (l < u)
            swap(xs, l++, w++);
}

void imsort(Key* xs, int l, int u) {
    int m, n, w;
    if (u - l > 1) {
        m = l + (u - l) / 2;
        w = l + u - m;
        wsort(xs, l, m, w); /* the last half contains sorted elements */
        while (w - l > 2) {
            n = w;
            w = l + (n - l + 1) / 2; /* ceiling */
            wsort(xs, w, n, l);  /* the first half contains sorted elements */
            wmerge(xs, l, l + n - w, n, u, w);
        }
        for (n = w; n > l; --n) /*switch to insertion sort*/
            for (m = n; m < u && xs[m] < xs[m-1]; ++m)
                swap(xs, m, m - 1);
    }
}
\end{lstlisting}

However, this program doesn't run faster than the version we developed in previous section, which doubles
the array in advance as working area. In my machine, it is about 60\% slower when sorting 100,000 random
numbers due to many swap operations.

\subsection{In-place merge sort vs. linked-list merge sort}
\index{Merge Sort!Linked-list merge sort}
The in-place merge sort is still a live area for research. In order to save the extra spaces for merging,
some overhead has be introduced, which increases the complexity of the merge sort algorithm. However, if
the underlying data structure isn't array, but linked-list, merge can be achieved without any extra spaces
as shown in the even-odd functional merge sort algorithm presented in previous section.

In order to make it clearer, we can develop a purely imperative linked-list merge sort solution.
The linked-list can be defined as a record type as shown in appendix A like below.

\lstset{language=C}
\begin{lstlisting}
struct Node {
    Key key;
    struct Node* next;
};
\end{lstlisting}

We can define an auxiliary function for node linking. Assume the list to be linked isn't empty, it
can be implemented as the following.

\lstset{language=C}
\begin{lstlisting}
struct Node* link(struct Node* xs, struct Node* ys) {
    xs->next = ys;
    return xs;
}
\end{lstlisting}

One method to realize the imperative even-odd splitting, is to initialize two empty sub lists.
Then iterate the list to be split. Every time, we link the current node in front of the
first sub list, then exchange the two sub lists. So that, the second sub list will be linked
at the next time iteration. This idea can be illustrated as below.

\begin{algorithmic}[1]
\Function{Split}{$L$}
  \State $(A, B) \gets (\Phi, \Phi)$
  \While{$L \neq \Phi$}
    \State $p \gets L$
    \State $L \gets $ \Call{Next}{$L$}
    \State $A \gets $ \Call{Link}{$p, A$}
    \State \textproc{Exchange} $A \leftrightarrow B$
  \EndWhile
  \State \Return $(A, B)$
\EndFunction
\end{algorithmic}

The following example ANSI C program implements this splitting algorithm embedded.

\lstset{language=C}
\begin{lstlisting}
struct Node* msort(struct Node* xs) {
    struct Node *p, *as, *bs;
    if (!xs || !xs->next) return xs;

    as = bs = NULL;
    while(xs) {
        p = xs;
        xs = xs->next;
        as = link(p, as);
        swap(as, bs);
    }
    as = msort(as);
    bs = msort(bs);
    return merge(as, bs);
}
\end{lstlisting}

The only thing left is to develop the imperative merging algorithm for linked-list. The idea
is quite similar to the array merging version. As long as neither of the sub lists is exhausted,
we pick the less one, and append it to the result list. After that, it just need link the
non-empty one to the tail the result, but not a looping for copying. It needs some carefulness
to initialize the result list, as its head node is the less one among the two sub lists.
One simple method is to use a dummy sentinel head, and drop it before returning. This implementation
detail can be given as the following.

\lstset{language=C}
\begin{lstlisting}
struct Node* merge(struct Node* as, struct Node* bs) {
    struct Node s, *p;
    p = &s;
    while (as && bs) {
        if (as->key < bs->key) {
            link(p, as);
            as = as->next;
        }
        else {
            link(p, bs);
            bs = bs->next;
        }
        p = p->next;
    }
    if (as)
        link(p, as);
    if (bs)
        link(p, bs);
    return s.next;
}
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item Proof the performance of in-place merge sort is bound to $O(n \lg n)$.
\end{itemize}
\end{Exercise}

\section{Nature merge sort}
\index{Merge Sort!Nature merge sort}
Knuth gives another way to interpret the idea of divide and conquer merge sort. It just likes
burn a candle in both ends \cite{TAOCP}. This leads to the nature merge sort algorithm.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/burn-candle-2-ends.eps}
 \caption{Burn a candle from both ends}
 \label{fig:burn-candle}
\end{figure}

For any given sequence, we can always find a non-decreasing sub sequence starts at any position.
One particular case is that we can find such a sub sequence from the left-most position. The following
table list some examples, the non-decreasing sub sequences are in bold font.

\begin{tabular}{ | l |}
\hline
{\bf 15 } , 0, 4, 3, 5, 2, 7, 1, 12, 14, 13, 8, 9, 6, 10, 11 \\
{\bf 8, 12, 14 }, 0, 1, 4, 11, 2, 3, 5, 9, 13, 10, 6, 15, 7 \\
{\bf 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 } \\
\hline
\end{tabular}

The first row in the table illustrates the worst case, that the second element is less than the first one,
so the non-decreasing sub sequence is a singleton list, which only contains the first element;
The last row shows the best case, the the sequence is ordered, and the non-decreasing list is the whole;
The second row shows the average case.

Symmetrically, we can always find a non-decreasing sub sequence from the end of the sequence
to the left. This indicates us that we can merge the two non-decreasing sub sequences, one
from the beginning, the other form the ending to a longer sorted sequence. The advantage of
this idea is that, we utilize the nature ordered sub sequences, so that we needn't recursive
sorting at all.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/nature-merge-sort.ps}
 \caption{Nature merge sort}
 \label{fig:nature-merge-sort}
\end{figure}

Figure \ref{fig:nature-merge-sort} illustrates this idea. We starts the algorithm by scanning
from both ends, finding the longest non-decreasing sub sequences respectively. After that,
these two sub sequences are merged to the working area. The merged result starts from beginning.
Next we repeat this step, which goes on scanning toward the center of the original sequence.
This time we merge the two ordered sub sequences to the right hand of the working area toward
the left. Such setup is easy for the next round of scanning. When all the elements
in the original sequence have been scanned and merged to the target, we switch to use the
elements stored in the working area for sorting, and use the previous sequence as new working area.
Such switching happens repeatedly in each round. Finally, we copy all elements from the
working area to the original array if necessary.

The only question left is when this algorithm stops. The answer is that when we start
a new round of scanning, and find that the longest non-decreasing sub list spans to the
end, which means the whole list is ordered, the sorting is done.

Because this kind of merge sort proceeds the target sequence in two ways, and uses the
nature ordering of sub sequences, it's named {\em nature two-way merge sort}. In order
to realize it, some carefulness must be paid. Figure \ref{fig:nature-msort-invariant}
shows the invariant during the nature merge sort. At anytime, all elements before marker
$a$ and after marker $d$ have been already scanned and merged. We are trying to
span the non-decreasing sub sequence $[a, b)$ as long as possible, at the same time,
we span the sub sequence from right to left to span $[c, d)$ as long as possible as well.
The invariant for the working area is shown in the second row. All elements before
$f$ and after $r$ have already been sorted. (Note that they may contain several
ordered sub sequences), For the odd times (1, 3, 5, ...), we merge $[a, b)$ and $[c, d)$
from $f$ toword right; while for the even times (2, 4, 6, ...), we merge the two
sorted sub sequences after $r$ toward left.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/nature-msort-invariant.ps}
 \caption{Invariant during nature merge sort}
 \label{fig:nature-msort-invariant}
\end{figure}

For imperative realization, the sequence is represented by array. Before sorting starts,
we duplicate the array to create a working area. The pointers $a, b$ are initialized to
point the left most position, while $c, d$ point to the right most position. Pointer $f$
starts by pointing to the front of the working area, and $r$ points to the rear
position.

\begin{algorithmic}[1]
\Function{Sort}{$A$}
  \If{$|A| > 1$}
    \State $n \gets |A|$
    \State $B \gets$ \Call{Create-Array}{$n$}  \Comment{Create the working area}
    \Loop
      \State $[a, b) \gets [1, 1)$
      \State $[c, d) \gets [n+1, n+1)$
      \State $f \gets 1, r \gets n$ \Comment{front and rear pointers to the working area}
      \State $t \gets $ False \Comment{merge to front or rear}
      \While{$b < c$} \Comment{There are still elements for scan}
        \Repeat \Comment{Span $[a, b)$}
          \State $b \gets b + 1$
        \Until{$b \geq c \lor A[b] < A[b-1]$}

        \Repeat \Comment{Span $[c, d)$}
          \State $c \gets c - 1$
        \Until{$c \leq b \lor A[c-1] < A[c]$}

        \If{$c < b$} \Comment{Avoid overlap}
          \State $c \gets b$
        \EndIf

        \If{$b - a \geq n$} \Comment{Done if $[a, b)$ spans to the whole array}
          \State \Return $A$
        \EndIf

        \If{$t$} \Comment{merge to front}
          \State $f \gets$ \Call{Merge}{$A, [a, b), [c, d), B, f, 1$}
        \Else \Comment{merge to rear}
          \State $r \gets$ \Call{Merge}{$A, [a, b), [c, d), B, r, -1$}
        \EndIf
        \State $a \gets b, d \gets c$
        \State $t \gets \lnot t$ \Comment{Switch the merge direction}
      \EndWhile
      \State \textproc{Exchange} $A \leftrightarrow B$ \Comment{Switch working area}
    \EndLoop
  \EndIf
  \State \Return $A$
\EndFunction
\end{algorithmic}

The merge algorithm is almost as same as before except that we need pass a parameter
to indicate the direction for merging.

\begin{algorithmic}[1]
\Function{Merge}{$A, [a, b), [c, d), B, w, \Delta$}
  \While{$a < b \land c < d$}
    \If{$A[a] < A[d-1]$}
      \State $B[w] \gets A[a]$
      \State $a \gets a + 1$
    \Else
      \State $B[w] \gets A[d-1]$
      \State $d \gets d - 1$
    \EndIf
    \State $w \gets w + \Delta$
  \EndWhile
  \While{$a < b$}
    \State $B[w] \gets A[a]$
    \State $a \gets a + 1$
    \State $w \gets w + \Delta$
  \EndWhile
  \While{$c < d$}
    \State $B[w] \gets A[d-1]$
    \State $d \gets d - 1$
    \State $w \gets w + \Delta$
  \EndWhile
  \State \Return $w$
\EndFunction
\end{algorithmic}

The following ANSI C program implements this two-way nature merge sort algorithm. Note that it
doesn't release the allocated working area explictly.

\lstset{language=C}
\begin{lstlisting}
int merge(Key* xs, int a, int b, int c, int d, Key* ys, int k, int delta) {
    for(; a < b && c < d; k += delta )
        ys[k] = xs[a] < xs[d-1] ? xs[a++] : xs[--d];
    for(; a < b; k += delta)
        ys[k] = xs[a++];
    for(; c < d; k += delta)
        ys[k] = xs[--d];
    return k;
}

Key* sort(Key* xs, Key* ys, int n) {
    int a, b, c, d, f, r, t;
    if(n < 2)
        return xs;
    for(;;) {
        a = b = 0;
        c = d = n;
        f = 0;
        r = n-1;
        t = 1;
        while(b < c) {
            do {      /* span [a, b) as much as possible */
                ++b;
            } while( b < c && xs[b-1] <= xs[b] );
            do{      /* span [c, d) as much as possible */
                --c;
            } while( b < c && xs[c] <= xs[c-1] );
            if( c < b )
                c = b;   /* eliminate overlap if any */
            if( b - a >= n)
                return xs;          /* sorted */
            if( t )
                f = merge(xs, a, b, c, d, ys, f, 1);
            else
                r = merge(xs, a, b, c, d, ys, r, -1);
            a = b;
            d = c;
            t = !t;
        }
        swap(&xs, &ys);
    }
    return xs; /*can't be here*/
}
\end{lstlisting}

The performance of nature merge sort depends on the actual ordering of the sub arrays. However, it in fact performs
well even in the worst case. Suppose that we are unlucky when scanning the array, that the length of the non-decreasing
sub arrays are always 1 during the first round scan. This leads to the result working area with merged ordered sub
arrays of length 2. Suppose that we are unlucky again in the second round of scan, however, the previous results
ensure that the non-decreasing sub arrays in this round are no shorter than 2, this time, the working area will
be filled with merged ordered sub arrays of length 4, ... Repeat this we get the length of the non-decreasing sub arrays
doubled in every round, so there are at most $O(\lg n)$ rounds, and in every round we scanned all the elements.
The overall performance for this worst case is bound to $O(n \lg n)$. We'll go back to this interesting phenomena
in the next section about bottom-up merge sort.

In purely functional settings however, it's not sensible to scan list from both ends since the underlying data
structure is singly linked-list. The nature merge sort can be realized in another approach.

Observe that the list to be sorted is consist of several non-decreasing sub lists, that we can pick every two
of such sub lists and merge them to a bigger one. We repeatedly pick and merge, so that the number of the
non-decreasing sub lists halves continuously and finally there is only one such list, which is the sorted
result. This idea can be formalized in the following equation.

\be
sort(L) = sort'(group(L))
\ee

Where function $group(L)$ groups the list into non-decreasing sub lists. This function can be described like
below, the first two are trivial edge cases.

\begin{itemize}
\item If the list is empty, the result is a list contains an empty list;
\item If there is only one element in the list, the result is a list contains a singleton list;
\item Otherwise, The first two elements are compared, if the first one is less than or equal to the second,
it is linked in front of the first sub list of the recursive grouping result; or a singleton list contains
the first element is set as the first sub list before the recursive result.
\end{itemize}

\be
group(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ L \} & |L| \leq 1 \\
  \{ \{ l_1 \} \cup L_1, L_2, ... \} & l_1 \leq l_2, \{ L_1, L_2, ...\} = group(L') \\
  \{ \{ l_1 \}, L_1, L_2, ... \} & otherwise
  \end{array}
\right.
\ee

It's quite possible to abstract the grouping criteria as a parameter to develop a generic grouping function,
for instance, as the following Haskell code \footnote{There is a `groupBy' function provided in
the Haskell standard library 'Data.List'. However, it doesn't fit here, because it accepts an equality
testing function as parameter, which must satisfy the properties of reflexive, transitive, and
symmetric. but what we use here, the less-than or equal to operation doesn't conform to transitive. Refer
to appendix A of this book for detail.}.

\lstset{language=Haskell}
\begin{lstlisting}
groupBy' :: (a->a->Bool) ->[a] ->[[a]]
groupBy' _ [] = [[]]
groupBy' _ [x] = [[x]]
groupBy' f (x:xs@(x':_)) | f x x' = (x:ys):yss
                         | otherwise = [x]:r
  where
    r@(ys:yss) = groupBy' f xs
\end{lstlisting}

Different from the $sort$ function, which sorts a list of elements, function $sort'$ accepts a list of
sub lists which is the result of grouping.

\be
sort'(\mathbb{L}) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & \mathbb{L} = \Phi \\
  L_1 & \mathbb{L} = \{ L_1 \} \\
  sort'(mergePairs(\mathbb{L})) & otherwise
  \end{array}
\right.
\ee

The first two are the trivial edge cases. If the list to be sorted is empty, the result is obviously empty;
If it contains only one sub list, then we are done. We need just extract this single sub list as result;
For the recursive case, we call a function $mergePairs$ to merge every two sub lists, then recursively call
$sort'$.

The next undefined function is $mergePairs$, as the name indicates, it repeatedly merges pairs of non-decreasing
sub lists into bigger ones.

\be
mergePairs(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L & |L| \leq 1 \\
  \{ merge(L_1, L_2) \} \cup mergePairs(L'') & otherwise
  \end{array}
\right.
\ee

When there are less than two sub lists in the list, we are done; otherwise, we merge the first two sub lists $L_1$ and $L_2$,
and recursively merge the rest of pairs in $L''$. The type of
the result of $mergePairs$ is list of lists, however, it will be flattened by $sort'$ function finally.

The $merge$ function is as same as before. The complete example Haskell program is given as below.

\lstset{language=Haskell}
\begin{lstlisting}
mergesort = sort' . groupBy' (<=)

sort' [] = []
sort' [xs] = xs
sort' xss = sort' (mergePairs xss) where
  mergePairs (xs:ys:xss) = merge xs ys : mergePairs xss
  mergePairs xss = xss
\end{lstlisting}

Alternatively, observing that we can first pick two sub lists, merge them to an intermediate result, then repeatedly
pick next sub list, and merge to this ordered result we've gotten so far until all the rest sub lists are merged.
This is a typical folding algorithm as introduced in appendix A.

\be
sort(L) = fold(merge, \Phi, group(L))
\ee

Translate this version to Haskell yields the folding version.

\lstset{language=Haskell}
\begin{lstlisting}
mergesort' = foldl merge [] . groupBy' (<=)
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
  \item Is the nature merge sort algorithm realized by folding is equivalent with the one by using $mergePairs$ in terms
of performance? If yes, prove it; If not, which one is faster?
\end{itemize}
\end{Exercise}

\section{Bottom-up merge sort}
\index{Merge Sort!Bottom-up merge sort}
The worst case analysis for nature merge sort raises an interesting topic, instead of realizing merge sort in
top-down manner, we can develop a bottom-up version. The great advantage is that, we needn't do book keeping
any more, so the algorithm is quite friendly for purely iterative implementation.

The idea of bottom-up merge sort is to turn the sequence to be sorted into $n$ small sub sequences each contains
only one element. Then we merge every two of such small sub sequences, so that we get $\frac{n}{2}$ ordered
sub sequences each with length 2; If $n$ is odd number, we left the last singleton sequence untouched.
We repeatedly merge these pairs, and finally we get the sorted result. Knuth names this variant as
`straight two-way merge sort' \cite{TAOCP}. The bottom-up merge sort is illustrated in figure \ref{fig:bottom-up-msort}

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.6]{img/bottom-up-msort.ps}
 \caption{Bottom-up merge sort}
 \label{fig:bottom-up-msort}
\end{figure}

Different with the basic version and even-odd version, we needn't explicitly split the list to be sorted
in every recursion. The whole list is split into $n$ singletons at the very beginning, and we merge these
sub lists in the rest of the algorithm.

\be
sort(L) = sort'(wraps(L))
\ee

\be
wraps(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  \{ \{l_1\} \} \cup wraps(L') & otherwise
  \end{array}
\right.
\ee

Of course $wraps$ can be implemented by using mapping as introduced in appendix A.

\be
sort(L) = sort'(map(\lambda_x \cdot \{ x \}, L))
\ee

We reuse the function $sort'$ and $mergePairs$ which are defined in section of nature merge sort. They
repeatedly merge pairs of sub lists until there is only one.

Implement this version in Haskell gives the following example code.

\lstset{language=Haskell}
\begin{lstlisting}
sort = sort' . map (\x->[x])
\end{lstlisting}

This version is based on what Okasaki presented in \cite{okasaki-book}. It is quite similar to the nature merge sort
only differs in the way of grouping. Actually, it can be deduced as a special case (the worst case) of
nature merge sort by the following equation.

\be
sort(L)= sort'(groupBy(\lambda_{x, y} \cdot False, L))
\ee

That instead of spanning the non-decreasing sub list as long as possible, the predicate always evaluates to false,
so the sub list spans only one element.

Similar with nature merge sort, bottom-up merge sort can also be defined by folding. The detailed implementation
is left as exercise to the reader.

Observing the bottom-up sort, we can find it's in tail-recursion call manner, thus it's quite easy
to translate into purely iterative algorithm without any recursion.

\begin{algorithmic}[1]
\Function{Sort}{$A$}
  \State $B \gets \Phi$
  \For{$\forall a \in A$}
    \State $B \gets$ \Call{Append}{$\{ a \}$}
  \EndFor
  \State $N \gets |B|$
  \While{$N > 1$}
    \For{$i \gets $ from $1$ to $\lfloor \frac{N}{2} \rfloor$}
      \State $B[i] \gets$ \Call{Merge}{$B[2i -1], B[2i]$}
    \EndFor
    \If{\Call{Odd}{$N$}}
      \State $B[\lceil \frac{N}{2} \rceil] \gets B[N]$
    \EndIf
    \State $N \gets \lceil \frac{N}{2} \rceil$
  \EndWhile
  \If{$B = \Phi$}
    \State \Return $\Phi$
  \EndIf
  \State \Return $B[1]$
\EndFunction
\end{algorithmic}

The following example Python program implements the purely iterative bottom-up merge sort.

\lstset{language=Python}
\begin{lstlisting}
def mergesort(xs):
    ys = [[x] for x in xs]
    while len(ys) > 1:
        ys.append(merge(ys.pop(0), ys.pop(0)))
    return [] if ys == [] else ys.pop()

def merge(xs, ys):
    zs = []
    while xs != [] and ys !=[]:
        zs.append(xs.pop(0) if xs[0] < ys[0] else ys.pop(0))
    return zs + (xs if xs !=[] else ys)
\end{lstlisting}

The Python implementation exploit the fact that instead of starting next round of merging
after all pairs have been merged, we can combine these rounds of merging by consuming
the pair of lists on the head, and appending the merged result to the tail. This greatly simply
the logic of handling odd sub lists case as shown in the above pseudo code.

\begin{Exercise}
\begin{itemize}
\item Implement the functional bottom-up merge sort by using folding.
\item Implement the iterative bottom-up merge sort only with array indexing. Don't use any library
supported tools, such as list, vector etc.
\end{itemize}
\end{Exercise}

\section{Parallelism}
\index{Parallel merge sort}
\index{Parallel quick sort}
We mentioned in the basic version of quick sort, that the two sub sequences can be sorted in
parallel after the divide phase finished. This strategy is also applicable for merge sort.
Actually, the parallel version quick sort and morege sort, do not only distribute
the recursive sub sequences sorting into two parallel processes, but divide the sequences into
$p$ sub sequences, where $p$ is the number of processors. Idealy, if we can achieve
sorting in $T'$ time with parallelism, which satisifies $O(n \lg n) = p T'$. We say it
is linear speed up, and the algorithm is parallel optimal.

However, a straightforward parallel extension to the sequential quick sort algorithm
which samples several pivots, divides $p$ sub sequences, and independantly
sorts them in parallel, isn't optimal. The bottleneck exists in the
divide phase, which we can only achive $O(n)$ time in average case.

The straightforward parallel extention to merge sort, on the other hand, block
at the merge phase. Both parallel merge sort and quick sort in practice need
good designes in order to achieve the optimal speed up. Actually, the divide and
conqure nature makes merge sort and quick sort relative easy for parallelisim.
Richard Cole found the $O(\lg n)$ parallel merge sort algorithm with $n$ processors
in 1986 in \cite{para-msort}.

Parallelism is a big and complex
topic which is out of the scope of this elementary book. Readers can refer to
\cite{para-msort} and \cite{para-qsort} for details.

\section{Short summary}
In this chapter, two popular divide and conquer sorting methods, quick sort and merge sort are introduced.
Both of them meet the upper performance limit of the comparison based sorting algorithms $O(n \lg n)$.
Sedgewick said that quick sort is the greatest algorithm invented in the 20th century. Almost
all programming environments adopt quick sort as the default sorting tool. As time goes on,
some environments, especially those manipulate abstract sequence which is dynamic and not based on
pure array switch to merge sort as the general purpose sorting tool\footnote{Actually, most of
them are kind of hybrid sort, balanced with insertion sort to achieve good performance when the
sequence is short}.

The reason for this interesting phenomena can be partly explained by the treatment in this chapter.
That quick sort performs perfectly in most cases, it needs fewer swapping than most other algorithms.
However, the quick sort algorithm is based on swapping, in purely functional settings, swapping isn't
the most efficient way due to the underlying data structure is singly linked-list, but not vectorized
array. Merge sort, on the other hand, is friendly in such environment, as it costs constant spaces,
and the performance can be ensured even in the worst case of quick sort, while the latter downgrade
to quadratic time. However, merge sort doesn't performs as well as quick sort in purely imperative
settings with arrays. It either needs extra spaces for merging, which is sometimes unreasonable, for
example in embedded system with limited memory, or causes many overhead swaps by in-place workaround.
In-place merging is till an active research area.

Although the title of this chapter is `quick sort vs. merge sort', it's not the case that one
algorithm has nothing to do with the other. Quick sort can be viewed as the optimized version of
tree sort as explained in this chapter. Similarly, merge sort can also be deduced from tree sort
as shown in \cite{sort-deriving}.

There are many ways to categorize sorting algorithms, such as in \cite{TAOCP}. One way is to
from the point of view of easy/hard partition, and easy/hard merge \cite{algo-fp}.

Quick sort, for example, is quite easy for merging, because all the elements in the sub
sequence before the pivot are no greater than any one after the pivot. The merging for
quick sort is actually trivial sequence concatenation.

Merge sort, on the other hand, is more complex in merging than quick sort. However, it's
quite easy to divide no matter what concrete divide method is taken:
simple divide at the middle point, even-odd splitting, nature splitting, or bottom-up
straight splitting. Compare to merge sort, it's more difficult for quick sort to
achieve a perfect dividing. We show that in theory, the worst case can't be completely
avoided, no matter what engineering practice is taken, median-of-three, random quick sort,
3-way partition etc.

We've shown some elementary sorting algorithms in this book till this chapter, including
insertion sort, tree sort, selection sort, heap sort, quick sort and merge sort. Sorting
is still a hot research area in computer science. At the time when I this chapter is written,
people are challenged by the buzz word `big data', that the traditional convenient method
can't handle more and more huge data within reasonable time and resources.
Sorting a sequence of hundreds of Gigabytes becomes a routine in some fields.

\begin{Exercise}
  \begin{itemize}
    \item Design an algorithm to create binary search tree by using merge sort strategy.
  \end{itemize}
\end{Exercise}

\begin{thebibliography}{99}

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)''. Addison-Wesley Professional; 2 edition (May 4, 1998) ISBN-10: 0201896850 ISBN-13: 978-0201896855

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein.
``Introduction to Algorithms, Second Edition''. ISBN:0262032937. The MIT Press. 2001

\bibitem{qsort-impl}
Robert Sedgewick. ``Implementing quick sort programs''. Communication of ACM. Volume 21, Number 10. 1978. pp.847 - 857.

\bibitem{pearls}
Jon Bentley. ``Programming pearls, Second Edition''. Addison-Wesley Professional; 1999. ISBN-13: 978-0201657883

\bibitem{3-way-part}
Jon Bentley, Douglas McIlroy. ``Engineering a sort function''. Software Practice and experience VOL. 23(11), 1249-1265 1993.

\bibitem{opt-qs}
Robert Sedgewick, Jon Bentley. ``Quicksort is optimal''. http://www.cs.princeton.edu/~rs/talks/QuicksortIsOptimal.pdf

\bibitem{fp-pearls}
Richard Bird. ``Pearls of functional algorithm design''. Cambridge University Press. 2010. ISBN, 1139490605, 9781139490603

\bibitem{algo-fp}
Fethi Rabhi, Guy Lapalme. ``Algorithms: a functional programming approach''. Second edition. Addison-Wesley, 1999. ISBN: 0201-59604-0

\bibitem{slpj}
Simon Peyton Jones. ``The Implementation of functional programming languages''. Prentice-Hall International, 1987. ISBN: 0-13-453333-X

\bibitem{msort-in-place}
Jyrki Katajainen, Tomi Pasanen, Jukka Teuhola. ``Practical in-place mergesort''. Nordic Journal of Computing, 1996.

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{sort-deriving}
Jos\`{e} Bacelar Almeida and Jorge Sousa Pinto. ``Deriving Sorting Algorithms''. Technical report, Data structures and Algorithms. 2008.

\bibitem{para-msort}
Cole, Richard (August 1988). ``Parallel merge sort''. SIAM J. Comput. 17 (4): 770-785. doi:10.1137/0217049. (August 1988)

\bibitem{para-qsort}
Powers, David M. W. ``Parallelized Quicksort and Radixsort with Optimal Speedup'', Proceedings of International Conference on Parallel Computing Technologies. Novosibirsk. 1991.

\bibitem{wiki-qs}
Wikipedia. ``Quicksort''. http://en.wikipedia.org/wiki/Quicksort

\bibitem{wiki-sweak-order}
Wikipedia. ``Strict weak order''. http://en.wikipedia.org/wiki/Strict\_weak\_order

\bibitem{wiki-total-order}
Wikipedia. ``Total order''. http://en.wokipedia.org/wiki/Total\_order

\bibitem{wiki-harmonic}
Wikipedia. ``Harmonic series (mathematics)''. http://en.wikipedia.org/wiki/Harmonic\_series\_(mathematics)

\end{thebibliography}

\ifx\wholebook\relax\else
\end{document}
\fi

% LocalWords:  ZF Lumoto's Burstle Okasaki
