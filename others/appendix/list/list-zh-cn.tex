\ifx\wholebook\relax \else
% ------------------------

\documentclass[UTF8]{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
\input{../../../common-zh-cn.tex}

\setcounter{page}{1}

\begin{document}

%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{链表}

\author{Larry~LIU~Xinyu
\thanks{{\bfseries Larry LIU Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\maketitle
\fi

\markboth{Sequences}{Elementary Algorithms}

\ifx\wholebook\relax
\chapter{链表}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{简洁}
\label{introduction}

在函数式环境中，本书集中、大量地使用了链表及各种递归操作。链表在函数式环境中的作用，如同数组在命令式环境中一样关键。他是众多函数式算法和数据结构的基石。

这一附录为不熟悉函数式列表的读者提供了一个快速参考。我们不仅为所有的基本操作提供了形式化的定义，还同时提供了函数式和命令式的例子实现。

除了列表的基本操作以外，这一附录还对一些重要的高阶函数概念加以解释，包括map和fold等。


% ================================================================
%                 Binary random access list
% ================================================================
\section{列表的定义}
\index{列表!定义}

类似于命令式环境中的数组，列表在函数式环境中扮演了关键的角色\footnote{有些读者可能会有不同意见，认为“lambda演算最关键”。Lambda演算相当于函数式计算中的“汇编语言”，从编程和计算的本质上来说，我们需要对它进行深入的研究。但是本书未能覆盖这一方面。读者可以参考\cite{mittype}了解更多细节。}。在某些编程语言中，例如Lisp语言家族，和ML语言家族，列表已被内部提供，使用者无需再自己定义列表。

列表，更精确地说是单向链表，是一种递归的数据结构。定义如下：

\begin{itemize}
\item 一个\underline{列表}或者为空；
\item 或者包含一个元素和一个\underline{列表}。
\end{itemize}

图\ref{fig:list-example}描述了一个含有$n$个节点的列表。每个节点包含两部分，一个元素（也称作key）和一个子列表。最后一个节点中的子列表为空，记为‘NIL’。

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.8]{img/list-example.ps}
  \caption{含有$n$个节点的列表。} \label{fig:list-example}
\end{figure}

在支持记录（或复合数据结构）概念的编程语言中，可以显示对这一数据结构进行定义。下面的C++例子代码定义了列表\footnote{这里使用模板仅仅是为了抽象列表元素的类型。除此之外，为避免涉及语言细节，所有的命令式例子代码都是C语言风格的。}。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
struct List {
  T key;
  List* next;
};
\end{lstlisting}

\subsection{空列表}
\index{列表!空}

这里需要对“空”列表的概念稍加说明。在支持nil概念的环境中，例如C或者Java类编程语言，空列表有两种不同的表示方法。一种是‘NIL’（根据语言不通可能为null或0等）；另外一种是非NIL的空列表$\{ \}$。后者通常分配有内存，但未填入内容。在Lisp的许多方言中，空通常表示为\texttt{'()}。在ML语言家族，空通常表示为\texttt{[]}。在本书中，我们在公式中使用符号$\phi$来表示空列表，而在伪代码中使用‘NIL’来表示空列表。

\subsection{获取元素和子列表}
\index{列表!head}
\index{列表!tail}

给定一个列表$L$，我们定义两个函数来分别获取列表中的元素和子列表。它们通常被命名为$first(L)$和$rest(L)$，或者$head(L)$和$tail(L)$。在Lisp中，由于历史原因，它们被命名为\texttt{car}和\texttt{cdr}用以代表当时机器中的寄存器\cite{SICP}。在支持模式匹配（pattern matching）的语言中（例如ML语言家族、Prolog、Erlang等），通常通过模式匹配\texttt{cons}来获取这两部分，我们稍后会介绍模式匹配。例如下面的Haskell例子程序：

\lstset{language=Haskell}
\begin{lstlisting}
head (x:xs) = x
tail (x:xs) = xs
\end{lstlisting}

如果使用上述的记录型语法，这两个函数可以通过访问记录中的项来实现\footnote{它们有时被命名为‘key’和‘next’，或者定义为对象的方法。}。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T first(List<T> *xs) { return xs->key; }

template<typename T>
List<T>* rest(List<T>* xs) { return xs->next; }
\end{lstlisting}

本书中，如果列表的内容为$L = \{ l_1, l_2, ..., l_n\}$，我们有时使用$L'$来表示$rest(L)$，用$l_1$来表示$first(L)$。

更有趣的一点是，只要环境支持递归，我们就可以定义列表。下面的例子定义了一个在编译期的整数列表。

\lstset{language=C++}
\begin{lstlisting}
struct Empty;

template<int x, typename T> struct List {
  static const int first = x;
  typedef T rest;
};
\end{lstlisting}

下面的代码在编译期构建了一个含有5个元素的列表$\{1, 2, 3, 4, 5\}$。

\begin{lstlisting}
typedef List<1, List<2, List<3, List<4 List<5, Empty> > > > > A;
\end{lstlisting}

\section{列表的基本操作}

\subsection{构建}
\index{列表!构建}
\index{列表!cons}

上面的C++模板元编程（tempalte meta programming）例子本质上是literate构建列表的形式。一个列表可以通过一个元素和一个子列表来构建。子列表可以为空。我们定义构造函数$cons(x, L)$。大量的Lisp方言都使用这一名称。在ML语言家族，‘cons’被定义为二元操作符\texttt{::}，在Haskell中，对应的操作符为\texttt{:}。

使用上述的C++例子定义，我们也可以定义相应的\texttt{cons}函数以构建列表结构。如下面的例子所示\footnote{通常被定义为类模板的构造函数，这里，我们将其定义为独立的函数。}。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* cons(T x, List<T>* xs) {
  List<T>* lst = new List<T>;
  lst->key = x;
  lst->next = xs;
  return lst;
}
\end{lstlisting}

\subsection{判空和长度计算}
\index{List!empty testing}
\index{List!length}

判断一个列表是否为空很简单。如果环境支持nil概念，判空时还要同时检测nil的情况。各种Lisp方言和ML语言家族都提供了\texttt{null}检测的函数。也可以通过和空列表进行模式匹配来判空。下面的Haskell例子程序使用模式匹配判断列表是否为空。

\lstset{language=Haskell}
\begin{lstlisting}
null [] = True
null _ = False
\end{lstlisting}

本书中，我们或者使用$empty(L)$，或者使用$L = \phi$来判断列表是否为空。

定义好如何判断列表为空后，就可以计算列表的长度了。在命令式环境中，\textproc{Length}通常实现如下：

\begin{algorithmic}
\Function{Length}{L}
  \State $n \gets 0$
  \While{$L \neq NIL$}
    \State $n \gets n + 1$
    \State $L \gets $ \Call{Next}{$L$}
  \EndWhile
\EndFunction
\end{algorithmic}

下面的C++例子程序实现了列表的长度计算。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
int length(List<T>* xs) {
  int n = 0;
  for (; xs; ++n, xs = xs->next);
  return n
}
\end{lstlisting}

但在纯函数式环境中，我们不能修改计数器变量。相应的思路为：如果列表为空，则长度为0；否则，我们递归求出子列表的长度，然后在加1，就是列表的最终长度。

\be
length(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & L = \phi \\
  1 + length(L') & otherwise
  \end{array}
\right.
\ee

这里$L' = rest(L)$，如果原列表含有$n$个元素，则$L'$表示$\{l_2, l_3, ..., l_n\}$。由于$L$和$L'$都可以为空$\phi$。在这一定义中，我们使用$=$号来判断列表$L$是否为空。为了计算列表的长度，我们需要从头到尾遍历全部元素，因此这一算法的时间复杂度和列表中存储的元素个数成正比，为$O(n)$。

下面的两个例子，分别是Haskell和Lisp方言Scheme的程序，它们实现了列表的长度计算。

\lstset{language=Haskell}
\begin{lstlisting}
length [] = 0
length (x:xs) = 1 + length xs
\end{lstlisting}

\lstset{language=Lisp}
\begin{lstlisting}
(define (length lst)
  (if (null? lst) 0 (+ 1 (length (cdr lst)))))
\end{lstlisting}

我们把如何判断两个列表相等留给读者作为练习。

\subsection{索引}
\index{列表!索引}
\index{列表!get at}

数组和列表（由单向链表实现的列表）的一个很大的不同是，数组支持常数时间的随机访问。很多编程语言支持使用\texttt{x[i]}的形式在常数时间$O(1)$内获取数组中的第$i$个元素。索引通常从0开始，但是也有例外，某些编程语言使用1作为第一个索引。本附录中，我们令索引从0开始。于数组不同，我们必须向前遍历$i$步以到达目标元素。这一遍历过程和长度计算类似。在命令式环境中，它同行表达如下。

\begin{algorithmic}[1]
\Function{Get-At}{$L, i$}
  \While{$i \neq 0$}
    \State $L \gets $ \Call{Next}{$L$}
  \EndWhile
  \State \Return \Call{First}{$L$}
\EndFunction
\end{algorithmic}

这一算法没有进行索引越界的错误处理。我们假设$0 \leq i < |L|$，其中$|L| = length(L)$。我们把错误处理作为练习留给读者。下面的C++例子代码实现了这一算法。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T getAt(List<T>* lst, int n) {
  while(n--)
    lst = lst->next;
  return lst->key;
}
\end{lstlisting}

在纯函数式环境中，我们使用递归而非循环来进行遍历。

\be
getAt(L, i) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  First(L) & i = 0 \\
  getAt(Rest(L), i-1) & otherwise
  \end{array}
\right.
\ee

为了\underline{获取第$i$个元素}，这一算法进行如下处理：
\begin{itemize}
\item 若$i$为0，结果为列表中的第一个元素；
\item 否则，结果为子列表中\underline{获取的第$i-1$个元素}。
\end{itemize}

下面的Haskell例子代码实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
getAt i (x:xs) = if i == 0 then x else getAt i-1 xs
\end{lstlisting}

这里，我们使用了模式匹配来确保列表不为空。这样就自然用匹配失败处理了越界错误。也就是说，若$i > |L|$，我们最终会到达一个边界情况，此时索引为$i - |L|$，而列表为空；另一方面，如果$i < 0$，继续减一使得索引变得更小，最终我们会得到同样的错误。此时索引为某一负数，而列表为空。

索引算法使用的时间和索引的大小成正比，为线性时间$O(i)$。本节只解释“读”语义，我们稍后会解释如果更改给定位置的元素。

\subsection{获取最后的元素}
\index{列表!last}
\index{列表!init}

虽然获取第一个元素和剩余的列表$L'$很简单，但是相反的操作——获取最后一个元素，以及出最后元素外的剩余列表却需要线性时间（如果不使用尾指针的话）。如果列表不为空，我们需要遍历到列表尾部以获取这两部分。下面是命令式的描述。

\begin{algorithmic}[1]
\Function{Last}{$L$}
  \State $x \gets $ NIL
  \While{$L \neq$ NIL}
    \State $x \gets $ \Call{First}{$L$}
    \State $L \gets $ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $x$
\EndFunction
\Statex
\Function{Init}{$L$}
  \State $L' \gets $ NIL
  \While{\Call{Rest}{$L$} $\neq$ NIL}
    \State $L' \gets$ \textproc{Append}($L'$, \Call{First}{$L$})
    \State $L \gets $ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $L'$
\EndFunction
\end{algorithmic}

算法假设输入的列表不为空，略去了相应的错误处理。其中\textproc{Init()}使用了append算法，我们将在稍后加以介绍。

下面的C++例子程序实现了这两个操作。为了优化性能，可以使用尾指针。我们将这一改进留给读者作为练习。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T last(List<T>* xs) {
  T x; /* Can be set to a special value to indicate empty list err. */
  for (; xs; xs = xs->next)
    x = xs->key;
  return x;
}

template<typename T>
List<T>* init(List<T>* xs) {
  List<T>* ys = NULL;
  for (; xs->next; xs = xs->next)
    ys = append(ys, xs->key);
  return ys;
}
\end{lstlisting}

这两个操作也可以用纯函数式的方式加以实现。当我们需要获取\underline{最后一个元素}时：

\begin{itemize}
\item 如果列表只含有一个元素（其子列表为空），结果就是列表中唯一的元素；
\item 否则，结果为子列表的\underline{最后一个元素}。
\end{itemize}

\be
last(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  First(L) & Rest(L) = \phi \\
  last(Rest(L)) & otherwise
  \end{array}
\right.
\ee

我们可以用类似的策略\underline{获取除最后一个元素外的剩余部分}。

\begin{itemize}
\item 边界情况：如果列表只含有一个元素，则结果为空列表；
\item 否则，我们首先从子列表中\underline{获取除最后一个元素外的剩余部分}，然后将当前列表的第一个元素附加在这一中间结果之前。
\end{itemize}

\be
init(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L' = \phi \\
  cons(l_1, init(L')) & otherwise
  \end{array}
\right.
\ee

这里，我们令$l_1$为$L$中的第一个元素，$L'$为剩余的部分。这一递归算法无需append操作，它本质上从右向左构造最终结果。我们稍后会介绍这类计算的高阶概念。

下面的Haskell例子程序使用模式匹配实现了$last()$和$init()$算法。

\lstset{language=Haskell}
\begin{lstlisting}
last [x] = x
last (_:xs) = last xs

init [x] = []
init (x:xs) = x : init xs
\end{lstlisting}

其中\texttt{[x]}匹配了列表只含有一个元素的模式，而\texttt{(\_:xs)}匹配了任何的非空列表。下划线（\texttt{\_}）表示我们并不关心这一元素的具体内容。读者可以参考Haskell的相关资料了解模式匹配的细节，例如\cite{learn-haskell}。

\subsection{反向索引}
\index{列表!反向索引}
\index{列表!rindex}

反向索引是$last()$操作的更一般形式，使用最小的内存空间，寻找单向链表中的倒数第$i$个元素是一道有趣的题目。不少公司使用它作为一道技术面试题目。最naive的解法需要遍历链表两轮。第一轮计算链表的长度$n$，然后计算出从左方算起的索引$n - i - 1$。第二轮遍历使用这一计算出的索引向前前进。这一思路定义如下：

\[
  getAtR(L, i) = getAt(L, length(L) - i -1)
\]

存在更好的命令式解法。简单起见，我们忽略越界之类的错误处理。思路是使用两个指针$p_1$和$p_2$，它们相距$i$步，即$rest^i(p_2) = p_1$，其中$rest^i(p_2)$表示重复执行函数$rest()$总共$i$次。也就是说，从$p_2$前进$i$步就可到达$p_1$。我们可以让$p_2$一开始指向链表的头部，然后同时向前移动它们，知道其中之一（$p_1$）到达链表的尾部。此时指针$p_2$恰好指向倒数第$i$个元素。图\ref{fig:list-rindex}描述了这一思路。

\begin{figure}[htbp]
    \centering
    \subfloat[$p_2$开始时指向表头，它在指针$p_1$之后，距离$i$步。]{\includegraphics[scale=0.8]{img/list-rindex.ps}} \\
    \subfloat[当$p_1$到达表尾时，$p_2$恰好指向从右数第$i$个元素。]{\includegraphics[scale=0.8]{img/list-rindex-2.ps}}
    \caption{用双指针法解决反向索引问题。} \label{fig:list-rindex}
\end{figure}

下面的算法描述了这一“双指针法”解法。

\begin{algorithmic}[1]
\Function{Get-At-R}{$L, i$}
  \State $p \gets L$
  \While{$i \neq 0$}
    \State $L \gets $ \Call{Rest}{$L$}
    \State $i \gets i - 1$
  \EndWhile
  \While{\Call{Rest}{$L$} $\neq$ NIL}
    \State $L \gets$ \Call{Rest}{$L$}
    \State $p \gets$ \Call{Rest}{$p$}
  \EndWhile
  \State \Return \Call{First}{$p$}
\EndFunction
\end{algorithmic}

下面的C++例子代码使用“双指针”法实现了从右侧的索引算法。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T getAtR(List<T>* xs, int i) {
  List<T>* p = xs;
  while(i--)
    xs = xs->next;
  for(; xs->next; xs = xs->next, p = p->next);
  return p->key;
}
\end{lstlisting}

也可以用递归的方式实现这一思路。如果要获取列表$L$的倒数第$i$个元素，我们可以同时检查$L$和$S=\{l_i, l_{i+1}, ..., l_n\}$这两个列表，其中$S$是$L$中除去前$i$个元素后的子列表。

\begin{itemize}
\item 边界条件：如果$S$中仅含有一个元素，则倒数第$i$个元素就是$L$中的第一个元素；
\item 否则，我们同时从$L$和$S$中各丢弃一个元素，然后递归地检查列表$L'$和$S'$。
\end{itemize}

这一算法描述可以形式化为下面的公式。

\be
getAtR(L, i) = examine(L, drop(i, L))
\ee

其中数函$examine(L, S)$的定义如下。

\be
examine(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  first(L) & |S| = 1 \\
  examine(rest(L), rest(S)) & otherwise
  \end{array}
\right.
\ee

我们稍后在列表修改操作的部分会详细介绍$drop()$函数。这里暂时可以实现为重复调用$rest()$函数一定的次数。

\[
drop(n, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L & n = 0 \\
  drop(n - 1, rest(L)) & otherwise
  \end{array}
\right.
\]

下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
atR :: [a] -> Int -> a
atR xs i = get xs (drop i xs) where
  get (x:_) [_] = x
  get (_:xs) (_:ys) = get xs ys
  drop n as@(_:as') = if n == 0 then as else drop (n-1) as'
\end{lstlisting}

这里我们使用特殊变量\texttt{\_}作为placeholder，来表示我们不关心的内容。

\subsection{修改}
\index{列表!修改}

严格来说，我们无法在纯函数环境下对列表进行修改。和命令式环境不同，我们实际上通过创建新的列表来实现修改。几乎所有的函数式环境都支持垃圾回收机制，原始列表或者被保留（persisit）复用，或者在某一时刻被释放（参考\cite{okasaki-book}中的第2章）。

\subsubsection{添加（Append）}
\index{列表!append}

函数$cons$可以被认为是通过在列表头部插入元素构建列表。如果将若干$cons$操作串联起来，就可以从右向左构建出一个列表。添加操作，是向列表的尾部加入元素。$cons$操作只需要常数时间$O(1)$，而添加时，我们必须遍历到列表的尾部以获取到插入的位置。这意味着添加操作的复杂度为$O(n)$，其中$n$是列表的长度。为了提高添加的效率，命令式实现通常使用一个尾指针变量，它总是记录列表尾部的位置，这样在添加时就无需遍历列表。但是，在纯函数环境中，我们无法使用这样的尾指针，只能通过递归来实现添加。

\be
append(L, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ x \} & L = \phi \\
  cons(first(L), append(rest(L), x)) & otherwise
  \end{array}
\right.
\ee

这一算法分别处理两种不同的添加情况：
\begin{itemize}
\item 若列表为空，结果列表只含有一个元素，就是待添加的$x$。我们通常用简记法$\{ x \} = cons(x, \phi)$表示对一个元素和一个空列表$\phi$进行$cons$操作；
\item 否则，若列表不为空，则结果可以这样构造：我们将$L$中的第一个元素取出，递归地将待加入的元素$x$添加到剩余的子列表中，然后通过$cons$再将第一个元素和递归添加的结果链接起来。
\end{itemize}

对于非边界情况，我们记$L= \{l_1, l_2, ... \}$，除第一个元素的剩余部分记为$L' = \{ l_2, l_3, ...\}$。上述公式可以简记为：

\be
append(L, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ x \} & L = \phi \\
  cons(l_1, append(L', x)) & otherwise
  \end{array}
\right.
\ee

本附录中，这两种记法都会被用到。

下面的Lisp方言Scheme例子程序实现了这一算法。

\lstset{language=Lisp}
\begin{lstlisting}
(define (append lst x)
  (if (null? lst)
      (list x)
      (cons (car lst) (append (cdr lst) x))))
\end{lstlisting}

不使用尾指针，在命令式环境下，我们需要遍历到列表的尾部然后将新元素加入。

\begin{algorithmic}[1]
\Function{Append}{$L, x$}
  \If{$L = $ NIL}
    \State \Return \Call{Cons}{$x$, NIL}
  \EndIf
  \State $H \gets L$
  \While{\Call{Rest}{$L$} $\neq$ NIL}
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Call{Rest}{$L$} $\gets$ \Call{Cons}{$x$, NIL}
  \State \Return $H$
\EndFunction
\end{algorithmic}

下面的C++例子程序实现了这一算法。我们将如何使用尾指针来提高添加的性能的实现作为练习留给读者。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* append(List<T>* xs, T x) {
  List<T> *tail, *head;
  for (head = tail = xs; xs; xs = xs->next)
    tail = xs;
  if (!head)
    head = cons<T>(x, NULL);
  else
    tail->next = cons<T>(x, NULL);
  return head;
}
\end{lstlisting}

\subsubsection{修改指定位置上的元素}
\index{列表!set at}

虽然我们定义了随机访问算法$getAt(L, i)$，但是在纯函数环境下，我们无法直接修改这一函数返回的元素。大多数命令式环境和一些函数式环境提供了引用语义，读者可以参考\cite{mittype}了解详细的实现。下面的C++例子程序返回指定位置上元素的引用而不是值。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T& getAt(List<T>* xs, int n) {
  while (n--)
    xs = xs->next;
  return xs->key;
}
\end{lstlisting}

在下面的例子中，我们使用这一函数来修改列表中的第二个元素。

\begin{lstlisting}
List<int>* xs = cons(1, cons(2, cons<int>(3, NULL)));
getAt(xs, 1) = 4;
\end{lstlisting}

在非纯函数环境中，例如Lisp方言Scheme，我们可以直接将新的值写入到第$i$个元素所引用的cell中。

\lstset{language=Lisp}
\begin{lstlisting}
(define (set-at! lst i x)
  (if (= i 0)
      (set-car! lst x)
      (set-at! (cdr lst) (- i 1) x)))
\end{lstlisting}

这一算法首先检查索引值$i$是否为0，如果是，它就直接修改列表中的第一个元素为指定的值$x$；否则，它将索引值$i$减一，然后递归再除第一个元素的剩余列表中，将新索引位置上的元素修改为$x$。这一函数并不返回一个有意义的值，它主要以副作用（side-effect）产生结果。下面的例子修改了列表中的第二个元素。

\begin{lstlisting}
(define lst '(1 2 3 4 5))
(set-at! lst 1 4)
(display lst)

(1 4 3 4 5)
\end{lstlisting}

为了实现一个纯函数式的$setAt(L, i, x)$算法，我们要避免直接修改cell，而是创建一个新列表。

\begin{itemize}
\item 边界条件：如果要修改的是第一个元素（$i = 0$），我们创建一个新列表，第一个元素是修改到的新值，剩余部分是原列表中除第一个元素外的其余元素；
\item 否则，我们创建一个新列表，第一个元素和以前的一样，剩余的部分中的第$i-1$个元素被修改为新值。
\end{itemize}

这一递归描述可以形式化为下面的定义。

\be
setAt(L, i, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  cons(x, L') & i = 0 \\
  cons(l_1, setAt(L', i-1, x)) & otherwise
  \end{array}
\right.
\ee

下面的Lisp方言Scheme例子程序实现了这一算法，和此前的例子对比可以发现它们之间的不同。

\lstset{language=Lisp}
\begin{lstlisting}
(define (set-at lst i x)
  (if (= i 0)
      (cons x (cdr lst))
      (cons (car lst) (set-at (cdr lst) (- i 1) x))))
\end{lstlisting}

这里我们跳过了越界的错误处理。同样，和随机访问算法类似，由于需要遍历列表以定位到待插入的位置，它的性能为线性时间。

\subsubsection{插入}
\index{列表!插入}
\index{List!insert at}

列表插入有两个不同的含义。一个是指在指定位置插入一个元素，可以表示为$insert(L, i, x)$，它的实现和$setAt(L, i, x)$类似；另外一个含义是指在一个已序列表中插入一个元素，使得结果列表仍然是已序的。

我们首先考虑如何在指定位置$i$插入一个元素$x$。很明显，我们需要先遍历$i$个元素以达到待插入的位置，接下来需要构造一个新的子列表，其中$x$是这一个子列表的第一元素。最后，我们将这一新子列表附加到前$i$个元素的后面，从而构造出最终的结果列表。

这一算法可以描述如下。如果要将元素$x$插入到列表$L$的第$i$个位置：

\begin{itemize}
\item 边界情况：若$i$为0，插入就转变成了‘cons’操作：$cons(x, L)$；
\item 否则，我们递归地将$x$\underline{插入}到子列表$L'$的第$i-1$个位置，然后将原列表的第一元素和这一递归插入的结果构造为最终结果。
\end{itemize}

这一算法可以形式化为下面的定义。

\be
insert(L, i, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  cons(x, L) & i = 0 \\
  cons(l_1, insert(L', i-1, x)) & otherwise
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
insert xs 0 y = y:xs
insert (x:xs) i y = x : insert xs (i-1) y
\end{lstlisting}

这一算法并未处理越界错误，我们也可以认为，当$i$超过列表的长度时，实际含义为添加。读者可以在本节的练习中考虑这一处理。

这一算法也可以用命令式的方式实现。如果待插入的位置为0，就用新元素作为第一个，并构造一个新列表；否则，我们记录下列表的头指针，然后连续遍历$i$步。我们还需要一个额外的变量以记录插入操作前的位置。

\begin{algorithmic}[1]
\Function{Insert}{$L, i, x$}
  \If{$i = 0$}
    \State \Return \Call{Cons}{$x, L$}
  \EndIf
  \State $H \gets L$
  \State $p \gets L$
  \While{$i \neq 0 $}
    \State $p \gets L$
    \State $L \gets $ \Call{Rest}{$L$}
    \State $i \gets i - 1$
  \EndWhile
  \State \Call{Rest}{$p$} $\gets$ \Call{Cons}{$x, L$}
  \State \Return $H$
\EndFunction
\end{algorithmic}

下面的C++例子程序实现了这一算法。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* insert(List<T>* xs, int i , int x) {
  List<T> *head, *prev;
  if (i == 0)
    return cons(x, xs);
  for (head = xs; i; --i, xs = xs->next)
    prev = xs;
  prev->next = cons(x, xs);
  return head;
}
\end{lstlisting}

如果列表$L$已序，即对任何位置$1 \leq i \leq j \leq n$，我们有$l_i \leq l_j$。我们可以设计一个算法，使得新元素$x$插入后，结果列表仍然已序。

\be
insert(x, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  cons(x, \phi) & L = \phi \\
  cons(x, L) & x < l_1 \\
  cons(l_1, insert(x, L')) & otherwise
  \end{array}
\right.
\ee

当讲元素$x$插入到已序列表$L$时：

\begin{itemize}
\item 若$L$为空，胡总和$x$小于$L$中的第一个元素，我们将$x$置于$L$中所有元素之前构造一个新列表；
\item 否则，我们递归地将元素$x$插入到子列表$L'$中。
\end{itemize}

下面的Haskell例子程序实现了这一算法。这里我们使用了小于等于（$\leq$）来决定元素间的顺序。实际上，这一条件可以放松为严格小于（$<$）。也就是说，只要可以用$<$来比较元素，就可以设计一个算法，使得插入新元素后列表仍然已序。读者可以参考本书关于排序的章节来了解“已序”概念的更多内容。

\lstset{language=Haskell}
\begin{lstlisting}
insert y [] = [y]
insert y xs@(x:xs') = if y <= x then y : xs else x : insert y xs'
\end{lstlisting}

由于算法需要逐一比较元素，它的负责度为线性时间。这里我们使用了模式匹配的‘as’记法。读者可以参考\cite{learn-haskell}和\cite{algo-fp}了解这一语言特性。

这一按序插入算法也可以用命令式的方式实现如下\footnote{读者可以参考本书“插入排序的进化”一章中给出的另一个版本，它们略有不同。}。

\begin{algorithmic}[1]
\Function{Insert}{$x, L$}
  \If{$L = \phi \lor x <$ \Call{First}{$L$}}
    \State \Return \Call{Cons}{$x, L$}
  \EndIf
  \State $H \gets L$
  \While{\Call{Rest}{$L$} $\neq \phi \land $ \textproc{First}(\Call{Rest}{$L$}) $< x$}
    \State $L \gets $ \Call{Rest}{$L$}
  \EndWhile
  \State \Call{Rest}{$L$} $\gets$ \textproc{Cons}($x$, \Call{Rest}{$L$})
  \State \Return $H$
\EndFunction
\end{algorithmic}

若列表为空，或者新元素小于列表中的第一个元素，我们将新元素置于列表之前；否则，我们记录下表头，然后遍历列表，直到到达一个位置，使得新元素$x$小于剩余子列表中的元素，并将$x$放置于这一位置。和此前的“insert at”算法相比，我们并未使用变量$p$在遍历过程中记录前一个元素的位置。

下面的C++例子程序实现了这一算法。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* insert(T x, List<T>* xs) {
  List<T> *head;
  if (!xs || x < xs->key)
    return cons(x, xs);
  for (head = xs; xs->next && xs->next->key < x; xs = xs->next);
  xs->next = cons(x, xs->next);
  return head;
}
\end{lstlisting}

使用这一线性时间的按序插入算法，我们可以实现一个平方时间的插入排序。我们逐一将元素按序插入到一个空列表中。

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  insert(l_1, sort(L')) & otherwise
  \end{array}
\right.
\ee

若待排序列表为空，则结果仍为空列表；否则，我们首先递归地将除第一个元素外的剩余部分排好序，然后再将第一个元素按序插入到这一结果中。

下面的Haskell例子程序实现这一插入排序算法。

\lstset{language=Haskell}
\begin{lstlisting}
isort [] = []
isort (x:xs) = insert x (isort xs)
\end{lstlisting}

命令式的链表排序可以描述如下。我们首先将结果列表初始化为空，然后逐一从列表中取出元素并按序插入到结果列表中。

\begin{algorithmic}[1]
\Function{Sort}{$L$}
  \State $L' \gets \phi$
  \While{$L \neq \phi$}
    \State $L' \gets$ \textproc{Insert}(\Call{First}{$L$}, $L'$)
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $L'$
\EndFunction
\end{algorithmic}

在循环中的任何时刻，结果列表都是已序的。和前面的递归算法相比，它们有一个本质不同，前者从右向左处理列表，而后者从左向右处理。我们稍后将在“尾递归”一节中讲述如何消除这一差异。

下面的C++例子程序实现了这一链表插入排序算法。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* isort(List<T>* xs) {
  List<T>* ys = NULL;
  for(; xs; xs = xs->next)
    ys = insert(xs->key, ys);
  return ys;
}
\end{lstlisting}

本书专门安排了一章来详细讨论插入排序。包括性能分析和各种改进。

\subsubsection{删除}
\index{列表!删除}
\index{List!delete at}

在纯函数式环境中，由于并不能真正修改列表，所以并没有删除的概念，由于数据被保留（persist），删除的语义实际是创建一个包含所有剩余元素的一个新列表。

和插入类似，删除也有两个不同的含义。一个是删除给定位置上的元素；另一个是查找指定值的元素并删除。前者可以表示为$delete(L, i)$，后者可以表示为$delete(L, x)$。

为了实现$delete(L,i)$（或‘delete at’）算法，我们可以使用类似于随机访问和插入的思路。首先遍历列表到达指定的位置，然后构造一个新列表，包含所有已遍历的元素，跳过下一个尚未遍历的元素，最后将剩余元素也包含进来。

删除过程可以递归进行实现，为了\underline{从列表$L$中删除第$i$个元素}，
\begin{itemize}
\item 若$i$为0，即删除列表中的第一个元素，结果为除第一元素外的剩余部分；
\item 若列表为空，则结果仍为空列表；
\item 否则，我们递归\underline{从子列表$L'$中删除第$i-1$个元素}，然后用$L$中的第一个元素和这一递归删除的结果构造最终的结果列表。
\end{itemize}

这里有两种边界情况，其中第二种主要用来进行错误处理。这一算法的形式化定义如下：

\be
delete(L, i) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L' & i = 0 \\
  \phi & L = \phi \\
  cons(l_1, delete(L', i-1))
  \end{array}
\right.
\ee

其中$L' = rest(L)$、$l_1 = first(L)$。下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
del (_:xs) 0 = xs
del [] _ = []
del (x:xs) i = x : del xs (i-1)
\end{lstlisting}

这同样是一个线性时间的算法。还有其他一些实现方法，例如我们可以首先在$i-1$的位置，将列表分成两部分$L_1$和$L_2$，然后去掉第二部分中的第一个元素，将$L_1$和$L_2'$连接到一起。

也可以用命令式的方式实现在指定位置删除元素的操作，我们需要通过循环遍历到这个位置：

\begin{algorithmic}[1]
\Function{Delete}{$L, i$}
  \If{$i = 0$}
    \State \Return \Call{Rest}{$L$}
  \EndIf
  \State $H \gets L$
  \State $p \gets L$
  \While{$i \neq 0$}
    \State $i \gets i - 1$
    \State $p \gets L$
    \State $L \gets $ \Call{Rest}{$L$}
  \EndWhile
  \State \Call{Rest}{$p$} $\gets$ \Call{Rest}{$L$}
  \State \Return $H$
\EndFunction
\end{algorithmic}

这里略过了越界错误处理。在不支持垃圾回收的环境中，还需要释放被删除元素所占用的空间。下面的C++例子程序在删除后释放了节点。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* del(List<T>* xs, int i) {
  List<T> *head, *prev;
  if (i == 0)
    head = xs->next;
  else {
    for (head = xs; i; --i, xs = xs->next)
      prev = xs;
    prev->next = xs->next;
  }
  xs->next = NULL;
  delete xs;
  return head;
}
\end{lstlisting}

这里使用了\texttt{xs->next = NULL}来避免在释放节点所占空间时递归释放掉链表剩余的部分。

“查找并删除”的语义可以进一步细分为两种情况，一种是仅仅找到第一个出现的元素，并将其从列表中删除；另外一种是找到\underline{所有}等于指定值的元素，并将它们全部删除。后者是更加通用的情况，可以对第一种情况略作修改加以实现。我们将其作为练习留给读者。

我们按照“查找并删除”，而非“查找然后删除”来实现这一算法，通过一轮遍历完成查找和删除两个操作。

\begin{itemize}
\item 如果列表为空，则结果显然也是空列表；
\item 如果列表不为空，首先检查第一个元素，如果它恰好等于要删除的值，则结果等于列表的剩余部分；
\item 否则，我们取出第一个元素，然后递归地在剩余部分删除指定的值，然后在将取出的第一个元素放在这一结果的前面。
\end{itemize}

这一算法可以形式化为下面的定义。

\be
delete(L, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  L' & l_1 = x \\
  cons(l_1, delete(L', x)) & otherwise
  \end{array}
\right.
\ee

由于需要遍历列表以查找待删除的元素，这一算法的复杂度为线性时间。下面的Haskell例子程序实现了这一算法。其中第一个边界条件使用模式匹配来处理，其余两种情况由if-else表达式处理。

\lstset{language=Haskell}
\begin{lstlisting}
del [] _ = []
del (x:xs) y = if x == y then xs else x : del xs y
\end{lstlisting}

此前的命令式算法中，大都跳过了错误处理。但是“查找并删除”时，必须要处理待查找的值不存在的情况。

\begin{algorithmic}[1]
\Function{Delete}{$L, x$}
  \If{$L = \phi$} \Comment{空列表}
    \State \Return $\phi$
  \EndIf
  \If{\Call{First}{$L$} $= x$}
    \State $H \gets$ \Call{Rest}{$L$}
  \Else
    \State $H \gets L$
    \While{$L \neq \phi \land$ \Call{First}{$L$} $\neq x$} \Comment{列表不为空}
      \State $p \gets L$
      \State $L \gets$ \Call{Rest}{$L$}
    \EndWhile
    \If{$L \neq \phi$} \Comment{找到}
      \State \Call{Rest}{$p$} $\gets$ \Call{Rest}{$L$}
    \EndIf
  \EndIf
  \State \Return $H$
\EndFunction
\end{algorithmic}

如果列表为空，结果仍为空列表；否则，算法遍历列表直到发现一个元素等于待删除的值，或者到达列表末尾。如果找到了这样的元素，就将其从列表中去掉。下面的C++例子程序实现了这一算法。这里我们释放掉了被删除元素所占的存储空间。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* del(List<T>* xs, T x) {
  List<T> *head, *prev;
  if (!xs)
    return xs;
  if (xs->key == x)
    head = xs->next;
  else {
    for (head = xs; xs && xs->key != x; xs = xs->next)
      prev = xs;
    if (xs)
      prev->next = xs->next;
  }
  if (xs) {
    xs->next = NULL;
    delete xs;
  }
  return head;
}
\end{lstlisting}

\subsubsection{连接}
\label{concat}
\index{列表!连接}

连接可以认为是添加操作的更一般形式，添加每次向列表尾部加入一个元素，而连接向列表尾部一次加入多个元素。

但是，如果通过多次添加来实现连接，则整体操作的性能不佳，为平方级别。考虑下面的实现。

\[
concat(L_1, L_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L_1 & L_2 = \phi \\
  concat(append(L_1, first(L_2)), rest(L_2)) & otherwise
  \end{array}
\right.
\]

每次添加都需要遍历到链表的尾部，一共需要$|L_2|$次遍历。总体性能为$O(|L_1| + (|L_1| + 1) + ... + (|L_1| + |L_2|)) = O(|L_1||L_2| + |L_2|^2)$。

与添加相比，链接操作的速度很快，为常数时间$O(1)$，我们可以只遍历$L_1$一次，然后将第二个列表链接到$L_1$的尾部。

\be
concat(L_1, L_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L_2 & L_1 = \phi \\
  cons(first(L_1), concat(rest(L_1), L_2)) & otherwise
  \end{array}
\right.
\ee

这一算法只通过一次遍历到达$L_1$的尾部，然后将第二个列表链接起来。因此总体性能为线性时间$O(|L_1|)$。

算法的描述如下。

\begin{itemize}
\item 若第一个列表为空，则连接的结果就是第二个列表；
\item 否则，我们将第二个列表连接到第一个列表中除去第一个元素外的剩余部分，然后再将第一个元素置于这一结果前。
\end{itemize}

大多数函数式环境提供了内置的函数或操作符来实现列表的连接操作，例如在ML语言家族中，\texttt{++}被用来连接两个列表。

\lstset{language=Haskell}
\begin{lstlisting}
[] ++ ys = ys
xs ++ [] = xs
(x:xs) ++ ys = x : xs ++ ys
\end{lstlisting}

这里我们加入了另外一种边界情况，如果第二个列表为空，我们无需遍历到第一个列表的尾部。连接结果为第一个列表。

在命令式环境中，通过在数据结构中增加一个尾指针，可以实现常数时间$O(1)$的连接操作。我们略过这种方法的实现。

若不使用尾指针，我们仍需遍历到第一个列表的尾部。

\begin{algorithmic}[1]
\Function{Concat}{$L_1, L_2$}
  \If{$L_1 = \phi$}
    \State \Return $L_2$
  \EndIf
  \If{$L_2 = \phi$}
    \State \Return $L_1$
  \EndIf
  \State $H \gets L_1$
  \While{\Call{Rest}{$L_1$} $\neq \phi$}
    \State $L_1 \gets$ \Call{Rest}{$L_1$}
  \EndWhile
  \State \Call{Rest}{$L_1$} $\gets L_2$
  \State \Return $H$
\EndFunction
\end{algorithmic}

下面的C++例子程序实现了列表的连接。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* concat(List<T>* xs, List<T>* ys) {
  List<T>* head;
  if (!xs)
    return ys;
  if (!ys)
    return xs;
  for (head = xs; xs->next; xs = xs->next);
  xs->next = ys;
  return head;
}
\end{lstlisting}

\subsection{和与积}
\index{列表!sum}
\index{列表!product}

\subsubsection{Recursive sum and product}

对于数字列表，我们常常要计算和与积。它们的计算结构很类似。我们稍后会介绍如何抽象这样的计算结构。

为了计算\underline{列表中元素的和}：

\begin{itemize}
\item 若列表为空，则结果为0；
\item 否则，结果为第一个元素加上\underline{剩余元素的和}。
\end{itemize}

求和的描述可以形式化为下面的定义。

\be
sum(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & L = \phi \\
  l_1 + sum(L') & otherwise
  \end{array}
\right.
\ee

但是，我们不能简单地将加法替换为乘法以获取列表中元素的积，否则结果总为0。我们需要定义空列表的积为1。

\be
product(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  1 & L = \phi \\
  l_1 \times product(L') & otherwise
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了和与积的计算。

\lstset{language=Haskell}
\begin{lstlisting}
sum [] = 0
sum (x:xs) = x + sum xs

product [] = 1
product (x:xs) = x * product xs
\end{lstlisting}

两个算法都需要遍历整个列表，因此它们的性能都为线性时间$O(n)$。

\subsubsection{尾递归}
\index{Tail call}
\index{Tail recursion}
\index{Tail recursive call}

注意到无论是求和还是求积的算法都从右向左计算。我们可以修改它们的实现，从左向右\underline{累积计算}结果。求和时，结果从0开始累积，逐一将每个元素加到结果上，直到处理完全部列表。具体描述如下：

当通过求和累积结果时：
\begin{itemize}
\item 若列表为空，则累积结束，返回累积结果；
\item 否则，取出列表中的第一个元素，将其加到累积结果上，然后继续处理剩余的列表。
\end{itemize}

将这一描述形式化为定义，就可以得到另一种累加的算法。

\be
sum'(A, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \phi \\
  sum'(A + l_1, L') & otherwise
  \end{array}
\right.
\ee

最终求和可以通过调用这一函数实现。我们传入0作为累加的起始值，同时传入待累加的列表。
\be
sum(L) = sum'(0, L)
\ee

这一改进除了将计算的顺序恢复为从左向右之外，还有一个重要的特点。观察函数$sum'(A, L)$的定义，我们发现它无需记录任何中间结果或者状态用于递归。所有的状态或者作为参数（例如$A$）传入接下来的递归调用，或者可以丢弃（例如列表中前面处理过的元素）。因此在实际的实现中，这样的递归函数可以进一步优化为循环，从而完全消除递归。

我们称这样的函数为“尾递归”（或“尾调用”），对其消除递归的优化称为“尾递归优化”\cite{wiki-tail-call}。顾名思义，这类函数中，递归发生在最后一步。尾递归优化可以极大地提高性能，并避免由于过深递归造成的调用栈溢出。

下面的Haskell例子程序给出了尾递归形式的求和与求积实现。

\lstset{language=Haskell}
\begin{lstlisting}
sum = sum' 0 where
    sum' acc [] = acc
    sum' acc (x:xs) = sum' (acc + x) xs

product = product' 1 where
    product' acc [] = acc
    product' acc (x:xs) = product' (acc * x) xs
\end{lstlisting}

在前面关于插入排序的部分，我们提到了函数式的实现从右向左对元素排序，我们也可以将其改为尾递归的形式。

\be
sort'(A, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \phi \\
  sort'(insert(l_1, A), L') & otherwise
  \end{array}
\right.
\ee

排序时，我们可以调用这一函数，传入一个空列表作为累积结果的起始值。
\be
sort(L) = sort'(\phi, L)
\ee

我们将它的具体实现作为练习留给读者。

作为本节的结尾，我们考虑一个有趣的题目，如何设计一个算法来高效地计算$b^n$？（参考\cite{SICP}中的1.16节。）

最直接的方法是从1开始重复乘以$b$共$n$次，这是一个线性时间$O(n)$的算法。

\begin{algorithmic}[1]
\Function{Pow}{$b, n$}
  \State $x \gets 1$
  \Loop{$n$ times}
    \State $x \gets x \times b$
  \EndLoop
  \State \Return $x$
\EndFunction
\end{algorithmic}

我们考虑如何改进它。考虑计算$b^8$的过程，上述算法经过前两次迭代，可以得到$x = b^2$的结果。此时，我们无需再次用$x$乘以$b$得到$b^3$，可以直接再次乘以$b^2$，从而得到$b^4$。然后再次乘方，就可以得到$(b^4) = b^8$。这样总共只要循环3次，而不是8次。

若$n$恰好为2的整数次幂，即$n = 2^m$，其中$m$是非负整数，则根据这一思路，我们可以用下面的等式快速计算$b^n$。

\[
pow(b, n) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  b & n = 1 \\
  pow(b, \frac{n}{2})^2 & otherwise
  \end{array}
\right.
\]

我们可以扩展这一分而治之的想法，从而将$n$推广到任意的非负整数。

\begin{itemize}
\item 边界情况，$n$为0，结果显然为1；
\item 若$n$为偶数，我们将$n$减半，先计算$b^{\frac{n}{2}}$。然后在将这一结果平方。
\item 否则，$n$为奇数。因为$n-1$是偶数，我们可以先递归计算$b^{n-1}$，然后在将这一结果乘以$b$。
\end{itemize}

这一算法可以定义为下面的等式。

\be
pow(b, n) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  1 & n = 0 \\
  pow(b, \frac{n}{2})^2 & 2 | n \\
  b \times pow(b, n-1) & otherwise
  \end{array}
\right.
\ee

但是，这一算法并不能直接转换为尾递归的形式，原因是第二条递归调用。实际上，我们可以先将底数平方，然后在将指数减半。

\be
pow(b, n) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  1 & n = 0 \\
  pow(b^2, \frac{n}{2}) & 2 | n \\
  b \times pow(b, n-1) & otherwise
  \end{array}
\right.
\ee

通过这一修改，就可以将这一算法转换为尾递归形式了。我们通过等式$b^n = pow'(b, n, 1)$计算。

\be
pow'(b, n, A) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & n = 0 \\
  pow'(b^2, \frac{n}{2}, A) & 2 | n \\
  pow'(b, n-1, A \times b) & otherwise
  \end{array}
\right.
\ee

和最初的方法相比，我们把性能提高到了$O(\lg n)$。实际上这意思算法还可以继续改进。

如果我们将$n$表示成二进制数$n = (a_ma_{m-1}...a_1a_0)_2$，如果$a_i = 1$，我们清楚地知道，需要计算$b^{2^i}$。这和二项式堆的情况很类似（请参考本书二项式堆一章）。因此，将所有二进制位为1对应的幂计算出，再累积乘到一起就可以得到最后的结果。

例如，当计算$b^{11}$时，由于11写成二进制为$11 = (1011)_2 = 2^3 + 2 +1$，因此$b^{11} = b^{2^3} \times b^2 \times b$。我们可以通过以下的步骤进行计算。

\begin{enumerate}
\item 计算$b^1$，得$b$；
\item 从这一结果进而得到$b^2$；
\item 将第2步的结果平方，从而得到$b^{2^2}$；
\item 将第3步的结果平方，得到$b^{2^3}$。
\end{enumerate}

最后，我们将第1、2、和第4步的结果乘到一起，得到$b^{11}$。

综上，我们可以进一步将算法改进如下。

\be
pow'(b, n, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & n = 0 \\
  pow'(b^2, \frac{n}{2}, A) & 2 | n \\
  pow'(b^2, \lfloor \frac{n}{2} \rfloor, A \times b) & otherwise
  \end{array}
\right.
\ee

这一算法本质上每次将$n$向右移动一个二进制位（通过将$n$除以2）。若LSB(Least Significant Bit，即最低位)为0，说明$n$为偶数。算法将底数平方，继续递归，无需改变累积结果。这对应上面例子的第3步；若LSB为1，说明$n$为基数。除了将底数平方，算法还要将$b$乘到累积结果上；边界条件是$n$为0时，此时我们已经处理完$n$中的所有位，最终结果就是累积的值$A$。在任何时候，最新的底数$b'$，移位后的指数$n'$，和累积结果$A$总满足不变条件$b^n = b'^{n'}A$。

下面的Haskell例子代码实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
pow b n = pow' b n 1 where
  pow' b n acc | n == 0 = acc
               | even n = pow' (b*b) (n `div` 2) acc
               | otherwise = pow' (b*b) (n `div` 2) (acc*b)
\end{lstlisting}

此前的算法当$n$为奇数时，仅仅将其减一转化为偶数进行处理；这一改进中，每次都将$n$减半。若$n$的二进制表示中有$m$位，这一算法只运行$m$轮。当然，它的复杂度仍然为$O(\lg n)$。我们将这一算法的命令式实现留给读者作为练习。

\subsubsection{命令式的求和与求积}
命令式实现中，一边遍历列表，一边应用加法或乘法累积结果。

\begin{algorithmic}[1]
\Function{Sum}{$L$}
  \State $s \gets 0$
  \While{$L \neq \phi$}
    \State $s \gets s +$ \Call{First}{$L$}
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $s$
\EndFunction
\Statex
\Function{Product}{$L$}
  \State $p \gets 1$
  \While{$L \neq \phi$}
    \State $p \gets p \times $ \Call{First}{$L$}
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $p$
\EndFunction
\end{algorithmic}

下面的C++例子程序实现了相应的求和与求积算法。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T sum(List<T>* xs) {
  T s;
  for (s = 0; xs; xs = xs->next)
    s += xs->key;
  return s;
}

template<typename T>
T product(List<T>* xs) {
  T p;
  for (p = 1; xs; xs = xs->next)
    p *= xs->key;
  return p;
}
\end{lstlisting}

利用求积算法，我们可以将递归的阶乘实现转换为递推的方式。即通过计算$\{1, 2, ..., n\}$的积来得到$n! = product([1..n])$。

\subsection{最大值和最小值}
\index{列表!最大值}
\index{列表!最小值}

另一个重要的操作是获取列表中的最大值或最小值。他们的算法结构同样很类似。我们稍后会归纳出相同的部分，抽象出一般性的高阶结构。对于最大值与最小值问题，我们假设列表不为空。

为了获取列表中的最小值。

\begin{itemize}
\item 若列表中只有一个元素（称为singleton列表），最小值就是这一唯一的元素；
\item 否则，我们首先找到除第一元素外，剩余部分中的最小值，然后再和第一个元素比较，选取较小的最为最终的最小值。
\end{itemize}

这一算法可以被定义如下。

\be
min(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & L = \{ l_1 \} \\
  l_1 & l_1 \leq min(L') \\
  min(L') & otherwise
  \end{array}
\right.
\ee

为了获取最大值，我们只需要将上述定义中的小于等于比较（$\leq$）换为大于等于（$\geq$）即可。

\be
max(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & L = \{ l_1 \} \\
  l_1 & l_1 \geq max(L') \\
  max(L') & otherwise
  \end{array}
\right.
\ee

注意上述两个算法都从右向左处理列表。在前面关于尾递归的部分我们讨论过类似的问题。我们可以将其变化为从左向右处理列表。另外，改成尾递归的形式后，算法具备了在线（on-line）处理能力，即任何时候，我们都知道已处理部分中的最大或者最小值。

\be
min'(L, a) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  a & L = \phi \\
  min(L', l_1) & l_1 < a \\
  min(L', a) & otherwise
  \end{array}
\right.
\ee

\be
max'(L, a) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  a & L = \phi \\
  max(L', l_1) & a < l_1 \\
  max(L', a) & otherwise
  \end{array}
\right.
\ee

对比求和与求积问题的尾递归解法，在实际中，我们不能向$min'$或$max'$传入一个常数，这是因为，理论上必须传入无穷（$min(L, \infty)$）或者负无穷（$max(L, -\infty)$），但是由于字长问题，我们不能严格给出无穷。

为了解决这一问题，可以将列表中的第一个元素传入，实际中，我们这样应用最大值和最小值算法。

\be
  \begin{array}{l}
  min(L) = min(L', l_1) \\
  max(L) = max(L', l_1)
  \end{array}
\ee

下面的Haskell例子程序实现了获取最大值和最小值的定义。
\lstset{language=Haskell}
\begin{lstlisting}
min (x:xs) = min' xs x where
    min' [] a = a
    min' (x:xs) a = if x < a then min' xs x else min' xs a

max (x:xs) = max' xs x where
    max' [] a = a
    max' (x:xs) a = if a < x then max' xs x else max' xs a
\end{lstlisting}

尾递归的最大值和最小值算法可以转换为循环的方式。

\begin{algorithmic}[1]
\Function{Min}{$L$}
  \State $m \gets$ \Call{First}{$L$}
  \State $L \gets$ \Call{Rest}{$L$}
  \While{$L \neq \phi$}
    \If{\Call{First}{$L$} $< m$ }
      \State $m \gets$ \Call{First}{$L$}
    \EndIf
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $m$
\EndFunction
\Statex
\Function{Max}{$L$}
  \State $m \gets$ \Call{First}{$L$}
  \State $L \gets$ \Call{Rest}{$L$}
  \While{$L \neq \phi$}
    \If{$m < $ \Call{First}{$L$}}
      \State $m \gets$ \Call{First}{$L$}
    \EndIf
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $m$
\EndFunction
\end{algorithmic}

下面的C++例子程序实现了最大值和最小值算法。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T min(List<T>* xs) {
  T x;
  for (x = xs->key; xs; xs = xs->next)
    if (xs->key < x)
      x = xs->key;
  return x;
}

template<typename T>
T max(List<T>* xs) {
  T x;
  for (x = xs->key; xs; xs = xs->next)
    if (x < xs->key)
      x = xs->key;
  return x;
}
\end{lstlisting}

另外一种尾递归的求最大值算法是每次丢弃掉较小的元素。边界情况和此前一样；对于递归情况，由于列表中至少有2个元素，我们每次拿出前两个比较，丢弃一个，然后继续处理剩余的元素。当列表中含有2个以上的元素时，记$L''$为$rest(rest(L)) = \{l_3, l_4, ...\}$，我们有如下的定义。

\be
max(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & |L| = 1 \\
  max(cons(l_1, L'')) & l_2 < l_1 \\
  max(L') & otherwise
  \end{array}
\right.
\ee

\be
min(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & |L| = 1 \\
  min(cons(l_1, L'')) & l_1 < l_2 \\
  min(L') & otherwise
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了这一种求最大值和最小值的算法。

\lstset{language=Haskell}
\begin{lstlisting}
min [x] = x
min (x:y:xs) = if x < y then min (x:xs) else min (y:xs)

max [x] = x
max (x:y:xs) = if x < y then max (y:ys) else max (x:xs)
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item 已知两个列表$L_1$和$L_2$，设计一个算法$eq(L_1, L_2)$，可以判定两个列表是否相等。这里相等的含义是列表的长度相同，并且每个对应的元素都相等。
\item 考虑处理列表随机访问时处理越界错误的各种方式，用函数式的方式和命令式的方式加以实现。比较使用异常和错误码的异同。
\item 给列表增加一个尾指针，使得向尾部添加可以在常数时间$O(1)$内完成，而无需线性时间$O(n)$。选择一门命令式语言实现这一改进。
\item 使用尾指针后，哪些列表操作中必须更新这一变量？对于性能会有怎样的影响？
\item 处理插入算法中的越界情况，将它作为添加元素处理。
\item 只使用小于比较（$<$），实现插入排序算法。
\item 设计并实现在列表中找到所有等于给定值的元素并删除的算法。
\item 使用尾递归重新实现计算列表长度的算法。
\item 使用尾递归实现插入排序。
\item 选择一门命令式编程语言，实现在$O(\lg n)$时间内计算$b^n$的算法。只需要在对应的二进制位不等于0时累积中间结果。
\end{itemize}
\end{Exercise}

\section{变换}
\index{列表!变换}

我们已经介绍了一些基本的列表操作。本节中，我们介绍列表的变换操作。某些抽象的变换操作是函数式编程的基石。我们同时会介绍如何使用变换操作解决一些趣题。

\subsection{映射（map）和for-each}
\index{列表!map}

在实际应用中，常常要输出一些可识别的字符串。如果有一个数字的列表，并且需要将这些数字在打印出来，例如“3 1 2 5 4”。我们可以首先将字符转换为字符串，这样就可以使用打印函数将其输出。下面是一个简单的转换程序。

\be
toStr(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  cons(str(l_1), toStr(L')) & otherwise
  \end{array}
\right.
\label{eq:tostr}
\ee

另一个例子是一个字典（dictionary）数据，包含若干单词，并以他们的首字母分组，例如：\texttt{[[a, an, another, ... ], [bat, bath, bool, bus, ...], ..., [zero, zoo, ...]]}。我们希望统计它们在英语中出现的频率。我们可以处理一些英文文本，例如《哈姆莱特》或者《圣经》，然后将每个单词的出现次数统计出。处理后，我们希望得到这样一个列表：

\begin{verbatim}
[[(a, 1041), (an, 432), (another, 802), ... ],
 [(bat, 5), (bath, 34), (bool, 11), (bus, 0), ...],
 ...,
 [(zero 12), (zoo, 0), ...]]
\end{verbatim}

如果我们希望找出，对应每个首字母，哪个单词被使用的次数最多。需要怎样实现呢？输出的结果是一个单词列表，表中每个单词都是在各自首字母组中出现最多的一个，形如：\texttt{[a, but, can, ...]}。我们需要实现一个程序，将一个分组的列表转换成一个单词列表。

我们接下来逐步实现这一程序。我们首先需要定义一个函数，接受一个列表，每个元素都是一对“单词——出现次数”，并搜索出现次数最多的单词。我们无需使用排序，只需要实现某种特殊的$max'()$函数。注意这里不能直接使用此前定义的$max()$函数。对于一对值$p = (a, b)$，定义函数$fst(p) = a$和$snd(p) = b$，用以获取其中的值。函数$max'()$可以定义如下。

\be
max'(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & |L| = 1 \\
  l_1 & snd(max'(L')) < snd(l_1) \\
  max'(L') & otherwise
  \end{array}
\right.
\ee

还有另外一种方法。我们可以定义一个函数，用以比较“单词——出现次数”，然后将这一比较函数传入一个抽象的$max()$函数。

\be
less(p_1, p_2) = snd(p_1) < snd(p_2)
\ee

\be
maxBy(cmp, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & |L| = 1 \\
  l_1 & cmp(l_1, maxBy(cmp, L')) \\
  maxBy(cmp, L') & otherwise
  \end{array}
\right.
\ee

这样，$max'()$实际上就成了$maxBy()$的一种特定实现，专门用以获取出现次数最多的单词。

\be
max'(L) = maxBy(\neg less, L)
\ee

这里，所有的函数多是以递归实现的。我们也可以将它们改为尾递归的形式。我们将这一修改作为练习留给读者。

定义好$max'()$函数后，就可以处理输入列表，完成这一程序。

\be
solve(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  cons(fst(max'(l_1)), solve(L')) & otherwise
  \end{array}
\right.
\label{eq:solve}
\ee

\subsubsection{Map}
\index{列表!map}

比较式(\ref{eq:solve})中定义的$solve()$函数，和式(\ref{eq:tostr})中定义的$toStr()$函数，可以发现他们的算法结构很类似。尽管它们解决的问题不同，一个较简单，另一个稍复杂。

在$toStr()$中，对列表中的所有元素，我们应用$str()$函数，将每一个数字转换为字符串；在$solve()$中，我们针对列表中的所有元素（每个元素是包含若干“单词——出现次数”的列表），我们首先应用$max'()$函数，然后再应用$fst()$函数，将一个列表转换为一个字符串。如果将这样的公共结构抽象出来，就可以获得\underline{映射}（map）的定义。

\be
map(f, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  cons(f(l_1)), map(f, L')) & otherwise
  \end{array}
\right.
\ee

由于映射接受一个转换函数$f$作为参数，它属于一种“高阶函数”（high-order function）。在很多函数式环境中，例如Haskell，映射就是通过上述定义实现的。

\lstset{language=Haskell}
\begin{lstlisting}
map :: (a->b)->[a]->[b]
map _ [] = []
map f (x:xs) = f x : map f xs
\end{lstlisting}

此前给出的两个具体问题，都可以通过高阶的映射来解决。

\[
\begin{array}{l}
toStr  = map \quad str \\
solve = map \quad (fst \cdot max')
\end{array}
\]

其中$f \cdot g$代表函数复合（compose），即首先应用函数$g$，然后再应用函数$f$。例如函数$h(x) = f(g(x))$可以表示为$h = f \cdot g$，读作函数$h$由$f$和$g$复合而成。这里我们使用了Curry形式，因而可以省略参数$L$，使得表达更加简洁。简单来说，对一个二元函数，如$f(x, y) = z$，如果我们仅提供了一个参数$x$，函数$f$就转变成为了一个新函数，它接受一个参数$y$，定义为$g(y) = f(x, y)$，或者$g = f x$。注意这里$x$不再是一个自由变量，而是一个绑定的值。读者可以参考关于函数式编程的材料来了解函数复合和Curry的更多内容。

也可以从域的角度来理解映射。考虑函数$y = f(x)$，它实际定义了从自变量$x$的域到$y$的值域的映射（$x$和$y$的类型可以不同）。若这些域可以表示为集合$X$和$Y$，我们有如下关系。

\be
Y = \{ f(x) | x \in X \}
\ee

这种形式的集合定义称为Zermelo Frankel集合抽象（亦称ZF表达式）\cite{algo-fp}。不同之处在于，我们的映射定义为从一个列表到另一个列表，而不是集合。因此可以含有重复的元素。在支持list comprehension的语言中，例如Haskell和Python等（Python中的list是一种内置数据类型，而不是本附录中所指的链表），映射可以被实现为list comprehension的某种特殊形式。

\lstset{language=Haskell}
\begin{lstlisting}
map f xs = [ f x | x <- xs]
\end{lstlisting}

List comprehension是一个强大的工具。例如，可以使用它来实现一个排列（permutation）算法。许多书籍介绍了全排列算法，如\cite{algo-fp}和\cite{erlang}。我们可以定义一个更加一般的排列函数$perm(L, r)$。给定一个含有$n$个元素的列表$L$，这一函数从$n$个元素中选择$r$个元素进行排列。我们知道一共有$P_n^r = \frac{n!}{(n-r)!}$种不同的排列。

\be
perm(L, r) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{\phi\} & r = 0 \lor |L| < r \\
  \{ \{l\} \cup P | l \in L, P \in perm(L-\{l\}, r-1)\} & otherwise
  \end{array}
\right.
\ee

其中，$\{l\} \cup P$的含义是$cons(l, P)$，而$L-\{l\}$表示$delete(L, l)$，它的定义此前已经介绍过。如果选出0个元素进行排列，或者列表中元素的个数小于$r$，排列结果为空列表；否则，我们逐一取出列表中每个元素$l$，递归地从剩余的$n-1$个元素中选择$r-1$个元素进行排列。然后再将$l$置于所有可能的$r-1$个元素排列的前面。下面的Haskell程序，实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
perm _ 0 = [[]]
perm xs r | length xs < r = [[]]
          | otherwise = [ x:ys | x <-xs, ys <- perm (delete x xs) (r-1)]
\end{lstlisting}

我们稍后在列表过滤（filter）部分还会再次讨论list comprehension。

映射也可以用命令式的方式实现。我们可以在遍历列表时应用传入的函数，从左向右构造新的列表。由于新元素被添加在结果列表的尾部，我们可以不断更新列表的尾指针，这样考虑传入的函数的调用次数时，整体的性能就是线性的。

\begin{algorithmic}[1]
\Function{Map}{$f, L$}
  \State $L' \gets \phi$
  \State $p \gets \phi$
  \While{$L \neq \phi$}
    \If{$p = \phi$}
      \State $p \gets$ \textproc{Cons}($f($ \Call{First}{$L$} $), \phi$)
      \State $L' \gets p$
    \Else
      \State \Call{Next}{$p$} $\gets$ \textproc{Cons}($f($ \Call{First}{$L$} $), \phi$)
      \State $p \gets$ \Call{Next}{$p$}
    \EndIf
    \State $L \gets$ \Call{Next}{$L$}
  \EndWhile
  \State \Return $L'$
\EndFunction
\end{algorithmic}

在一些静态类型的语言中，例如C++\footnote{例如ISO C++ 1998标准}，如果没有类型推断（type inference），标记（annotate）传入函数的类型会比较复杂\cite{sgi-stl-transform}。某一时期的C++标准库提供了\verb|std::transform|来实现映射的概念。由于涉及某些语言特性，我们略去了C++的例子程序。

简单起见，我们使用Python来给出例子程序。无需编译期间进行类型推断。下面的例子定义了单向链表的节点。

\lstset{language=Python}
\begin{lstlisting}
class List:
    def __init__(self, x = None, xs = None):
        self.key = x
        self.next = xs

def cons(x, xs):
    return List(x, xs)
\end{lstlisting}

映射的例子程序接受一个函数和一个链表，然后依照上述描述的算法，逐一对每个元素应用传入的函数。

\begin{lstlisting}
def mapL(f, xs):
    ys = prev = List()
    while xs is not None:
        prev.next = List(f(xs.key))
        prev = prev.next
        xs = xs.next
    return ys.next
\end{lstlisting}

和伪代码不同，这一程序使用了一个dummy节点作为结果列表的头部，这样可以简化实现，无需在尾部添加时检查是否为NIL。只要在最后返回接过前丢弃掉dummy节点即可。

\subsubsection{For each}
\index{列表!for each}

对于一些较简单的操作，如打印一个列表的内容，我们可以逐一打印每个元素而无需将整个列表转换为一个字符串列表。这样就可以简化程序如下：

\begin{algorithmic}[1]
\Function{Print}{$L$}
  \While{$L \neq \phi$}
    \State print \Call{First}{$L$}
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
\EndFunction
\end{algorithmic}

通常，我们可以在遍历时传入一个过程，例如打印，然后逐一（for each）执行这一过程。

\begin{algorithmic}[1]
\Function{For-Each}{$L, P$}
  \While{$L \neq \phi$}
    \State \textproc{P}(\Call{First}{$L$})
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
\EndFunction
\end{algorithmic}

也可以用递归来定义for-each算法。

\be
foreach(L, p) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  u & L = \phi \\
  do(p(l_1), foreach(L', p)) & otherwise
  \end{array}
\right.
\ee

这里符号$u$表示unit，它的含义是不做任何事。它的类型和C或Java中的void概念相似。函数$do()$对它的所有参数求值，丢弃除最后一个外的所有其他值，返回组后一个值作为$do()$的结果。它和Lisp中的\texttt{(begin ...)}，以及Haskell中的\texttt{do}区块类似。有关unit类型的更多信息，读者可以参考\cite{mittype}。

for-each算法本质上是一种简化的映射，它们只有两点不同：

\begin{itemize}
\item for-each无需构造结果列表，在使用时，我们更关注它的“副作用”（side effect）而非返回的结果；
\item for-each强调遍历，而映射强调应用函数，因此他们的参数顺序分别为$map(f, L)$和$foreach(L, p)$。
\end{itemize}

某些函数式环境同时提供了带和不带返回值列表的两种实现。例如Haskell的Monad库同时提供了\texttt{mapM}、\texttt{mapM\_}和\texttt{forM}、\texttt{forM\_}。读者可以参考相关语言的具体资料。

\subsubsection{映射的例子}

作为使用映射的例子，我们思考一道ACM/ICPC\cite{poj-drunk-jailer}中的趣题。简单起见，我们修改了题目的描述。假设屋子里有$n$个灯泡，所有灯泡都是暗的。我们执行下面的过程$n$次。

\begin{enumerate}
\item 将所有的灯都打开；
\item 扳动第2、4、6、……所有偶数位置灯的开关。如果灯是亮的，则变暗；如果是暗的，则变亮；
\item 每三个灯，扳动一次开关。第3、6、9、……位置上的灯的明暗状态切换；
\item ……
\end{enumerate}

最后一轮的时候，只有最好一盏灯（第$n$盏）的开关被扳动。

问最终有几盏灯是亮的？

在给出最佳答案前，我们先考虑暴力解法。把$n$盏灯表示为一列0、1数字，其中0表示灯灭，1表示灯亮。最初时，所有灯都是灭的，因此为$n$个零：$\{0, 0, ..., 0\}$。

我们将灯分别编号为1到$n$。我们首先通过映射将灯的状态转换为带有编号的列表\footnote{在函数式编程中，通常使用zip来实现。我们稍后会详细解释zip。}。

\[
map(\lambda_i \cdot (i, 0), \{1, 2, 3, ... n\})
\]

这一映射将每个自然数都绑定一个零状态，结果为一个列表，每个元素都是一对值：$L = \{(1, 0), (2, 0), ..., (n, 0)\}$。

然后，我们操作这一列表$n$次。从1到$n$，对于第$i$次操作，我们逐一检查列表中的每对值，如果编号能被$i$整除，我们就将状态翻转。考虑$1 - 0 = 1$、且$1 - 1 = 0$，我们可以将电灯亮灭状态$x$的切换实现为$1 - x$。在第$i$轮操作中，对于灯$(j, x)$，若$i | j$（或$j \mod i = 0$），我们就翻转灯的亮灭状态，否则就跳过不做任何处理。

\be
switch(i, (j, x)) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (j, 1 - x) &  j \mod i = 0 \\
  (j, x) & otherwise
  \end{array}
\right.
\ee

真对有所灯的第$i$轮操作也可以用映射实现。

\be
map(switch(i), L)
\ee

这里，我们使用了$switch()$函数的Curry形式，它等价于：

\[
map(\lambda_{(j, x)} \cdot switch(i, (j, x)), L)
\]

我们需要定义一个函数$proc()$，它可以重复执行上述对$L$的映射$n$次。一种方法是通过下面定义的递归，调用形式为：$proc(\{1, 2, ..., n\}, L)$\footnote{通常被实现为fold，我们稍后会详加解释。}。

\be
proc(I, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L & I = \phi \\
  operate(I', map(switch(i_1), L)) & otherwise
  \end{array}
\right.
\ee

其中$I = cons(i_1, I')$，即$I$不为空时，其第一个元素为$i_1$，剩余部分为$I'$。

最后，我们可以将列表$L$中每一对值的第二个元素累加起来得到最终的答案。累加的实现在前面定义过，我们需要定义映射的方法，并将结果传入累加函数。

\be
solve(n) = sum(map(snd, proc(\{1, 2, ..., n\}, L)))
\ee

下面的Haskell例子程序实现了这一暴力解法。

\lstset{language=Haskell}
\begin{lstlisting}
solve' = sum . (map snd) . proc  where
    proc n = operate [1..n] $ map (\i -> (i, 0)) [1..n]
    operate [] xs = xs
    operate (i:is) xs = operate is (map (switch i) xs)
\end{lstlisting} %$

我们列出灯的数目为1、2、……、100盏时，经过上述操作，组后仍然亮的数目：

\begin{verbatim}
[1,1,1,2,2,2,2,2,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,
6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,
8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10]
\end{verbatim}

这一结果很有趣。

\begin{itemize}
\item 3盏灯以内时，最后仍然亮的灯位1盏；
\item 4盏灯到8盏灯时，最后仍然有2盏灯是亮的；
\item 9盏灯到15栈灯时，最后又3盏灯是亮的；
\item ……
\end{itemize}

看起来，当灯的数目为$i^2$到$(i+1)^2-1$盏时，最后会有$i$盏灯是亮的。事实上，我们可以证明这一结论。

\begin{proof}
将$n$盏灯编号为1到$n$，考虑最后仍然亮的那些灯。由于初始时，所有灯都是灭的，我们可以确定，被扳动奇数次开关的灯最后是亮的。对于编号为$i$的灯，若$i$可以被$j$整除（表示为$j | i$），则在第$j$轮，它的开关被扳动一次。所以当灯的编号含有奇数个因子时，最后的状态是亮的。

因此，为了找出最后亮的灯，我们需要找出所有含有奇数个因子的数。对于任意自然数$n$，记$S$为$n$的所有因子的集合。$S$初始化为$\phi$，若$p$为$n$的一个因子，则必然存在一个正整数$q$，使得$n = p q$。也就是说$q$也是$n$的因子。因此当且仅当$p \neq q$时，我们向集合$S$中添加两个因子，这样$S$将总是偶数。除非$p = q$，此时，$n$必将是一个完全平方数，所以我们只能向集合$S$中增加一个因子。这样$n$就有技术个因子。
\end{proof}

根据这一结论，我们可以通过寻找$n$以内的完全平方数来快速解决这一趣题。

\be
solve(n) = \lfloor \sqrt{n} \rfloor
\ee

下面的Haskell命令输出灯的数目为1、2、……、100盏时最后有多少灯是亮的结果，这和暴力方法的结果一致。

\begin{lstlisting}
map (floor.sqrt) [1..100]
[1,1,1,2,2,2,2,2,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,
6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,
8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10]
\end{lstlisting}

映射是一个一般性的概念，它不仅局限于列表，也可以扩展到许多复杂的数据结构。本书中二叉搜索树一章解释了如何对树进行映射。只要我们能够以某种顺序遍历一个数据结构，并且能够标识出空数据结构，就可以使用映射的概念。我们在稍后fold一节，会再次看到这样的高阶概念。

\subsection{反转}
\index{列表!反转}

如何用最小的空间反转一个单向链表曾经是一道流行的面试题。在支持指针的命令式环境中，例如C语言，反转单向链表需要仔细的指针操作。我们将展示一种简单的方式来得到答案。

\begin{enumerate}
\item 首先，写出一个简单、直观的纯递归解；
\item 然后，将纯递归解转换为尾递归形式；
\item 最后，将尾递归解转换为纯命令式的指针操作。
\end{enumerate}

纯递归解非常简单，我们可以很容易地定义出。为了“反转列表$L$”。

\begin{itemize}
\item 若列表$L$为空，反转结果也是空。这是边界情况；
\item 否则，我们首先反转除第一元素外的子列表，然后将第一个元素添加到尾部。
\end{itemize}

这一思路可以形式化为下面的定义。

\be
reverse(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  append(reverse(L'), l_1) & otherwise \\
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了这一解法。

\lstset{language=Haskell}
\begin{lstlisting}
reverse [] = []
reverse (x:xs) = reverse xs ++ [x]
\end{lstlisting}

但是这一方法的性能不佳，为了向列表末尾添加元素，必须遍历列表。因此总体时间是平方级的。为了提高性能，可以将其转换为尾递归形式。我们使用一个累积器来记录中间的反转结果。传入一个空列表来启动反转$reverse(L) = reverse'(L, \phi)$。

\be
reverse'(L, A) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \phi \\
  reverse'(L', \{l_1\} \cup A) & otherwise
  \end{array}
\right.
\ee

其中$\{l_1\} \cup A$表示$cons(l_1, A)$。和在尾部追加相比，这是一个常数时间$O(1)$的操作。我们不断从列表的头部逐一取出元素，将其置于累积结果的前面。这相当于将全部元素压入一个堆栈，然后再依次弹出。整体上是一个线性时间算法。

下面的Haskell例子程序实现了这一尾递归的程序。

\begin{lstlisting}
reverse' [] acc = acc
reverse' (x:xs) acc = reverse' xs (x:acc)
\end{lstlisting}

由于尾递归无需通过调用栈记录上下文，大多数现代编译器都能将其优化为纯命令式的循环。我们接下来要做的是手工进行这一优化，消除递归，从而得到一个命令式的算法。

\begin{algorithmic}[1]
\Function{Reverse}{$L$}
  \State $A \gets \phi$
  \While{$L \neq \phi$}
    \State $A \gets $ \textproc{Cons}(\Call{First}{$L$}, $A$)
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
\EndFunction
\end{algorithmic}

但是，这一算法生成了一个新的反转列表，而不是在原列表上直接修改。我们接下来要通过重用$L$将其改为in-place的形式。下面的C++例子程序实现了这一in-place的单向链表反转。它只需要常数空间，在线性时间$O(n)$内完成反转。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
List<T>* reverse(List<T>* xs) {
  List<T> *p, *ys = NULL;
  while (xs) {
    p = xs;
    xs = xs->next;
    p->next = ys;
    ys = p;
  }
  return ys;
}
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item 选择一门编程语言，实现尾递归形式的求最大值算法。
\end{itemize}
\end{Exercise}

\section{提取子列表}
\index{列表!提取子列表}

数组可以很方便、快速地分割为连续的子空间。而分割列表则需要更多的工作，大多数这类操作都是线性时间的。

\subsection{take、drop、和split-at}
\index{列表!take}
\index{列表!drop}
\index{列表!split at}

从列表中取出前$n$个元素，在语义上和从最左侧获取子列表$sublist(L, 1, n)$相同。其中$sublist$的第2个参数是子列表的起始位置，第3个参数是子列表的结束位置。对于边界情况，或者$n$为0，或者列表为空，结果是一个空的子列表；否则，我们可以取出第一个元素，递归地在剩余部分取出$n-1$个元素，组后再将取出的元素置于最前。

\be
take(n, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \lor n = 0 \\
  cons(l_1, take(n-1, L')) & otherwise
  \end{array}
\right.
\ee

边界情况同时也处理了越界的错误。下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
take _ [] = []
take 0 _ = []
take n (x:xs) = x : take (n-1) xs
\end{lstlisting}

另一个操作是从列表中丢弃前$n$个元素，并返回剩余的部分作为结果。它等价于从右侧获取子列表$sublist(L, n+1, |L|)$，其中$|L|$是列表的长度。我们可以通过递归地丢弃第一个元素的方式实现。

\be
drop(n, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  L & n = 0 \\
  drop(n-1, L')) & otherwise
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了丢弃操作。

\lstset{language=Haskell}
\begin{lstlisting}
drop _ [] = []
drop 0 L = L
drop n (x:xs) = drop (n-1) xs
\end{lstlisting}

命令式的取出和丢弃简单、直观，我们把它们的实现作为练习留给读者。

使用取出和丢弃操作，可以在列表的任何位置获取任何长度的子列表。

\be
sublist(L, from, count) = take(count, drop(from - 1, L))
\ee

另外一种形式，是传入左侧和右侧的边界：

\be
sublist(L, from, to) = drop(from - 1, take(to, L))
\ee

这一函数返回在闭区间$[from, to]$内的元素，包括边界上的元素。本节介绍的所有算法都是线性时间的。

\subsubsection{take-while和drop-while}
\index{列表!take while}
\index{列表!drop while}

和take与drop相比，还有另外一类操作，只要某种条件成立，我们就不断取出或者丢弃元素，称为take-while或者drop-while。take和drop可以看作是它们的一种特殊形式。

take-while不断检查元素是否满足给定条件并取出，如果条件不满足，则停止检查剩余的所有元素，即使剩余元素中可能有满足条件的也不予处理。这和稍后介绍的filter有所不同。后者会遍历整个列表找出满足条件的所有元素。

\be
takeWhile(p, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  \phi & \lnot p(l_1) \\
  cons(l_1, takeWhile(p, L')) & otherwise
  \end{array}
\right.
\ee

take-while接受两个参数，一个是条件函数$p$，我们可以将其应用到元素上，得到一个布尔值作为结果；另一个参数是待处理的列表。drop-while也可以用对称的方式加以定义。

\be
dropWhile(p, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  L & \lnot p(l_1) \\
  dropWhile(p, L') & otherwise
  \end{array}
\right.
\ee

相应的Haskell例子程序实现如下。

\lstset{language=Haskell}
\begin{lstlisting}
takeWhile _ [] = []
takeWhile p (x:xs) = if p x then x : takeWhile p xs else []

dropWhile _ [] = []
dropWhile p xs@(x:xs') = if p x then dropWhile p xs' else xs
\end{lstlisting}

\subsubsection{split-at}
\index{列表!split at}

使用take和drop，我们可以进一步定义出split-at。

\be
splitAt(i, L) = (take(i, L), drop(i, L))
\ee

\subsection{切分和分组}

\subsubsection{切分}
\index{列表!break}
\index{列表!span}

切分可以被认为是一种特殊的split，我们不是在指定的位置将列表分成两部分，而是检查每个元素是否满足某一条件，找到列表中满足条件的最长前缀。切分解雇哦是一对子列表，一个是最长前缀，另一个包含剩余的部分。

有两种切分的语义，一种是选择满足条件的最长子列表；另一种是选择不满足条件的最长子列表。前者通常称为$span$，后者称为$break$。

span可以用递归描述如下：为了寻找列表$L$中满足条件$p$的最长span：

\begin{itemize}
\item 若列表为空；结果为一对空列表$(\phi, \phi)$;
\item 否则，我们检查第一个元素$l_1$是否满足条件，若满足，我们记递归寻找剩余列表span的结果为$(A, B) = span(p, L')$，然后，我们将$l_1$置于$A$的前面，从而得到最终结果$(\{ l_1 \} \cup A, B)$；否则，我们返回$(\phi, L)$作为结果。
\end{itemize}

对于break，我们只需要检查条件没有被满足，其余部分和span相同。另一种方法是使用span来定义break，如后面的Haskell例子程序所示。

\be
span(p, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\phi, \phi) & L = \phi \\
  (\{ l_1 \} \cup A, B) & p(l_1) = True, (A, B) = span(p, L') \\
  (\phi, L) & otherwise
  \end{array}
\right.
\ee

\be
break(p, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\phi, \phi) & L = \phi \\
  (\{ l_1 \} \cup A, B) & \lnot p(l_1), (A, B) = break(p, L') \\
  (\phi, L) & otherwise
  \end{array}
\right.
\ee

这两个函数都只找到最长“前缀”，就立即停止，即使后面仍有元素满足（或不满足）传入的条件。下面的Haskell例子程序实现了span和break。

\lstset{language=Haskell}
\begin{lstlisting}
span _ [] = ([], [])
span p xs@(x:xs') = if p x then let (as, bs) = span xs' in (x:as, bs) else ([], xs)

break p = span (not . p)
\end{lstlisting}

也可以用命令式的方式实现break和span。

\begin{algorithmic}[1]
\Function{Span}{$p, L$}
  \State $A \gets \phi$
  \While{$L \neq \phi \land p(l_1)$}
    \State $A \gets $ \Call{Cons}{$l_1, A$}
    \State $L \gets $ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $(A, L)$
\EndFunction
\Statex
\Function{Break}{$p, L$}
  \State \Return \Call{Span}{$\lnot p, L$}
\EndFunction
\end{algorithmic}

这一算法创建了一个新的列表用以存放最长前缀，我们也可以将其转换为in-place的算法，复用原列表的空间，如下面的Python例子程序所示。

\lstset{language=Python}
\begin{lstlisting}
def span(p, xs):
    ys = xs
    last = None
    while xs is not None and p(xs.key):
        last = xs
        xs = xs.next
    if last is None:
        return (None, xs)
    last.next = None
    return (ys, xs)
\end{lstlisting}

由于span和break都需要遍历列表检查条件是否满足，所以他们都是线性时间$O(n)$的算法。

\subsubsection{分组}
\index{列表!分组}

我们有时需要将列表中的元素分成若干组。例如要把字符串“Mississippi”，实际上是字符的列表\{ 'M', 's', 's', 'i', 's', 's', 'i', 'p', 'p', 'i'\}分成若干组，每组包含连续相同的字符。分组结果希望如下：

\begin{verbatim}
group(`Mississippi') = { `M'', `i', `ss', `i', `ss', `i', `pp', `i'}
\end{verbatim}

我们再给一个例子，下面是一个数字的列表：

\[
L = \{15, 9, 0, 12, 11, 7, 10, 5, 6, 13, 1, 4, 8, 3, 14, 2\}
\]

我们希望把它分成若干小组，每组中的元素都按照降序排列。分组的结果希望如下：

\[
group(L) = \{ \{15, 9, 0\}, \{12, 11, 7\}, \{10, 5\}, \{6\}, \{13, 1\}, \{4\}, \{8, 3\}, \{14, 2\}\}
\]

它们都是真实算法中的重要例子。字符串分组后，可用于构造Trie或Patricia数据结构，它是字符搜索和处理领域中的有力工具；将列表分组成有序子列表是自然归并排序中的步骤。本书中有专门的章节讲述这两个算法。

显然，我们需要抽象出分组的条件用以将列表分割成较小的部分。我们可以将这一条件作为参数传入，如$group(p, L)$，其中$p$接受两个相邻的元素作为参数，并检查是否满足分组的条件。

显然可以通过遍历来实现分组——每次取出两个元素，若分组条件满足，则将它们置于一个小组中；否则，仅将第一个元素放入组中，而把第二个元素放入一个新的小组中。记列表中的前两个元素（如果存在）为$l_1, l_2$，除去第一个元素后的剩余部分为$L'$。分组的结果为一个列表的列表$G = \{g_1, g_2, ...\}$，记为$G = group(p, L)$。

\be
group(p, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{\phi\} & L = \phi \\
  \{\{l_1\}\} & |L| = 1 \\
  \{\{l_1\} \cup g'_1, g'_2, ...\} & p(l_1, l_2), G' = group(p, L') = \{g'_1, g'_2, ...\} \\
  \{\{l_1\}, g'_1, g'_2, ...\} & otherwise
  \end{array}
\right.
\ee

这里$\{l_1\} \cup g'_1$的含义是$cons(l_1, g'_1)$，是一个常数间的操作。整个算法需要遍历列表一遍，用时为线性时间$O(n)$。

\lstset{language=Haskell}
\begin{lstlisting}
group _ [] = [[]]
group _ [x] = [[x]]
group p (x:xs@(x':_)) | p x x' = (x:ys):yss
                      | otherwise = [x]:r
  where
    r@(ys:yss) = group p xs
\end{lstlisting}

也可以用命令式的方式实现这一算法，若$L$不为空，我们将分组结果初始化为$\{{l_1\}}$。然后从第二个元素开始遍历列表，若相邻的两个元素满足条件，我们就将遍历到的元素放入最后一组，否则就新创建一个组。

\begin{algorithmic}[1]
\Function{Group}{$p, L$}
  \If{$L = \phi$}
    \State \Return $\{ \phi \}$
  \EndIf
  \State $x \gets$ \Call{First}{$L$}
  \State $L \gets$ \Call{Rest}{$L$}
  \State $g \gets \{ x \}$
  \State $G \gets \{ g \}$
  \While{$L \neq \phi$}
    \State $y \gets$ \Call{First}{$L$}
    \If{$p(x, y)$}
      \State $g \gets $ \Call{Append}{$g, y$}
    \Else
      \State $g \gets \{y\}$
      \State $G \gets$ \Call{Append}{$G, g$}
    \EndIf
    \State $x \gets y$
    \State $L \gets$ \Call{Next}{$L$}
  \EndWhile
  \State \Return $G$
\EndFunction
\end{algorithmic}

如果上述算法中的$L$是链表，并且append函数没有使用尾指针优化，这一方法的性能会退化为平方级别。下面的Python例子程序实现了这一算法。

\lstset{language=Python}
\begin{lstlisting}
def group(p, xs):
    if xs is None:
        return List(None)
    (x, xs) = (xs.key, xs.next)
    g = List(x)
    G = List(g)
    while xs is not None:
        y = xs.key
        if p(x, y):
            g = append(g, y)
        else:
            g = List(y)
            G = append(G, g)
        x = y
        xs = xs.next
    return G
\end{lstlisting}

使用这一定义好的分组函数，本节开头的两个例子就可以通过传入不同的分组条件加以实现。

\[
group(=, \{m,i,s,s,i,s,s,i,p,p,i\}) = \{ \{M\}, \{i\}, \{ss\}, \{i\}, \{ss\}, \{i\}, \{pp\}, \{i\} \}
\]

\[
\begin{array}{l}
group(\geq,  \{15, 9, 0, 12, 11, 7, 10, 5, 6, 13, 1, 4, 8, 3, 14, 2\}) \\
  = \{ \{15, 9, 0\}, \{12, 11, 7\}, \{10, 5\}, \{6\}, \{13, 1\}, \{4\}, \{8, 3\}, \{14, 2\}\}
\end{array}
\]

也可以使用此前定义的$span$函数来实现分组。我们向span传入一个条件，结果会将列表分割成两部分，其中那个第一部分是满足条件的最长子列表。我们对剩余部分不断执行span，直到处理完所有元素。

但是我们传入span的条件判断函数是一个“一元函数”（unary function），它只接受一个元素作为参数，检查它是否满足条件。但是在分组时，我们需要的条件判断函数是一个“二元函数”（binary function）。它接受两个相邻的元素，检查它们是否满足条件。为了解决这一差异，我们使用Curry方法，首先将第一个元素传入二元条件判断函数，然后使用Curry后的函数判断剩余的元素。

\be
group(p, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{\phi\} & L = \phi \\
  \{ \{l_1\} \cup A \} \cup group(p, B) & otherwise
  \end{array}
\right.
\ee

其中$(A, B) = span(\lambda_x \cdot p(l_1, x), L')$，是对列表$L$中除第一元素外的剩余部分进行span的结果。

虽然这一新定义的分组函数可以将单词中的相同字母分组，如下：

\lstset{language=Haskell}
\begin{lstlisting}
groupBy (==) "Mississippi"
["m","i","ss","i","ss","i","pp","i"]
\end{lstlisting}

但是，它却不能正确地将一个列数字，按照降序分组：

\begin{lstlisting}
groupBy (>=) [15, 9, 0, 12, 11, 7, 10, 5, 6, 13, 1, 4, 8, 3, 14, 2]
[[15,9,0,12,11,7,10,5,6,13,1,4,8,3,14,2]]
\end{lstlisting}

在这一例子中，第一个元素是15，它被span用来置于$\geq$的左侧进行比较。但15是整个列表中最大的元素，因此span的结果是，所有元素都在$A$中，而$B$为空。这看起来像是一个缺陷，但其实，如果认为分组的条件是一个\underline{抽象相等}判断的话，这是一个正确的行为。

严格说来，相等条件必须满足三个性质：自反性（reflexive）、传递性（transitive）、和对称性（symmetric）。它们分别描述如下：

\begin{itemize}
\item 自反性。$x = x$，即任何元素和他自己相等；
\item 传递性。$x = y, y = z \Rightarrow x = z$，如果两个元素相等，并且它们其中的一个和第三个元素相等，则这三个元素相等；
\item 对称性。$x = y \Leftrightarrow y = x$，即比较的顺序不影响结果。
\end{itemize}

当我们对字符列表“Mississippi”分组时，我们使用等号（$=$）作为判断条件，上述三个条件都被满足。这自然产生了正确的结果。但是当我们将大于等于号（$\gep$）作为相等条件传入，以对列表中的数字分组，就违反了自反性和对称性。这就是我们得到错误分组结果的原因。

这一事实说明，我们用span实现的第二个分组算法，将分组的语义限制为严格\underline{抽象相等}，而第一分组算法则无此种限制。它仅检查任何两个相邻元素是否满足条件，这笔相等性的限制要弱很多。

\begin{Exercise}
\begin{enumerate}
\item 选择一门命令式语言，实现in-place的take和drop算法。请注意处理越界情况。建议同时使用带有GC和不带有GC的语言进行练习。
\item 选择一门命令式语言，实现take-while和drop-while算法。建议同时使用动态类型和静态类型（不带有类型推导）的语言进行练习。考虑在静态类型语言中，如何声明通用的条件函数类型？
\item 考虑下面span的定义
\[
span(p, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\phi, \phi) & L = \phi \\
  (\{ l_1 \} \cup A, B) & p(l_1) = True, (A, B) = span(p, L') \\
  (A, \{l_1\} \cup B) & otherwise
  \end{array}
\right.
\]
它和我们本节介绍的实现有何不同？
\item 选择一门命令式语言，通过span来实现分组算法。
\end{enumerate}
\end{Exercise}

\section{Folding}
\index{folding}

We are ready to introduce one of the most critical concept in high order programming, folding. It is so powerful tool
that almost all the algorithms so far in this appendix can be realized by folding. Folding is sometimes be named as
reducing (the abstracted concept is identical to the buzz term `map-reduce' in cloud computing in some sense). For example,
both STL and Python provide reduce function which realizes partial form of folding.

\subsection{folding from right}
\index{List!foldr}
\index{List!fold from right}
Remind the sum and product definition in previous section, they are quite similar actually.

\[
sum(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & L = \phi \\
  l_1 + sum(L') & otherwise
  \end{array}
\right.
\]

\[
product(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  1 & L = \phi \\
  l_1 \times product(L') & otherwise
  \end{array}
\right.
\]

It is obvious that they have same structure. What's more, if we list the insertion sort definition, we can
find that it also shares this structure.

\[
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  insert(l_1, sort(L')) & otherwise
  \end{array}
\right.
\]

This hint us that we can abstract this essential common structure, so that we needn't repeat it again and again.
Observing $sum$, $product$, and $sort$, there are two different points which we can parameterize.

\begin{itemize}
\item The result of the trivial edge case varies. It is zero for sum, 1 for product, and empty list for sorting.
\item The function applied to the first element and the intermediate result varies. It is plus for sum, multiply for product,
and ordered-insertion for sorting.
\end{itemize}

If we parameterize the result of trivial edge case as initial value $z$ (stands for abstract zero concept), the
function applied in recursive case as $f$ (which takes two parameters, one is the first element in the list,
the other is the recursive result for the rest of the list), this common structure can be defined as something
like the following.

\[
proc(f, z, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  z & L = \phi \\
  f(l_1, proc(f, z, L')) & otherwise
  \end{array}
\right.
\]

That's it, and we should name this common structure a better name instead of the meaningless `proc'. Let's
see the characteristic of this common structure. For list $L = \{x_1, x_2, ..., x_n \}$, we can expand the
computation like the following.

\[
\begin{array}{rl}
proc(f, z, L) & = f(x_1, proc(f, z, L') \\
        & = f(x_1, f(x_2, proc(f, z, L'')) \\
        & ... \\
        & = f(x_1, f(x_2, f(..., f(x_n, f(f, z, \phi))...) \\
        & = f(x_1, f(x_2, f(..., f(x_n, z))...)
\end{array}
\]

Since $f$ takes two parameters, it's a binary function, thus we can write it in infix form. The infix
form is defined as below.

\be
x \oplus_f y = f(x, y)
\ee

The above expanded result is equivalent to the following by using infix notion.

\[
proc(f, z, L) = x_1 \oplus_f (x_2 \oplus_f (... (x_n \oplus_f z))...)
\]

Note that the parentheses are necessary, because the computation starts from the right-most ($x_n \oplus_f z$),
and repeatedly fold to left towards $x_1$. This is quite similar to folding a Chinese hand-fan as illustrated
in the following photos. A Chinese hand-fan is made of bamboo and paper. Multiple bamboo frames are stuck
together with an axis at one end. The arc shape paper is fully expanded by these frames as shown in Figure
\ref{fig:fold-fan} (a);
The fan can be closed by folding the paper. Figure \ref{fig:fold-fan} (b) shows that some part of the fan
is folded from right. After these folding finished, the fan results a stick, as shown in Figure \ref{fig:fold-fan} (c).

\begin{figure}[htbp]
    \centering
    \subfloat[A folding fan fully opened.]{\includegraphics[scale=0.3]{img/fold-fan1.eps}} \\
    \subfloat[The fan is partly folded on right.]{\includegraphics[scale=0.3]{img/fold-fan2.eps}}
    \subfloat[The fan is fully folded, closed to a stick.]{\includegraphics[scale=0.3]{img/fold-fan3.eps}}
    \caption{Folding a Chinese hand-fan} \label{fig:fold-fan}
\end{figure}

We can considered that each bamboo frame along with the paper on it as an element, so these frames forms a
list. A unit process to close the fan is to rotate a frame for a certain angle, so that it lays on top
of the collapsed part. When we start closing the fan, the initial collapsed result is the first bamboo frame.
The close process is folding from one end, and repeatedly apply the unit close steps, till all the frames
is rotated, and the folding result is a stick closed form.

Actually, the sum and product algorithms exactly do the same thing as closing the fan.

\[
\begin{array}{rl}
sum(\{1, 2, 3, 4, 5 \}) & = 1 + (2 + (3 + (4 + 5))) \\
         & = 1 + (2 + (3 + 9)) \\
         & = 1 + (2 + 12) \\
         & = 1 + 14 \\
         & = 15
\end{array}
\]

\[
\begin{array}{rl}
product(\{1, 2, 3, 4, 5 \}) & = 1 \times (2 \times (3 \times (4 \times 5))) \\
         & = 1 \times (2 \times (3 \times 20)) \\
         & = 1 \times (2 \times 60) \\
         & = 1 \times 120 \\
         & = 120
\end{array}
\]

In functional programming, we name this process {\em folding}, and particularly, since we execute from
the most inner structure, which starts from the right-most one. This type of folding is named
{\em folding right}.

\be
foldr(f, z, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  z & L = \phi \\
  f(l_1, foldr(f, z, L')) & otherwise
  \end{array}
\right.
\ee

Let's see how to use fold-right to realize sum and product.

\be
\begin{array}{rl}
\sum_{i=1}^{N} x_i & = x_1 + (x_2 + (x_3 + ... + (x_{N_1} + x_{N}))...) \\
             & = foldr(+, 0, \{x_1, x_2, ..., x_n\})
\end{array}
\ee

\be
\begin{array}{rl}
\prod_{i=1}^{N} x_i & = x_1 \times (x_2 \times (x_3 \times ... + (x_{N_1} \times x_{N}))...) \\
         & = foldr(\times, 1, \{x_1, x_2, ..., x_n\})
\end{array}
\ee

The insertion-sort algorithm can also be defined by using folding right.

\be
sort(L) = foldr(insert, \phi, L)
\ee

\subsection{folding from left}
\index{List!foldl}
\index{List!fold from left}
As mentioned in section of `tail recursive` call. Both pure recursive sum and product compute from right
to left and they must book keep all the intermediate results and contexts. As we abstract fold-right from
the very same structure, folding from right does the book keeping as well. This will be expensive if
the list is very long.

Since we can change the realization of sum and product to tail-recursive call manner, it quite possible
that we can provide another folding algorithm, which processes the list from left to right in normal order,
and enable the tail-call optimization by reusing the same context.

Instead of induction from sum, product and insertion, we can directly change the folding right to tail call.
Observe that the initial value $z$, actually represents the intermediate result at any time. We can use it
as the accumulator.

\be
foldl(f, z, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  z & L = \phi \\
  foldl(f, f(z, l_1), L') & otherwise
  \end{array}
\right.
\ee

Every time when the list isn't empty, we take the first element, apply function $f$ on the accumulator
$z$ and it to get a new accumulator $z' = f(z, l_1)$. After that we can repeatedly folding with the very
same function $f$, the updated accumulator $z'$, and list $L'$.

Let's verify that this tail-call algorithm actually folding from left.

\[
\begin{array}{rl}
\sum_{i=1}^{5}i & = foldl(+, 0, \{1, 2, 3, 4, 5\}) \\
                & = foldl(+, 0 + 1, \{ 2, 3, 4, 5 \}) \\
                & = foldl(+, (0 + 1) + 2 \{3, 4, 5 \} \\
                & = foldl(+, ((0 + 1) + 2) + 3, \{4, 5\}) \\
                & = foldl(+, (((0 + 1) + 2) + 3) + 4, \{5\}) \\
                & = foldl(+, ((((0 + 1) + 2 + 3) + 4 + 5, \phi) \\
                & = 0 + 1 + 2 + 3 + 4 + 5
\end{array}
\]

Note that, we actually delayed the evaluation of $f(z, l_1)$ in every step. (This is the exact behavior
in system support lazy-evaluation, for instance, Haskell. However, in strict system such as standard ML, it's not the case.) Actually, they will be evaluated in sequence
of $\{ 1, 3, 6, 10, 15\}$ in each call.

Generally, folding-left can be expanded in form of

\be
foldl(f, z, L) = f(f(...(f(f(z, l_1), l_2), ..., l_n)
\ee

Or in infix manner as

\be
foldl(f, z, L) = ((...(z \oplus_f l_1) \oplus_f l_2) \oplus_f ...) \oplus l_n
\ee

With folding from left defined, sum, product, and insertion-sort can be transparently implemented by calling
$foldl$ as $sum(L) = foldl(+, 0, L)$, $product(L) = foldl(+, 1, L)$, and $sort(L) = foldl(insert, \phi, L)$.
Compare with the folding-right version, they are almost same at first glares, however, the internal implementation
differs.

\subsubsection{Imperative folding and generic folding concept}
The tail-call nature of folding-left algorithm is quite friendly for imperative settings, that even the compiler
isn't equipped with tail-call recursive optimization, we can anyway implement the folding in while-loop manually.

\begin{algorithmic}[1]
\Function{Fold}{$f, z, L$}
  \While{$L \neq \phi$}
    \State $z \gets f(z, $ \Call{First}{$L$} $)$
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Return $z$
\EndFunction
\end{algorithmic}

Translating this algorithm to Python yields the following example program.

\lstset{language=Python}
\begin{lstlisting}
def fold(f, z, xs):
    for x in xs:
        z = f(z, x)
    return z
\end{lstlisting}

Actually, Python provides built-in function `reduce' which does the very same thing. (in ISO C++, this is
provided as reduce algorithm in STL.) Almost no imperative environment provides folding-right function because
it will cause stack overflow problem if the list is too long. However, there still exist cases that the folding from right
semantics is necessary. For example, one defines a container, which only provides insertion function to
the head of the container, but there is no any appending method, so that we want such a $fromList$
tool.

\[
fromList(L) = foldr(insertHead, empty, L)
\]

Calling $fromList$ with the insertion function as well as an empty initialized container, can turn a list
into the special container. Actually the singly linked-list is such a container, which performs well
on insertion to the head, but poor to linear time if appending on the tail. Folding from right is quite
nature when duplicate a linked-list while keeps the elements ordering. While folding from left will generate
a reversed list.

In such cases, there exists an alternative way to implement imperative folding right by first reverse the list, and then
folding the reversed one from left.

\begin{algorithmic}[1]
\Function{Fold-Right}{$f, z, L$}
  \State \Return \textproc{Fold}($f, z$, \Call{Reverse}{$L$})
\EndFunction
\end{algorithmic}

Note that, here we must use the tail-call version of reversing, or the stack overflow issue still exists.

One may think that folding-left should be chosen in most cases over folding-right because it's friendly for
tail-recursion call optimization, suitable for both functional and imperative settings, and it's an online
algorithm. However, folding-right plays a critical role when the input list is infinity and the binary function
$f$ is lazy. For example, below Haskell program wraps every element in an infinity list to a singleton, and
returns the first 10 result.

\lstset{language=Haskell}
\begin{lstlisting}
take 10 $ foldr (\x xs ->[x]:xs) [] [1..]
[[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]
\end{lstlisting} %$

This can't be achieved by using folding left because the outer most evaluation can't be finished until
all the list being processed. The details is specific to lazy evaluation feature, which is out of the
scope of this book. Readers can refer to \cite{Haskell-wiki} for details.

Although the main topic of this appendix is about singly linked-list related algorithms, the folding
concept itself is generic which doesn't only limit to list, but also can be applied to other data structures.

We can fold a tree, a queue, or even more complicated data structures as long as we have the following:

\begin{itemize}
\item The empty data structure can be identified for trivial edge case; (e.g. empty tree)
\item We can traverse the data structure (e.g. traverse the tree in pre-order).
\end{itemize}

Some languages provide this high-level concept support, for example, Haskell achieve this via
{\em monoid}, readers can refer to \cite{learn-haskell} for detail.

There are many chapters in this book use the widen concept of folding.

\subsection{folding in practice}

We have seen that $max$, $min$, and insertion sort all can be realized in folding. The
brute-force solution for `drunk jailer' puzzle shown in mapping section can also be
designed by mixed use of mapping and folding.

Remind that we create a list of pairs, each pair contains the number of the light, and
the on-off state. After that we process from 1 to $n$, switch the light if the number
can be divided. The whole process can be viewed as folding.

\[
fold(step, \{(1, 0), (2, 0), ..., (n, 0) \}, \{1, 2, ..., n\})
\]

The initial value is the very first state, that all the lights are off. The list to be
folding is the operations from 1 to $n$. Function $step$ takes two arguments, one is
the light states pair list, the other is the operation time $i$. It then maps
on all lights and performs switching. We can then substitute the $step$ with mapping.

\[
fold(\lambda_{L, i} \cdot map(switch(i), L), \{(1, 0), (2, 0), ..., (n, 0) \}, \{1, 2, ..., n\})
\]

We'll simplify the $\lambda$ notation, and directly write $map(switch(i), l)$ for brevity purpose.
The result of this folding is the final states pairs, we need take the second one of the pair
for each element via mapping, then calculate the summation.

\be
sum(map(snd, fold(map(switch(i), L), \{(1, 0), (2, 0), ..., (n, 0) \}, \{1, 2, ..., n\})))
\ee

There are materials provides plenty of good examples of using folding, especially in \cite{fp-pearls},
folding together with fusion law are well explained.

\subsubsection{concatenate a list of list}
\index{List!concats}
In previous section \ref{concat} about concatenation, we explained how to concatenate two lists.
Actually, concatenation of lists can be considered equivalent to summation of numbers. Thus we
can design a general algorithm, which can concatenate multiple lists into one big list.

What's more, we can realize this general concatenation by using folding. As sum can be represented
as $sum(L) = foldr(+, 0, L)$, it's straightforward to write the following equation.

\be
concats(L) = foldr(concat, \phi, L)
\ee

Where $L$ is a list of list, for example $\{\{1, 2, 3\}, \{4, 5, 6\}, \{7, 8, 9\}, ...\}$. Function
$concat(L_1, L_2)$ is what we defined in section \ref{concat}.

In some environments which support lazy-evaluation, such as Haskell, this algorithm is capable to
concatenate infinite list of list, as the binary function \verb|++| is lazy.

\begin{Exercise}
\begin{itemize}
\item What's the performance of $concats$ algorithm? is it linear or quadratic?
\item Design another linear time $concats$ algorithm without using folding.
\item Realize mapping algorithm by using folding.
\end{itemize}
\end{Exercise}

\section{Searching and matching}

Searching and matching are very important algorithms. They are not only limited to linked list, but also
applicable to a wide range of data structures.
We just scratch the surface of searching and matching in this appendix. There are dedicated chapters explain about them
in this book.

\subsection{Existence testing}
\index{List!elem}
\index{List!existence testing}

The simplest searching case is to test if a given element exists in a list. A linear time traverse
can solve this problem. In order to determine element $x$ exists in list $L$:

\begin{itemize}
\item If the list is empty, it's obvious that the element doesn't exist in $L$;
\item If the first element in the list equals to $x$, we know that $x$ exists;
\item Otherwise, we need recursively test if $x$ exists in the rest sub-list $L'$;
\end{itemize}

This simple description can be directly formalized to equation as the following.

\be
x \in L =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  False & L = \phi \\
  True & l_1 = x \\
  x \in L' & otherwise
  \end{array}
\right.
\ee

This is definitely a linear algorithm which is bound to $O(n)$ time. The best case
happens in the two trivial clauses that either the list is empty or the first element
is what we are finding; The worst case happens when the element doesn't exist at all
or it is the last element. In both cases, we need traverse the whole list. If the probability
is equal for all the positions, the average case takes about $\frac{N+1}{2}$ steps
for traversing.

This algorithm is so trivial that we left the implementation as exercise to the reader.
If the list is ordered, one may expect to improve the algorithm to logarithm time
but not linear. However, as we discussed, since list doesn't support constant time
random accessing, binary search can't be applied here. There is a dedicated chapter
in this book discusses how to evolve the linked list to binary tree to achieve
quick searching.

\subsection{Looking up}
\index{List!lookup}
One extra step from existence testing is to find the interesting information stored in the list.
There are two typical methods to augment extra data to the element. Since the linked list is chain
of nodes, we can store satellite data in the node, then provide $key(n)$ to access the
key of the node, $rest(n)$ for the rest sub-list, and $value(n)$ for the augmented data.
The other method, is to pair the key and data, for example $\{(1, hello), (2, world), (3, foo), ...\}$.
We'll introduce how to form such pairing list in later section.

The algorithm is almost as same as the existence testing, that it traverses the list, examines
the key one by one. Whenever it finds a node which has the same key as what we are looking up,
it stops, and returns the augmented data. It is obvious that this is linear strategy.
If the satellite data is augmented to the node directly,
the algorithm can be defined as the following.

\be
lookup(x, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  value(l_1) & key(l_1) = x \\
  lookup(x, L') & otherwise
  \end{array}
\right.
\ee

In this algorithm, $L$ is a list of nodes which are augmented with satellite data. Note that
the first case actually means looking up failure, so that the result is empty. Some functional
programming languages, such as Haskell, provide \verb|Maybe| type to handle the possibility of
fail. This algorithm can be slightly modified to handle the key-value pair list as well.

\be
lookup(x, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  snd(l_1) & fst(l_1) = x \\
  lookup(x, L') & otherwise
  \end{array}
\right.
\ee

Here $L$ is a list of pairs, functions $fst(p)$ and $snd(p)$ access the first part and second part
of the pair respectively.

Both algorithms are in tail-call manner, they can be transformed to imperative looping easily. We
left this as exercise to the reader.

\subsection{finding and filtering}
\index{List!find}
\index{List!filter}

Let's take one more step ahead, looking up algorithm performs linear search by comparing the
key of an element is equal to the given value. A more general case is to find an element matching
a certain predicate. We can abstract this matching condition as a parameter for this generic
linear finding algorithm.

\be
find(p, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  l_1 & p(l_1) \\
  find(p, L') & otherwise
  \end{array}
\right.
\ee

The algorithm traverses the list by examining if the element satisfies the predicate $p$. It
fails if the list is empty while there is still nothing found. This is handled in the first
trivial edge case; If the first element in the list satisfies the condition, the algorithm
returns the whole element (node), and user can further handle it as he like (either extract
the satellite data or do whatever); otherwise, the algorithm recursively perform finding
on the rest of the sub-list. Below is the corresponding Haskell example program.

\lstset{language=Haskell}
\begin{lstlisting}
find _ [] = Nothing
find p (x:xs) = if p x then Just x else find p xs
\end{lstlisting}

Translating this to imperative algorithm is straightforward. Here we use 'NIL' to represent
the fail case.

\begin{algorithmic}[1]
\Function{Find}{$p, L$}
  \While{$L \neq \phi$}
    \If{$p$(\Call{First}{$L$})}
      \State \Return \Call{First}{$L$}
    \EndIf
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Return NIL
\EndFunction
\end{algorithmic}

And here is the Python example of finding.

\lstset{language=Python}
\begin{lstlisting}
def find(p, xs):
    while xs is not None:
        if p(xs.key):
            return xs
        xs = xs.next
    return None
\end{lstlisting}

It is quite possible that there are multiple elements in the list which satisfy the precondition.
The finding algorithm designed so far just picks the first one it meets, and stops immediately.
It can be considered as a special case of finding all elements under a certain condition.

Another viewpoint of finding all elements with a given predicate is to treat the finding algorithm
as a black box, the input to this box is a list, while the output is another list contains
all elements satisfying the predicate. This can be called as filtering as shown in the below
figure.

\begin{figure}[htbp]
        \centering
        \includegraphics[scale=0.8]{img/filter.ps}
        \caption{The input is the original list $\{x_1, x_2, ..., x_n\}$, the output is a list $\{x_1', x_2', ..., x_m'\}$, that for $\forall x_i'$, predicate $p(x_i')$ is satisfied.} \label{fig:filter}
\end{figure}

This figure can be formalized in another form in taste of set enumeration. However, we actually
enumerate among list instead of a set.

\be
filter(p, L) = \{ x | x \in L \land p(x) \}
\ee

Some environment such as Haskell (and Python for any iterable), supports this form as list comprehension.

\lstset{language=Haskell}
\begin{lstlisting}
filter p xs = [ x | x <- xs, p x]
\end{lstlisting}

And in Python for built-in list as

\lstset{language=Python}
\begin{lstlisting}
def filter(p, xs):
    return [x for x in xs if p(x)]
\end{lstlisting}

Note that the Python built-in list isn't singly-linked list as we mentioned in this appendix.

In order to modify the finding algorithm to realize filtering, the found elements are appended
to a result list. And instead of stopping the traverse, all the rest of elements should be examined
with the predicate.

\be
filter(p, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  cons(l_1, filter(p, L')) & p(l_1) \\
  filter(p, L') & otherwise
  \end{array}
\right.
\ee

This algorithm returns empty result if the list is empty for trivial edge case; For non-empty list,
suppose the recursive result of filtering the rest of the sub-list is $A$, the algorithm examine
if the first element satisfies the predicate, it is put in front of $A$ by a `cons' operation ($O(1)$ time).

The corresponding Haskell program is given as below.

\lstset{language=Haskell}
\begin{lstlisting}
filter _ [] = []
filter p (x:xs) = if p x then x : filter p xs else filter p xs
\end{lstlisting}

Although we mentioned that the next found element is `appended' to the result list, this algorithm
actually constructs the result list from the right most to the left, so that appending
is avoided, which ensure the linear $O(n)$ performance. Compare this algorithm with the following
imperative quadratic realization reveals the difference.

\begin{algorithmic}[1]
\Function{Filter}{$p, L$}
  \State $L' \gets \phi$
  \While{$L \neq \phi$}
    \If{$p$(\Call{First}{$L$})}
      \State $L' \gets$ \textproc{Append}($L'$, \Call{First}{$L$}) \Comment{Linear operation}
    \EndIf
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
\EndFunction
\end{algorithmic}

As the comment of appending statement, it's typically proportion to the length of the result list
if the tail position isn't memorized. This fact indicates that directly transforming the recursive filter
algorithm into tail-call form will downgrade the performance from $O(n)$ to $O(n^2)$. As shown
in the below equation, that $filter(p, L) = filter'(p, L, \phi)$ performs as poorly as the
imperative one.

\be
filter'(p, L, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \phi \\
  filter'(p, L', A \cup \{l_1\}) & p(l_1) \\
  filter'(p, L', A) & otherwise
  \end{array}
\right.
\ee

One solution to achieve linear time performance imperatively is to construct the result list in
reverse order, and perform the $O(n)$ reversion again (refer to the above section) to get the final result.
This is left as exercise to the reader.

The fact of construction the result list from right to left indicates the possibility of realizing
filtering with folding-right concept. We need design some combinator function $f$, so that
$filter(p, L) = foldr(f, \phi, L)$. It requires that function $f$ takes two arguments, one
is the element iterated among the list; the other is the intermediate result constructed
from right. $f(x, A)$ can be defined as that it tests the predicate against $x$, if succeed,
the result is updated to $cons(x, A)$, otherwise, $A$ is kept same.

\be
f(x, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  cons(x, A) & p(x) \\
  A & otherwise
  \end{array}
\right.
\ee

However, the predicate must be passed to function $f$ as well. This can be achieved by using
currying, so $f$ actually has the prototype $f(p, x, A)$, and filtering is defined as following.

\be
filter(p, L) = foldr(\lambda_{x, A} \cdot f(p, x, A), \phi, L)
\ee

Which can be simplified by $\eta$-conversion. For detailed definition of $\eta$-conversion,
readers can refer to \cite{slpj-book-1987}.

\be
filter(p, L) = foldr(f(p), \phi, L)
\ee

The following Haskell example program implements this equation.

\lstset{language=Haskell}
\begin{lstlisting}
filter p = foldr f [] where
    f x xs = if p x then x : xs else xs
\end{lstlisting}

Similar to mapping and folding, filtering is actually a generic concept, that we can apply
a predicate on any traversable data structures to get what we are interesting. readers can
refer to the topic about monoid in \cite{learn-haskell} for further reading.

\subsection{Matching}
\index{List!matching}
\index{List!prefix}
\index{List!suffix}
\index{List!infix}

Matching generally means to find a given pattern among some data structures. In this section,
we limit the topic within list. Even this limitation will leads to a very wide and deep topic,
that there are dedicated chapters in this book introduce matching algorithms. So we only select
the algorithm to test if a given list exists in another (typically longer) list.

Before dive into the algorithm of finding the sub-list at any position, two special edge cases
are used for warm up. They are algorithms to test if a given list is either prefix or suffix
of another.

In the section about span, we have seen how to find a prefix under a certain condition.
prefix matching can be considered as a special case in some sense. That it compares each
of the elements between the two lists from the beginning until meets any different elements
or pass the end of one list. Define $P \subseteq L$ if $P$ is prefix of $L$.

\be
P \subseteq L = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  True & P = \phi \\
  False & p_1 \neq l_1 \\
  P' \subseteq L' & otherwise
  \end{array}
\right.
\ee

This is obviously a linear algorithm. However, We can't use the very same approach
to test if a list is suffix of another because it isn't cheap to start from the
end of the list and keep iterating backwards. Arrays, on the other hand which support
random access can be easily traversed backwards.

As we only need the yes-no result, one solution to realize a linear suffix testing
algorithm is to reverse both lists, (which is linear time), and use prefix testing
instead. Define $L \supseteq P$ if $P$ is suffix of $L$.

\be
L \supseteq P = reverse(P) \subseteq reverse(L)
\ee

With $\subseteq$ defined, it enables to test if a list is infix of another.
The idea is to traverse the target list, and repeatedly applying the prefix testing
till any success or arrives at the end.

\begin{algorithmic}[1]
\Function{Is-Infix}{$P, L$}
  \While{$L \neq \phi$}
    \If{$P \subseteq L$}
      \State \Return TRUE
    \EndIf
    \State $L \gets$ \Call{Rest}{$L$}
  \EndWhile
  \State \Return FALSE
\EndFunction
\end{algorithmic}

Formalize this algorithm to recursive equation leads to the below definition.

\be
infix?(P, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  True & P \subseteq L \\
  False & L = \phi \\
  infix?(P, L') & otherwise
  \end{array}
\right.
\ee

Note that there is a tricky implicit constraint in this equation. If the pattern $P$ is empty,
it is definitely the infix of any target list. This case is actually covered by the first condition
in the above equation because empty list is also the prefix of any list. In most programming languages
support pattern matching, we can't arrange the second clause as the first edge case, or it will
return false for $infix?(\phi, \phi)$. (One exception is Prolog, but this is a language specific
feature, which we won't covered in this book.)

Since prefix testing is linear, and it is called while traversing the list, this algorithm
is quadratic $O(nm)$. where $n$ and $m$ are the length of the pattern and target lists respectively.
There is no trivial way to improve this `position by position' scanning algorithm to linear
even if the data structure changes from linked-list to randomly accessible array.

There are chapters in this book introduce several approaches for fast matching, including
suffix tree with Ukkonen algorithm, Knuth-Morris-Pratt algorithm and Boyer-Moore algorithm.

Alternatively, we can enumerate all suffixes of the target list, and check if the pattern
is prefix of any these suffixes. Which can be represented as the following.

\be
infix?(P, L) = \exists S \in suffixes(L) \land P \subseteq S
\ee

This can be represented as list comprehension, for example the below Haskell program.

\lstset{language=Haskell}
\begin{lstlisting}
isInfixOf x y = (not . null) [ s | s <- tails(y), x `isPrefixOf`s]
\end{lstlisting}

Where function \verb|isPrefixOf| is the prefixing testing function defined according to
our previous design. function \verb|tails| generate all suffixes of a list. The implementation
of \verb|tails| is left as an exercise to the reader.

\begin{Exercise}
\begin{itemize}
\item Implement the linear existence testing in both functional and imperative approaches in
your favorite programming languages.
\item Implement the looking up algorithm in your favorite imperative programming language.
\item Realize the linear time filtering algorithm by firstly building the result list in reverse
order, and finally reverse it to resume the normal result. Implement this algorithm in both
imperative looping and functional tail-recursion call.
\item Implement the imperative algorithm of prefix testing in your favorite programming language.
\item Implement the algorithm to enumerate all suffixes of a list.
\end{itemize}
\end{Exercise}

\section{zipping and unzipping}
\index{List!zip}
\index{List!unzip}

It is quite common to construct a list of paired elements. For example, in the naive
brute-force solution for 'Drunk jailer' puzzle which is shown in section of mapping,
we need to represent the state of all lights. It is initialized as $\{(1, 0), (2, 0), ..., (n, 0)\}$.
Another example is to build a key-value list, such as $\{(1, a), (2, an), (3, another), ... \}$.

In 'Drunk jailer' example, the list of pairs is built like the following.

\[
map(\lambda_i \cdot (i, 0), \{1, 2, ..., n\})
\]

The more general case is that, There have been already two lists prepared, what we need
is a handy `zipper' method.

\be
zip(A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & A = \phi \lor B = \phi \\
  cons((a_1, b_1), zip(A', B')) & otherwise
  \end{array}
\right.
\ee

Note that this algorithm is capable to handle the case that the two lists being zipped have different
lengths. The result list of pairs aligns with the shorter one. And it's even possible to zip
an infinite list with another one with limited length in environment support lazy evaluation.
For example with this auxiliary function defined,
we can initialize the lights state as

\[
zip(\{0, 0, ...\}, \{1, 2, ..., n\}
\]

In some languages support list enumeration, such as Haskell (Python provides similar \verb|range| function, but it
manipulates built-in list, which isn't linked-list actually), this can be expressed as \verb|zip (repeat 0) [1..n]|.
Given a list of words, we can also index them with consecutive numbers as

\[
zip(\{1, 2, ...\}, \{a, an, another, ...\})
\]

Note that the zipping algorithm is linear, as it uses constant time `cons' operation in each recursive call.
However, directly translating $zip$ into imperative manner would down-grade the performance to quadratic
unless the linked-list is optimized with tail position cache or we in-place modify one of the passed-in list.

\begin{algorithmic}[1]
\Function{Zip}{$A, B$}
  \State $C \gets \phi$
  \While{$A \neq \phi \land B \neq \phi$}
    \State $C \gets $ \textproc{Append}(C, (\Call{First}{$A$}, \Call{First}{$B$}))
    \State $A \gets$ \Call{Rest}{$A$}
    \State $B \gets$ \Call{Rest}{$B}$
  \EndWhile
  \State \Return $C$
\EndFunction
\end{algorithmic}

Note that, the appending operation is proportion to the length of the result list $C$, so it will get
more and more slowly along with traversing. There are three solutions to improve this algorithm to
linear time. The first method is to use a similar approach as we did in infix-testing, that we construct
the result list of pairs in reverse order by always insert the paired elements on head; then perform
a linear reverse operation before return the final result; The second method is to modify one passed-in
list, for example $A$, in-place while traversing. Translate it from list of elements to list of pairs;
The third method is to remember the last appending position. Please try these solutions as exercise.

The key point of linear time zipping is that the result list is actually built from right to left, which
is similar to the infix-testing algorithm. So it's quite possible to provide a folding-right realization.
This is left as exercise to the reader.

It is natural to extend the zipper algorithm so that multiple lists can be zipped to one list of multiple-elements.
For example, Haskell standard library provides, \verb|zip|, \verb|zip3|, \verb|zip4|, ..., till \verb|zip7|.
Another typical extension to zipper is that, sometimes, we don't want to list of pairs (or tuples
more generally), instead, we want to apply some combinator function to each pair of elements.

For example, consider the case that we have a list of unit prices for every fruit: apple, orange, banana, ...,
 as $\{1.00, 0.80, 10.05, ...\}$, with same unit of Dollar; And the cart of customer holds a list
of purchased quantity, for instance $\{3, 1, 0, ...\}$, means this customer, put 3 apples, an orange in the
cart. He doesn't take any banana, so the quantity of banana is zero. We want to generate a list of cost for the
customer, contains how much should pay for apple, orange, banana,... respectively.

The program can be written from scratch as below.

\[
paylist(U, Q) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & U = \phi \lor Q = \phi \\
  cons(u_1 \times q_1, paylist(U', Q')) & otherwise
  \end{array}
\right.
\]

Compare this equation with the zipper algorithm. It is easy to find the common structure of the two, and
we can parameterize the combinator function as $f$, so that the `generic' zipper algorithm can be
defined as the following.

\be
zipWith(f, A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & A = \phi \lor B = \phi \\
  cons(f(a_1, b_1), zipWith(f, A', B')) & otherwise
  \end{array}
\right.
\ee

Here is an example that defines the inner-product (or dot-product)\cite{wiki-dot-product} by using $zipWith$.

\be
A \cdot B = sum(zipWith(\times, A, B))
\ee

It is necessary to realize the inverse operation of zipping, that converts a list of pairs, to different
lists of elements. Back to the purchasing example, It is quite possible that the unit price information
is stored in a association list like $U = \{(apple, 1.00), (orange, 0.80), (banana, 10.05), ...\}$, so that
it's convenient to look up the price with a given product name, for instance, $lookup(melon, U)$. Similarly, the
cart can also be represented clearly in such manner, for example, $Q = \{(apple, 3), (orange, 1), (banana, 0), ...\}$.

Given such a `product - unit price' list and a `product - quantity' list, how to calculate the total payment?

One straight forward idea derived from the previous solution is to extract the unit price list and the purchased
quantity list, then calculate the inner-product of them.

\be
pay = sum(zipWith(\times, snd(unzip(P)), snd(unzip(Q))))
\ee

Although the definition of $unzip$ can be directly written as the inverse of $zip$, here we give a realization based on
folding-right.

\be
unzip(L) = foldr(\lambda_{(a, b), (A, B)} \cdot (cons(a, A), cons(b, B)), (\phi, \phi), L)
\ee

The initial result is a pair of empty list. During the folding process, the head of the list, which is a pair
of elements, as well as the intermediate result are passed to the combinator function. This combinator function
is given as a lambda expression, that it extracts the paired elements, and put them in front of the two
intermediate lists respectively. Note that we use implicit pattern matching to extract the elements from
pairs. Alternatively this can be done by using $fst$, and $snd$ functions explicitly as

\[
\lambda_{p, P} \cdot (cons(fst(p), fst(P)), cons(snd(p), snd(P)))
\]

The following Haskell example code implements $unzip$ algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
unzip = foldr \(a, b) (as, bs) -> (a:as, b:bs) ([], [])
\end{lstlisting}

Zip and unzip concepts can be extended more generally rather than only limiting within linked-list. It is quite
useful to zip two lists to a tree, where the data stored in the tree are paired elements from both lists.
General zip and unzip can also be used to track the traverse path of a collection to mimic the `parent' pointer
in imperative implementations. Please refer to the last chapter of \cite{learn-haskell} for a good treatment.

\begin{Exercise}
\begin{itemize}
\item Design and implement iota ($I$) algorithm, which can enumerate a list with some given parameters. For example:
  \begin{itemize}
  \item $iota(..., n) = \{1, 2, 3, ..., n\}$;
  \item $iota(m, n) = \{m, m+1, m+2, ..., n\}$, Where $m \leq n$;
  \item $iota(m, m+a, ..., n) = \{m, m+a, m+2a, ..., n \}$;
  \item $iota(m, m, ...) = repeat(m) = \{m, m, m, ...\}$;
  \item $iota(m, ...) = \{m, m+1, m+2, ... \}$.
  \end{itemize}
  Note that the last two cases demand generate infinite list essentially. Consider how to represents infinite list?
  You may refer to the streaming and lazy evaluation materials such as \cite{SICP} and \cite{learn-haskell}.
\item Design and implement a linear time imperative zipper algorithm.
\item Realize the zipper algorithm with folding-right approach.
\item For the purchase payment example, suppose the quantity association list only contains those items with
the quantity isn't zero, that instead of a list of $Q = \{(apple, 3), (banana, 0), (orange, 1), ...\}$, it
hold a list like $Q = \{(apple, 3), (orange, 1), ...\}$. The `banana' information is filtered because the customer
doesn't pick any bananas. Write a program, taking the unit-price association list, and this kind of quantity
list, to calculate the total payment.
\end{itemize}
\end{Exercise}

% ================================================================
%                 Short summary
% ================================================================
\section{Notes and short summary}
In this appendix, a quick introduction about how to build, manipulate, transfer, and searching singly
linked list is briefed in both purely functional and imperative approaches. Most of the modern programming
environments have been equipped with tools to handle such elementary data structures. However, such tools
are designed for general purpose cases, Serious programming shouldn't take them as black-boxes.

Since linked-list is so critical that it builds the corner stones for almost all functional programming
environments, just like the importance of array to imperative settings. We take this topic as an appendix
to the book. It is quite OK that the reader starts with the first chapter about binary search tree, which
is a kind of `hello world' topic, and refers to this appendix when meets any unfamiliar list operations.

\begin{Exercise}
\begin{itemize}
\item Develop a program to remove the duplicated elements in a linked-list.
In imperative settings, the duplicated elements should be removed in-place. In purely functional
settings, construct a new list contains the unique elements. The order of the elements
should be kept as their origianl appearence. What is the complexity of the
program? Try to simplify the solution if auxiliary data structures are allowed.
\item A decimal non-negative integer can be represented in linked-list. For example 1024 can be represented as
'$4 \rightarrow 2 \rightarrow 0 \rightarrow 1$'. Generally, $n = d_m...d_2d_1$ can be represented as
'$d_1 \rightarrow d_2 \rightarrow ... \rightarrow d_m$'. Given two numbers $a$, $b$ in linked-list form.
Realize basic arithmetic operations such as plus and minus.
\end{itemize}
\end{Exercise}

% ================================================================
%                 Appendix
% ================================================================

\begin{thebibliography}{99}

\bibitem{fp-pearls}
Richard Bird. ``Pearls of Functional Algorithm Design''. Cambridge University Press; 1 edition (November 1, 2010). ISBN: 978-0521513388

\bibitem{slpj-book-1987}
Simon L. Peyton Jones. ``The Implementation of Functional Programming Languages''. Prentice-Hall International Series in Computer Since. Prentice Hall (May 1987). ISBN: 978-0134533339

\bibitem{moderncxx}
Andrei Alexandrescu. ``Modern C++ design: Generic Programming and Design Patterns Applied''. Addison Wesley February 01, 2001, ISBN 0-201-70431-5

\bibitem{mittype}
Benjamin C. Pierce. ``Types and Programming Languages''. The MIT Press, 2002. ISBN:0262162091

\bibitem{SICP}
Harold Abelson, Gerald Jay Sussman, Julie Sussman. ``Structure and Interpretation of Computer Programs, 2nd Edition''. MIT Press, 1996, ISBN 0-262-51087-1

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{algo-fp}
Fethi Rabhi, Guy Lapalme. ``Algorithms: a functional programming approach''. Second edition. Addison-Wesley, 1999. ISBN: 0201-59604-0

\bibitem{learn-haskell}
Miran Lipovaca. ``Learn You a Haskell for Great Good! A Beginner's Guide''. No Starch Press; 1 edition April 2011, 400 pp. ISBN: 978-1-59327-283-8

\bibitem{erlang}
Joe Armstrong. ``Programming Erlang: Software for a Concurrent World''. Pragmatic Bookshelf; 1 edition (July 18, 2007). ISBN-13: 978-1934356005

\bibitem{wiki-tail-call}
Wikipedia. ``Tail call''. https://en.wikipedia.org/wiki/Tail\_call

\bibitem{sgi-stl-transform}
SGI. ``transform''. http://www.sgi.com/tech/stl/transform.html

\bibitem{poj-drunk-jailer}
ACM/ICPC. ``The drunk jailer.'' Peking University judge online for ACM/ICPC. http://poj.org/problem?id=1218.

\bibitem{Haskell-wiki}
Haskell wiki. ``Haskell programming tips''. 4.4 Choose the appropriate fold. http://www.haskell.org/haskellwiki/Haskell\_programming\_tips

\bibitem{wiki-dot-product}
Wikipedia. ``Dot product''. http://en.wikipedia.org/wiki/Dot\_product

\end{thebibliography}

\ifx\wholebook\relax \else
\end{document}
\fi

% LocalWords:  typedef struct typename
