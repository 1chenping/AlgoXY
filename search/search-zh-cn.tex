\ifx\wholebook\relax \else
% ------------------------

\documentclass[UTF8]{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
\input{../common-zh-cn.tex}

\setcounter{page}{1}
\setcounter{secnumdepth}{4}

\begin{document}

%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{搜索}

\author{刘新宇
\thanks{{\bfseries 刘新宇 } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\maketitle
\fi

\markboth{搜索}{初等算法}

\ifx\wholebook\relax
\chapter{搜索}
\numberwithin{Exercise}{chapter}
\fi

\def\includetikz{}

% ================================================================
%                 Introduction
% ================================================================
\section{简介}
\label{introduction}

搜索是一个巨大并且重要的领域。计算机使得很多困难的搜索问题得以实现。某些问题由人来解决的话几乎是不可能的。现代的工业机器人可以在生产线的一堆零件中找出正确的零件进行组装；带有全球卫星导航系统GPS的汽车可以在地图中找到前往目的地的最佳路线。带有地图导航系统的现代手机还能搜索到最便宜的购物方案。

本章仅仅介绍基本搜索算法中最简单的内容。计算机的一大优点就是可以在巨大的序列中进行暴力扫描。我们通过两个题目来介绍分而治之的搜索策略：一个是在未排序的序列中寻找第$k$大的元素；另一个是在已序序列中进行二分查找。我们还将介绍多位数据中的二分查找。

文本搜索是日常生活中的重要应用。本章介绍两种常见的文本搜索算法：Knuth-Morris-Pratt（简称KMP）算法，和Boyer-Moore算法。它们体现了另一种重要的搜索策略——信息重用。

除了序列搜索，我们还会介绍一些基本的算法用来寻找某些问题的解。它们被广泛用于早期的人工智能领域，包括基本的深度优先搜索（DFS）和广度优先搜索（BFS）。

最后我们会简单介绍动态规划，用于寻找问题的最优解。我们同时会介绍贪心算法，特别适合用来解决某些特定问题。

% ================================================================
% Sequence search
% ================================================================
\section{序列搜索}

虽然现代计算机可以高速地进行暴力查找，即使假设“摩尔定律”被严格遵守，数据增长的速度还是远远超过暴力查找的能力。在本书的最开始，我们就介绍了这样的例子。这就是人们为何不断研究计算机搜索算法的原因。

\subsection{分而治之的搜索}

分而治之是一种常用的解法。我们可以不断地缩小搜索范围，丢弃无需查找的数据。这样就能显著提高搜索的速度。

\subsubsection{$k$选择问题}
\index{选择算法}

考虑在$n$个元素中寻找第$k$小的元素。最直观的想法是先找到最小的一个，将其丢弃，然后再剩余元素中寻找第二小的元素。重复这一寻找最小值再丢弃的步骤$k$次就可以找到第$k$小的元素。在$n$个元素中寻找最小的元素是线性时间$O(n)$的。因此这一方法的性能为$O(kn)$。

另一种方法是使用我们此前介绍过的堆（heap）数据结构。无论何种堆，例如使用数组实现的隐式二叉堆、斐波那契堆或其它堆，获取堆顶元素再弹出通常性能为$O(\lg n)$。因此这一方法，如式(\ref{eq:kth-heap1})和(\ref{eq:kth-heap2})所示，找到第$k$小元素的性能为$O(k \lg n)$。

\be
top(k, L) = find(k, heapify(L))
\label{eq:kth-heap1}
\ee

\be
find(k, H) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  top(H) & k = 0 \\
  find(k-1, pop(H)) & otherwise
  \end{array}
\right.
\label{eq:kth-heap2}
\ee

但是，使用堆的解法相对比较复杂。是否存在有一种简单、快速的方法能找到第$k$小的元素呢？

我们可以使用分而治之的方法来解决这一问题。如果将全部元素划分为两个子序列$A$和$B$，使得$A$中的全部元素都小于等于$B$中的任何元素，我们就可按照下面的方法减小问题的规模\footnote{这需要给出一个序列$L$中第$k$小的元素的精确定义：它等于序列$L'$中的第$k$个元素，其中$L'$是$L$的一个排列，并且$L'$满足单调非递减的顺序。}：

\begin{enumerate}
\item 比较子序列$A$的长度和$k$的大小；
\item 若$k < |A|$，则第$k$小的元素必然在$A$中，我们可以丢弃子序列$B$，然后在$A$中\underline{进一步查找}；
\item 若$|A| < k$，则第$k$小的元素必然在$B$中，我们可以丢弃子序列$A$，然后在$B$中\underline{进一步查找}第$(k-|A|)$小的元素。
\end{enumerate}

注意\underline{下划线}部分强调了递归的特性。理想情况下，我们总是将序列划分为相等长度的两个子序列$A$和$B$，因此每次都将问题的规模减半。因此性能为线性时间$O(n)$。

关键问题是如何实现划分，将前$m$小的元素放入一个子序列中，将剩余元素放入另一个中。

回忆快速排序中的划分算法，它将所有小于pivot的元素移动到前面，将大于pivot的元素移动到后面。根据这一思路，我们可以构造一个分而治之的$k$选择算法，称为“快速选择算法”。

\begin{enumerate}
\item 随机选择一个元素（例如第一个）作为pivot；
\item 将所有不大于pivot的元素放入子序列$A$；将剩余元素放入子序列$B$；
\item 比较$A$的长度和$k$，若$|A| = k - 1$，则pivot就是第$k$小的元素；
\item 若$|A| > k - 1$，递归在$A$中寻找第$k$小的元素；
\item 否则，递归在$B$中寻找第$(k - |A|)$小的元素；
\end{enumerate}

这一算法可以形式化为下面的等式。设$0 < k \leq |L|$，其中$L$是一个非空列表。记$l_1$为$L$中的第一个元素，它被选作pivot；$L'$包含除$l_1$外的剩余元素。$(A, B) = partition(\lambda_x \cdot x \leq l_1, L')$。它使用快速排序中介绍的算法将$L'$划分为两部分。

\be
top(k, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & |A| = k - 1 \\
  top(k - 1 - |A|, B) & |A| < k - 1 \\
  top(k, A) & otherwise
  \end{array}
\right.
\ee

\be
partition(p, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, \Phi) & L = \Phi \\
  (\{ l_1 \} \cup A, B) & p(l_1), (A, B) = partition(p, L') \\
  (A, \{ l_1 \} \cup B) & \lnot p(l_1)
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
top n (x:xs) | len == n - 1 = x
             | len < n - 1 = top (n - len - 1) bs
             | otherwise = top n as
    where
      (as, bs) = partition (<=x) xs
      len = length as
\end{lstlisting}

Haskell的标准库中提供了\texttt{partition}函数，具体实现可以参考前面关于快速排序的章节。

最幸运的情况下，第$k$个元素一开始就恰好被选为pivot。划分函数检查全部列表，发现有$k-1$个元素不大于pivot，搜索在$O(n)$时间完成。最差情况下，每次都选择了待查找序列中的最大值或者最小值作为pivot。划分的结果中，$A$或者$B$之一总有一个为空。如果每次总选择最小的元素作为pivot，则性能为$O(kn)$。如果每次总选择最大的元素作为pivot，则性能为$O((n-k)n)$。如果$k$远远小于$n$，则性能下降为平方级别$O(n^2)$。

最好情况（不是最幸运情况）是每次pivot恰好完美划分列表。$A$的长度和$B$的长度几乎相同。序列每次减半。这样总共需要$O(\lg n)$次划分，每次划分的时间和不断减半的序列长度成正比。因此总体性能为$O(n + \frac{n}{2} + \frac{n}{4} + ... + \frac{n}{2^m})$，其中$m$是满足不等式$\frac{n}{2^m} < k$的最小整数。对上述序列求和结果为$O(n)$。

平均情况的性能分析需要使用数学期望。方法和快速排序的平均性能分析类似。我们将其作为练习留给读者。和快速排序类似，这一分而治之的选择算法在实际中的绝大部分情况下表现良好。我们可以使用和快速排序中同样的工程方法，例如三点中值法（median-of-three）或随机pivot选择来减少最差情况的发生。如下面的命令式实现所示：

\begin{algorithmic}[1]
\Function{Top}{$k, A, l, u$}
  \State \textproc{Exchange} $A[l] \leftrightarrow A[$ \Call{Random}{$l, u$} $]$ \Comment{Randomly select in $[l, u]$}
  \State $p \gets$ \Call{Partition}{$A, l, u$}
  \If{$p - l + 1 = k$}
    \State \Return $A[p]$
  \EndIf
  \If{$k < p - l + 1$}
    \State \Return \Call{Top}{$k, A, l, p-1$}
  \EndIf
  \State \Return \Call{Top}{$k - p + l - 1, A, p + 1, u$}
\EndFunction
\end{algorithmic}

这一算法在数组$A$的闭区间$[l, u]$范围内（包括边界上的元素）搜索第$k$小的元素。首先随机选择一个位置，然后把这一位置上的元素选为pivot并和第一个元素交换。划分算法在数组内移动元素，并返回最终pivot所在的位置。如果pivot的最终位置恰好是$k$，则搜索结束；如果不大于pivot的元素个数多于$k-1$个，算法就递归在范围$[l, p-1]$内搜索第$k$小的元素；否则，我们从$k$中减去不大于pivot的元素个数，然后递归在$[p+1, u$内搜索。

有多种方法可以用来实现划分算法，例如下面给出的基于N. Lumoto方法的实现。其他实现我们作为练习留给读者。

\begin{algorithmic}[1]
\Function{Partition}{A, l, u}
  \State $p \gets A[l]$
  \State $L \gets l$
  \For{$R \gets l+1$ to $u$}
    \If{$\lnot (p < A[R])$}
      \State $L \gets L + 1$
      \State \textproc{Exchange} $A[L] \leftrightarrow A[R]$
    \EndIf
  \EndFor
  \State \textproc{Exchange} $A[L] \leftrightarrow p$
  \State \Return $L$
\EndFunction
\end{algorithmic}

下满的C语言例子程序实现了这一算法。它处理了某些特殊的情况。一种是数组为空的情况，另一种是$k$超出了数组边界的情况。这些情况下它返回-1表示搜索失败。

\lstset{language=C}
\begin{lstlisting}
int partition(Key* xs, int l, int u) {
    int r, p = l;
    for (r = l + 1; r < u; ++r)
        if (!(xs[p] < xs[r]))
            swap(xs, ++l, r);
    swap(xs, p, l);
    return l;
}

/* The result is stored in xs[k], returns k if u-l >=k, otherwise -1 */
int top(int k, Key* xs, int l, int u) {
    int p;
    if (l < u) {
        swap(xs, l, rand() % (u - l) + l);
        p = partition(xs, l, u);
        if (p - l + 1 == k)
            return p;
        return (k < p - l + 1) ? top(k, xs, l, p) :
                                 top(k- p + l - 1, xs, p + 1, u);
    }
    return -1;
}
\end{lstlisting}

Blum、Floyd、PrattRivest和Tarjan在1973年给出了一个方法，可以保证在最差情况下的性能仍然为$O(n)$\cite{CLRS}、\cite{median-of-median}。它将列表划分为若干小组，每组最多5个元素。每组的中值（median）可以很快确定。这样总共选出$\frac{n}{5}$个中值。我们重复这一步骤，再将选出的值分成若干不超过五个元素的组，并选出“中值的中值”（median of median）。显然可以在$O(\lg n)$时间内选出最终“真正”的中值。这是划分列表的最佳pivot。接下来，我们用这一pivot划分列表，将问题规模缩小一半，然后递归寻找第$k$小的元素。性能可以计算如下：

\be
T(n) = c_1 lg n + c_2 n + T(\frac{n}{2})
\ee

其中$c_1$是计算“中值的中值”的常数系数，$c_2$是划分的常数系数。可以使用telescope方法解此方程，或者直接用master定理\cite{CLRS}可以得到性能为$O(n)$。算法的具体实现留给读者作为练习。

如果需要选出前$k$小的元素，而无需关心它们的具体顺序，我们通过可以调整上面的算法来满足这一需要：

\be
tops(k, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & k = 0 \lor L = \Phi \\
  A & |A| = k \\
  A \cup \{ l_1 \} \cup tops(k - |A| - 1, B) & |A| < k \\
  tops(k, A) & otherwise
  \end{array}
\right.
\ee

其中$A$、$B$的定义和此前一样，若$L$不为空，则：$(A, B) = partition(\lambda_x \cdot x \leq l_1, L')$。下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
tops _ [] = []
tops 0 _  = []
tops n (x:xs) | len ==n = as
              | len < n  = as ++ [x] ++ tops (n-len-1) bs
              | otherwise = tops n as
    where
      (as, bs) = partition (<= x) xs
      len = length as
\end{lstlisting}

\subsubsection{二分查找}
\index{Binary search}

二分查找是另一种常见的分而治之算法。我们曾经在插入排序一章提到过。我的中学数学老师曾经表演过这样的“魔术”：我首先想好一个不大于1000的数，不说出来。然后他接下来问我一些问题，我只需要回答是或者不是。他需要在十个问题之内猜出那个数。他通常会问这样一些问题：

\begin{itemize}
\item 是偶数么？
\item 是素数么？
\item 所有位上的数字都相同么？
\item 能被3整除么？
\item ……
\end{itemize}

大多数情况下，我的数学老师总能在十个问题内猜到答案。我和同学们都感到很惊奇。

曾经有一段时间，电视里热播这样的价格竞猜节目。主持人展示一件商品，然后现场的幸运观众需要在30秒内猜出价格。对于每次猜测，主持人告知是猜高了，还是猜低了。如果观众能够在30秒内猜到正确价格，就可以拿走商品。最好的竞猜策略就是分而治之的二分查找。我们常常可以看到下面这样的猜测和反馈：

\begin{itemize}
\item 观众：1000元；
\item 主持人：高了；
\item 观众: 500元；
\item 主持人：低了；
\item 观众：750元；
\item 主持人：低了；
\item 观众：890元；
\item 主持人：低了；
\item 观众：990元；
\item 主持人：正确！
\end{itemize}

我的数学老师说，因为数字不大于1000，如果通过设计良好的问题，每次能排除一半可能的数字，就可以在10次内找出答案。这是因为$2^{10} = 1024 > 1000$。但是，如果简单地问“比500大么？比250小么？……”就太枯燥了。而问题“是偶数么？”就是一个非常好的问题，它总是能去掉一半的数字。

回到二分查找的问题上。它只能在已序的序列中进行查找。我曾经看到有人试图对未排序的数组进行二分查找，花了几个小时也没有搞清楚为什么不正确。二分查找的思路很直观，为了在已序序列$A$中寻找数字$x$，我们首先检查中点上的数字，和$x$进行比较，如果恰好相等，则它就是答案，查找结束；如果$x$较小，由于$A$是已序的，我们只需要在前半部分中继续查找；否则，我们在后半部分中继续查找。如果当$A$变成空序列，而我们仍未找到$x$，则说明$x$不存在序列中。

在给出形式化的算法定义前，有一个很令人吃惊的事实。高德纳（Donald Knuth）指出：“虽然二分查找的基本思想相对直观，具体细节却复杂得不可思议……”。Jon Bentley指出，大多数二分查找的实现中含有错误。并且他本人在《编程珠玑》（Programming pearls）第一版中给出实现也隐藏了一个错误，直到20多年后才被发现\cite{programming-pearls}。

有两种实现，一种是递归的，另一种是迭代的。上面给出的描述，实际就是递归的解法。令数组的上下界分别为$l$和$u$，不包含$u$位置上的元素。

\begin{algorithmic}[1]
\Function{Binary-Search}{$x, A, l, u$}
  \If{$u < l$}
    \State Not found error
  \Else
     \State $m \gets l + \lfloor \frac{u - l}{2} \rfloor$ \Comment{避免计算$\lfloor \frac{l+u}{2} \rfloor$溢出}
     \If{$A[m] = x$}
       \State \Return $m$
     \EndIf
     \If{$x < A[m]$}
       \State \Return \Call{Binary-Search}{x, A, l, m - 1}
     \Else
       \State \Return \Call{Binary-Search}{x, A, m + 1, u}
     \EndIf
  \EndIf
\EndFunction
\end{algorithmic}

如注释中强调的，因为使用有限的字节表示整数，我们不能简单地用$\lfloor \frac{l+u}{2} \rfloor$来计算中点，如果$l$和$u$很大，可能会造成溢出。

二分查找也可以用迭代的方式实现，根据中点上数字比较的结果，我们不断更改待所搜范围的边界。

\begin{algorithmic}[1]
\Function{Binary-Search}{$x, A, l, u$}
  \While{$l < u$}
    \State $m \gets l + \lfloor \frac{u - l}{2} \rfloor$
    \If{$A[m] = x$}
      \State \Return $m$
    \EndIf
    \If{$x < A[m]$}
      \State $u \gets m - 1$
    \Else
      \State $l \gets m + 1$
    \EndIf
  \EndWhile
  \Return NIL
\EndFunction
\end{algorithmic}

实现二分查找是一个很好的练习。我们把它留给读者，请尝试用各种方法来验证程序的正确性。

由于每次都将待查找数组缩短一半，二分查找的性能为$O(\lg n)$。

在纯函数式环境中，列表本质上是单向链表。随机访问指定位置的元素需要线性时间。二分查找无法发挥它的优势。下面的分析给出了性能会怎样下降。考虑下面的定义：

\[
bsearch(x, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  Err & L = \Phi \\
  b_1 & x = b_1, (A, B) = splitAt(\lfloor \frac{|L|}{2} \rfloor, L) \\
  bsearch(x, A) & B = \Phi \lor x < b_1 \\
  bsearch(x, B') & otherwise
  \end{array}
\right.
\]

其中$b_1$是列表$B$不为空时的第一个元素，$B'$包含除$b_1$外的剩余部分。函数$splitAt$需要$O(n)$时间将列表分成两个子列表$A$和$B$（参见附录A和归并排序一章）。若$B$不为空，且$x$等于$b_1$，则搜索结束；如果$x$小于$b_1$，由于列表已序，我们需要递归在$A$中搜索，否则，需要在$B$中搜索。如果列表为空，则表示搜索失败，待查找的元素不存在。

由于总是在中点位置分割列表，每次递归都将待搜索的元素减半。在每次递归中，都需要线性时间进行分割。分割函数只需要遍历单向链表的前半部分，因此总时间可以表示为：

\[
T(n) = c \frac{n}{2} + c \frac{n}{4} + c \frac{n}{8} + ...
\]

这一结果为$O(n)$，和从头至尾进行扫描的结果是一样的。

\[
search(x, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  Err & L = \Phi \\
  l_1 & x = l_1 \\
  search(x, L') & otherwise
  \end{array}
\right.
\]

在插入排序一章中，我们曾经指出，函数式的二分查找本质上是通过二叉搜索树实现的。将已序序列表示为一棵树（如有必要使用自平衡树），可以提供对数时间的搜索\footnote{有些读者认为，应该使用数组而不是单向链表，例如Haskell中提供了能在常数时间进行随机访问的数组。本书只讨论用finger树实现的纯函数式序列，和Haskell中的数组不同，它并不支持常数时间的随机访问。}

虽然无法对单向链表进行分而治之的二分查找，但二分查找在函数式环境中也有很多应用。考虑方程$a^x = y$，对于给定的自然数$a$和$y$，其中$a \leq y$。我们希望寻找$x$的整数解。显然可以用穷举搜索：从0开始依次尝试$a^0, a^1, a^2, ...$，知道发现某个$a^i = y$，或者发现$a^i < y < a^{i+1}$，这表示方程无整数解。我们定义解$x$的范围为$X = \{0, 1, 2, ...\}$，并且定义下面的穷举搜索函数$solve(a, y, X)$。

\[
solve(a, y, X) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  x_1 & a^{x_1} = y \\
  solve(a, y, X') & a^{x_1} < y \\
  Err & otherwise
  \end{array}
\right.
\]

这一函数按照单调增的顺序检查解的可能范围。它首先从$X$选择一个候选元素$x_1$，比较$a^{x_1}$和$y$，如果相等，则$x_1$就是方程的解；如果小于$y$，则丢弃$x_1$，继续在剩余的元素$X'$中查找；否则，由于函数$f(x)=a^x$在$a$为自然数时，是非减函数。剩余元素会令$f(x)$变得更大。因此方程不存在整数解。这种情况下我们返回错误。

对于很大的$a$和$x$，如果需要保持精度，则计算$a^x$会消耗一定的时间\footnote{当然，我们可以复用$a^n$的结果来计算$a^{n+1} = a a^n$。这里我们考虑一般意义下的单调函数$f(n)$。}。有没有什么办法可以减小计算量呢？我们可以使用分而治之的二分查找来进行改进。我们可以估计出解的范围的上限。由于$a^y \leq y$，我们可以在区间$\{0, 1, ..., y\}$内搜索。由于函数$f(x) = a^x$是非减函数，对于自变量$x$，我们可以先检查区间的中点$x_m = \lfloor \frac{0 + y}{2} \rfloor$，如果$a^{x_m} = y$，则$x_m$就是方程的解；如果值小于$y$，我们可以丢弃$x_m$前的全部元素；否则，我们丢弃$x_m$后的全部元素；两种情况下都将搜索范围减半。我们重复这一过程直到找到解或者查找范围变成空，这表示方程不存在整数解。

二分查找的方法可以形式化为下面的定义。我们将非减函数抽象为一个参数。为了解决上面的方程，我们只需要调用$bsearch(f, y, 0, y)$，其中$f(x) = a^x$。

\be
bsearch(f, y, l, u) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  Err & u < l \\
  m & f(m) = y, m = \lfloor \frac{l + u}{2} \rfloor \\
  bsearch(f, y, l, m-1) & f(m) > y \\
  bsearch(f, y, m+1, u) & f(m) < y
  \end{array}
\right.
\label{eq:bsearch}
\ee

由于我们每次递归都将搜索范围减半，这一方法只计算了$O(\log y)$次$f(x)$。要远好于穷举法。

\subsubsection{二维搜索}

一个很自然的想法是把二分搜索的思想扩展到二维括者更高维的搜索域。但事实上这种扩展却并不简单。

作为一个例子，考虑一个$m \times n$矩阵$M$。每行、每列的元素都是严格递增的。图\ref{fig:matrix-eg}给出了一个这样的矩阵。

\begin{figure}[htbp]
 \centering
\[
\left [
  \begin{array}{ccccc}
    1 & 2 & 3 & 4 & ... \\
    2 & 4 & 5 & 6 & ... \\
    3 & 5 & 7 & 8 & ... \\
    4 & 6 & 8 & 9 & ...
    ... \\
  \end{array}
\right ]
\]
\caption{每行、每列都严格单调增的矩阵。}
\label{fig:matrix-eg}
\end{figure}

任给一个$x$，如果快速地在矩阵中定位到所有等于$x$的元素呢？我们需要给出一个算法，返回一组位置$(i, j)$的列表，使得所有的$M_{i,j} = x$。

Richard Bird说他曾经用这一问题作为牛津大学的入学面试题\cite{fp-pearls}。耐人寻味的是，那些在中学就接触过计算机科学的候选人，往往会尝试使用二分查找来解决这个问题，但却很容易陷入困境。

按照二分查找的思路，通常会先检查位于$M_{\frac{m}{2}, \frac{n}{2}}$上的元素。如果它小于$x$，我们只能丢弃左上区域的元素；如果它大于$x$，只能丢弃右下区域的元素。图\ref{fig:bsearch-2D}描述了这两种情况，灰色的区域表示可以丢弃的元素。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/bsearch-2D.eps}
 \caption{左：中点的元素小于$x$。所有灰色区域的元素都小于$x$；右：中点的元素大于$x$。所有灰色区域的元素都大于$x$。}
 \label{fig:bsearch-2D}
\end{figure}

这里出现的问题是，两种情况下，搜索区域都从一个矩形变成了一个L形，我们无法继续递归地进行搜索。为了系统化地解决这一问题，我们先给出一个通用的定义，然后从穷举法开始，逐步改进，直到获得满意的答案。

考虑严格单调增函数$f(x, y)$，例如$f(x, y) = a^x + b^y$，其中$a$和$b$都是自然数。给定自然数$z$，我们希望寻找全部的非负整数解$(x, y)$。

使用这一定义，上述的矩阵搜索问题，可以特殊化为下面的函数：

\[
f(x, y) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  M_{x, y} & 1 \leq x \leq m, 1 \leq y \leq n \\
  -1 & otherwise
  \end{array}
\right.
\]

\paragraph{穷举法二维搜索}

既然要找出$f(x, y)$的所有解，最简单的方法就是双重循环的穷举法：

\begin{algorithmic}[1]
\Function{Solve}{$f, z$}
  \State $A \gets \phi$
  \For{$x \in \{0, 1, 2, ..., z\}$}
    \For{$y \in \{0, 1, 2, ..., z\}$}
      \If{$f(x, y) = z$}
        \State $A \gets A \cup \{(x, y)\}$
      \EndIf
    \EndFor
  \EndFor
  \State \Return $A$
\EndFunction
\end{algorithmic}

显然，这一方法计算了$(z+1)^2$次$f$。它可以形式化为式(\ref{eq:bsearch-brute})的定义：

\be
solve(f, z) = \{ (x, y) | x \in \{0, 1, ..., z\}, y \in \{0, 1, ..., z\}, f(x, y) = z\}
\label{eq:bsearch-brute}
\ee

\paragraph{Saddleback搜索}
\index{Saddelback搜索}

我们尚未使用$f(x, y)$为严格单调增的条件。Dijkstra指出\cite{saddle-back}，有效的解法不是从左下角出发，而是从左上角出发开始查找。如图\ref{fig:saddleback-1}所示，搜索从$(0, z)$开始，对于每个点$(p, q)$，我们比较$f(p q)$和$z$的关系：

\begin{itemize}
\item 如果$f(p, q) < z$，由于$f$单调增，对于所有的$0 \leq y < q$，必然有$f(p, y) < z$。我们可以丢弃垂直线段上的所有点（红色线段）；
\item 如果$f(p, q) > z$，则对于所有的$p < x \leq z$，必然有$f(x, q) > z$。我们可以丢弃水平线段上的所有点（蓝色线段）；
\item 否则，若$f(p, q) = z$，则$(p, q)$是一个解，两条线段上的点都可以丢弃。
\end{itemize}

这样，我们就可以逐步缩小矩形的搜索区域。每次要么丢弃一行，要么丢弃一列，或者同时丢弃行和列。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/saddleback-1.eps}
 \caption{从左上角搜索。}
 \label{fig:saddleback-1}
\end{figure}

这一方法可以定义为一个函数$search(f, z, p, q)$，它在矩形区域内搜索方程$f(x, y) = z$的整数解，矩形的左上角为$(p, q)$，右下角为$(z, 0)$。这个矩形的左上角一开始时为$(p, q) = (0, z)$，然后启动搜索$solve(f, z) = search(f, z, 0, z)$。

\be
search(f, z, p, q) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & p > z \lor q < 0 \\
  search(f, z, p + 1, q) & f(p, q) < z \\
  search(f, z, p, q - 1) & f(p, q) > z \\
  \{(p, q)\} \cup search(f, z, p + 1, q - 1) & otherwise
  \end{array}
\right.
\ee

第一行为边界条件，如果$(p q)$不再$(z, 0)$的左上方，则无解。下面的Haskell例子程序实现了这一算法：

\lstset{language=Haskell}
\begin{lstlisting}
solve f z = search 0 z where
  search p q | p > z || q < 0 = []
             | z' < z = search (p + 1) q
             | z' > z = search p (q - 1)
             | otherwise = (p, q) : search (p + 1) (q - 1)
    where z' = f p q
\end{lstlisting}

考虑到计算$f$的过程消耗可能较大，这一程序将计算结果$f(p, q)$存储在变量$z'$中。算法也可以用imperative的方式实现，在循环中不断更新搜索区域的边界。

\begin{algorithmic}[1]
\Function{Solve}{$f, z$}
  \State $p \gets 0, q \gets z$
  \State $S \gets \Phi$
  \While{$p \leq z \land q \geq 0$}
    \State $z' \gets f(p, q)$
    \If{$z' < z$}
      \State $p \gets p + 1$
    \ElsIf{$z' > z$}
      \State $q \gets q - 1$
    \Else
      \State $S \gets S \cup \{(p, q)\}$
      \State $p \gets p + 1, q \gets q - 1$
    \EndIf
  \EndWhile
  \State \Return $S$
\EndFunction
\end{algorithmic}

下面的Python例子程序实现了这一算法。

\lstset{language=Python}
\begin{lstlisting}
def solve(f, z):
    (p, q) = (0, z)
    res = []
    while p <= z and q >= 0:
        z1 = f(p, q)
        if z1 < z:
            p = p + 1
        elif z1 > z:
            q = q - 1
        else:
            res.append((p, q))
            (p, q) = (p + 1, q - 1)
    return res

\end{lstlisting}

显然在每次迭代中，$p$和$q$中至少有一个会向右下角前进一步。因此最多需要$2(z+1)$次迭代以完成搜索。这是最差情况下的结果。最好的情况又分为三种，第一种是每次迭代$p$和$q$同时前进一步，因此只需要$z+1$步就可以完成搜索；第二种是不断沿着水平方向向右前进，最后$p$超过$z$；第三种与此类似，不断沿着垂直方向向下前进，最终$q$变为负。

图\ref{fig:saddleback-1-cases}描述了最好和最坏的情况。图\ref{fig:saddleback-1-cases} (a)中，对角线上的每个点$(x, z-x)$都满足$f(x, z-x) = z$，总共需要$z+1$步到达$(z, 0)$；(b)中，最上不水平线上的每个点$(x, z)$都使得$f(x, z) < z$，$z+1$步后，搜索结束；(c)中，左侧垂直线上的每个点$(0, x)$都使得$f(0, x) > z$，因此$z+1$步后，搜索结束；(d)描述的是最差情况。如果我们将搜索路径上的所有水平线段投射$x$轴上，所有垂直线段投射到$y$轴上，就可以得到总共的搜索步数为$2(z+1)$。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/saddleback-1-cases.eps}
 \caption{最好和最差情况。}
 \label{fig:saddleback-1-cases}
\end{figure}

和复杂度为$O(z^2)$的穷举法相比，这一改进将复杂度提高到线性时间$O(z)$。

Bird猜测，这一算法的名称saddleback的由来是因为函数$f$的3维图像中，左下部的最小值和右上部的最大值，以及两侧的翼形图像，合起来像一个马鞍。如图\ref{fig:saddleback-frame}所示。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/saddleback-xx-yy.eps}
 \caption{函数$f(x, y) = x^2 + y^2$的图像。}
 \label{fig:saddleback-frame}
\end{figure}

\paragraph{改进的saddleback搜索}

问题扩展到2维后，我们尚未使用二分查找来改进算法。基本的saddleback搜索从左上角$(0, z)$开始，向右下角$(z, 0)$进行搜索。这一范围实可以进一步缩小。

因为$f$单调增，我们可以沿着$y$轴找到最大的$m$，使得$0 \leq m \leq z$且$f(0, m) \leq z$；同样，我们可以沿着$x$轴找到最大的$n$，使得$0 \leq n \leq z$且$f(n, 0) \leq z$；这样搜索区域就从 原来的$(0, z) - (z, 0)$缩小到$(0, m) - (n, 0)$，如图\ref{fig:saddleback-2}所示。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/saddleback-2.eps}
 \caption{缩小的灰色搜索区域。}
 \label{fig:saddleback-2}
\end{figure}

显然$m$和$n$可用穷举法找到：

\be
\begin{array}{rl}
m & = max(\{y | 0 \leq y \leq z, f(0, y) \leq z\}) \\
n & = max(\{x | 0 \leq x \leq z, f(x, 0) \leq z\})
\end{array}
\ee

当搜索$m$时，函数$f$的变量$x$固定为0。这就转化为了一维的单调增函数搜索问题（在函数式环境中，称为Curried化函数$f(0, y)$）。可以使用二分查找来改进这一搜索。但是我们需要对式(\ref{eq:bsearch})略加改动。给定$y$，我们不是要寻找$l \leq x \leq u$，使得$f(x) = y$。而是要寻找$l \leq x \leq u$使得满足不等式$f(x) \leq y < f(x+1)$。

\be
bsearch(f, y, l, u) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l & u \leq l \\
  m & f(m) \leq y < f(m+1), m = \lfloor \frac{l + u}{2} \rfloor \\
  bsearch(f, y, m + 1, u) & f(m) \leq y \\
  bsearch(f, y, l, m-1) & otherwise
  \end{array}
\right.
\label{eq:bsearch-general}
\ee

第一行处理搜索区域为空的边界情况，此时我们返回搜索区域的下界；如果中点对应的函数值小于等于$y$，而下一个值对应的函数值大于$y$，则中点就是我们要搜索的结果；否则，如果中点下一个值对应的函数值也不大于$y$，就将中点的下一个值作为新的下届，递归地进行二分查找；最后，如果中点对应的函数值大于$y$，则用中点前的一个值作为新的上界，递归进行查找。下面的Haskell例子程序实现了这样的二分查找。

\lstset{language=Haskell}
\begin{lstlisting}
bsearch f y (l, u) | u <= l = l
                   | f m <= y = if f (m + 1) <= y
                                then bsearch f y (m + 1, u) else m
                   | otherwise = bsearch f y (l, m-1)
  where m = (l + u) `div` 2
\end{lstlisting}

这样，$m$和$n$可以使用二分查找来确定：

\be
\begin{array}{rl}
m & = bsearch(\lambda_y \cdot f(0, y), z, 0, z) \\
n & = bsearch(\lambda_x \cdot f(x, 0), z, 0, z)
\end{array}
\label{eq:bsearch-boundaries}
\ee

我们可以将saddleback搜索的区域缩小为更精确的矩形$solve(f, z) = search(f, z, 0, m)$：

\be
search(f, z, p, q) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & p > n \lor q < 0 \\
  search(f, z, p + 1, q) & f(p, q) < z \\
  search(f, z, p, q - 1) & f(p, q) > z \\
  \{(p, q)\} \cup search(f, z, p + 1, q - 1) & otherwise
  \end{array}
\right.
\ee

大部分和基本的saddleback一样，但是当$p$超过$n$的时候，就可以停止，而无需达到$z$。在实际的实现中，可以将$f(p, q)$的值保存下来，而不用每次计算。如下面的Haskell例子代码所示：

\lstset{language=Haskell}
\begin{lstlisting}
solve' f z = search 0 m where
  search p q | p > n || q < 0 = []
             | z' < z = search (p + 1) q
             | z' > z = search p (q - 1)
             | otherwise = (p, q) : search (p + 1) (q - 1)
    where z' = f p q
  m = bsearch (f 0) z (0, z)
  n = bsearch (\x->f x 0) z (0, z)
\end{lstlisting}

这一改进的saddleback搜索，首先使用两轮二分查找得到$m$和$n$。每轮二分查找都计算了$O(\lg z)$次$f$；此后，算法在最坏情况下计算$O(m+n)$次；而在最好的情况下计算$O(min(m, n))$次。总体的性能如下表所示：

\begin{tabular}{|l|l|}
\hline
 & 计算$f$的次数 \\
\hline
最坏情况 & $2 \log z + m + n$ \\
最好情况 & $2 \log z + min(m, n)$ \\
\hline
\end{tabular}

某些函数，例如$f(x, y) = a^x + b^y$，对于正整数$a$和$b$，$m$和$n$相对很小，因此整体性能接近$O(\lg z)$。

这一算法也可以用命令式的方法实现。首先需要修改命令式的二分查找算法：

\begin{algorithmic}[1]
\Function{Binary-Search}{$f, y, (l, u)$}
  \While{$l < u$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$
    \If{$f(m) \leq y$}
      \If{$y < f(m+1)$}
        \State \Return m
      \EndIf
      \State $l \gets m + 1$
    \Else
      \State $u \gets m$
    \EndIf
  \EndWhile
  \State \Return $l$
\EndFunction
\end{algorithmic}

使用上述二分查找，在开始saddleback搜索前，先确定$m$和$n$。

\begin{algorithmic}[1]
\Function{Solve}{$f, z$}
  \State $m \gets$ \Call{Binary-Search}{$\lambda_y \cdot f(0, y), z, (0, z)$}
  \State $n \gets$ \Call{Binary-Search}{$\lambda_x \cdot f(x, 0), z, (0, z)$}
  \State $p \gets 0, q \gets m$
  \State $S \gets \Phi$
  \While{$p \leq n \land q \geq 0$}
    \State $z' \gets f(p, q)$
    \If{$z' < z$}
      \State $p \gets p + 1$
    \ElsIf{$z' > z$}
      \State $q \gets q - 1$
    \Else
      \State $S \gets S \cup \{(p, q)\}$
      \State $p \gets p + 1, q \gets q - 1$
    \EndIf
  \EndWhile
  \State \Return $S$
\EndFunction
\end{algorithmic}

具体的实现留给读者作为练习。

\paragraph{Saddleback搜索的进一步改进}

图\ref{fig:bsearch-2D}展示的两种情况中，矩阵中点的值要么比目标值小，要么比目标值大。都只能丢弃$\frac{1}{4}$区域中的元素，而剩余的搜索区域变为一个L形。

事实上，我们忽略了另外的一个重要情况。我们观察矩形搜索区域中的任一一点，如图\ref{fig:saddleback-drop}所示。

\begin{figure}[htbp]
 \centering
 \subfloat[如果$f(p, q) \neq z$，只能丢弃左下或右上的区域（灰色部分）。两种情况下，剩余的搜索区域都变成了L形。]{\includegraphics[scale=0.5]{img/saddleback-3.eps}} \\
 \subfloat[如果$f(p, q) = z$，可以同时丢弃两个子区域，问题的搜索域减半。]{\includegraphics[scale=0.5]{img/saddleback-4.eps}}
 \caption{缩小搜索区域的效率。}
 \label{fig:saddleback-drop}
\end{figure}

考虑搜索一个矩形区域，左下角为$(a, b)$，右上角为$(c, d)$。如果$(p, q)$不是矩形的中点，并且$f(p, q) \neq z$，我们并不能保证被丢弃的部分总是1/4。但是，如果$f(p, q) = z$，由于$f$是单调增的，我们可以同时丢弃左下和右上的子区域，并且$p$列和$q$行上的所有其他点也都可以丢弃掉。这样每次只剩下1/2的区域，可以迅速缩小搜索的区间。

由此可知，我们无需找到矩形的中点进行搜索。更有效的方法是找到函数值等于目标值的点。我们可以沿着矩形中心的水平方向或者垂直方向使用二分查找来定位这样的点。

在线段上进行二分查找的性能和线段的长度成对数关系。我们可以选取水平和垂直方向中较短的中线进行搜索，如图\ref{fig:saddleback-centerline}所示。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/saddleback-centerline.eps}
 \caption{沿较短的中线进行二分查找。}
 \label{fig:saddleback-centerline}
\end{figure}

但是，如果中线行不存在满足$(p, q) = z$的点时如何处理呢？例如，水平中线上不存在这样的点。此时，我们仍然能够找到一点满足$f(p, q) < z < f(p+1, q)$。唯一不同之处是我们不能将$p$列和$q$行上的点完全丢弃。

综合上述情况，沿着水平线二分搜索以要找到一点$p$，其满足件$f(p, q) \leq z < f(p+1, q)$；而沿着垂直线二分搜索的条件是$f(p, q) \leq z < f(p, q + 1)$。

如果线段上所有的点都使得$f(p, q) < z$，则修改后的二分查找会返回上界作为结果；反之，如果所有点对应的函数值都大于$z$，则返回下届作为结果。此时，我们可以将中线一侧的整个区域全部丢弃。

总结这些结论，我们可以给出下面的改进saddleback搜索算法：

\begin{enumerate}
\item 沿着$y$轴和$x$轴进行二分搜索，定位出搜索区域的边界，从$(0, m)$到$(n, 0)$；
\item 记待搜索的矩形区域为$(a, b) - (c, d)$，若矩形为空，则无解；
\item 若矩形的高大于宽，则沿着水平中线进行二分查找；否则，沿着垂直中线进行二分查找；记查找的结果为点$(p, q)$；
\item 若$f(p, q) = z$，记录$(p, q)$为一个解，然后递归搜索两个子矩形区域$(a, b) - (p-1, q+1)$和$(p+1, q-1) - (c, d)$；
\item 否则，若$f(p, q) \neq z$，递归搜索同样的两个子矩形区域和一条线段。线段或者为$(p, q+1) - (p, b)$如图\ref{fig:include-line} (a)；或者为$(p+1, q) - (c, q)$如图\ref{fig:include-line} (b)。
\end{enumerate}

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/saddleback-include-ln.eps}
 \caption{递归搜索灰色的区域，如果$f(p, q) \neq z$，还需要搜索加粗的线段。}
 \label{fig:include-line}
\end{figure}

我们复用前面式(\ref{eq:bsearch-general})和(\ref{eq:bsearch-boundaries})的定义。定义$Search_{(a, b), (c, d)}$为新的搜索函数，它搜索一个矩形区域，其中左上角为$(a, b)$，右下角为$(c, d)$。

\be
search_{(a, b), (c, d)} =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & c < a \lor d < b \\
  csearch & c - a < b - d \\
  rsearch & otherwise
  \end{array}
\right.
\ee

函数$csearch$在水平中线上进行二分查找，寻找一点$(p, q)$使得$f(p, q) \leq z < f(p+1, q)$。如图\ref{fig:include-line} (a)所示。如果中线上所有点对应的函数值都大于$z$，二分查找返回下界作为结果，即$(p, q) = (a, \lfloor \frac{b + d}{2} \rfloor)$。中线和它上侧的区域全部可以丢弃，如图\ref{fig:saddleback-edge-cases} (a).所示。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/saddleback-edge-cases.eps}
 \caption{沿中线进行二分查找时的特殊情况。}
 \label{fig:saddleback-edge-cases}
\end{figure}

\be
csearch = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  search_{(p, q-1), (c, d)} & z < f(p, q) \\
  search_{(a, b), (p-1, q+1)} \cup \{(p, q)\} \cup search_{(p+1, q-1), (c, d)} & f(p, q) = z \\
  search_{(a, b), (p, q+1)} \cup search_{(p+1, q-1), (c, d)} & otherwise
  \end{array}
\right.
\ee

其中

\[
\begin{array}{l}
q = \lfloor \frac{b + d}{2} \rfloor) \\
p = bsearch(\lambda_x \cdot f(x, q), z, (a, c))
\end{array}
\]

函数$rsearch$与此类似，它沿着垂直中线进行搜索。

\be
rsearch = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  search_{(a, b), (p - 1, q)} & z < f(p, q) \\
  search_{(a, b), (p-1, q+1)} \cup \{(p, q)\} \cup search_{(p+1, q-1), (c, d)} & f(p, q) = z \\
  search_{(a, b), (p-1, q+1)} \cup search_{(p+1, q), (c, d)} & otherwise
  \end{array}
\right.
\ee

其中

\[
\begin{array}{l}
p = \lfloor \frac{a + c}{2} \rfloor) \\
q = bsearch(\lambda_y \cdot f(p, y), z, (d, b))
\end{array}
\]

下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
search f z (a, b) (c, d) | c < a || b < d = []
                         | c - a < b - d = let q = (b + d) `div` 2 in
                                             csearch (bsearch (\x -> f x q) z (a, c), q)
                         | otherwise = let p = (a + c) `div` 2 in
                                             rsearch (p, bsearch (f p) z (d, b))
  where
    csearch (p, q) | z < f p q = search f z (p, q - 1) (c, d)
                   | f p q == z = search f z (a, b) (p - 1, q + 1) ++
                                    (p, q) : search f z (p + 1, q - 1) (c, d)
                   | otherwise = search f z (a, b) (p, q + 1) ++
                                   search f z (p + 1, q - 1) (c, d)
    rsearch (p, q) | z < f p q = search f z (a, b) (p - 1, q)
                   | f p q == z = search f z (a, b) (p - 1, q + 1) ++
                                    (p, q) : search f z (p + 1, q - 1) (c, d)
                   | otherwise = search f z (a, b) (p - 1, q + 1) ++
                                    search f z (p + 1, q) (c, d)
\end{lstlisting}

主程序首先沿着$X$轴和$Y$轴进行二分查找，然后调用上述函数。

\lstset{language=Haskell}
\begin{lstlisting}
solve f z = search f z (0, m) (n, 0) where
  m = bsearch (f 0) z (0, z)
  n = bsearch (\x -> f x 0) z (0, z)
\end{lstlisting}

由于每次都丢弃一半的区域，算法总共搜索$O(\log (mn))$轮搜索。但是，为了寻找点$(p, q)$使得问题规模减半，我们需要沿着中线进行二分查找。这样需要计算$f$的次数为$O(\log min(m, n))$。令在大小为$m \times n$的矩形区域搜索的时间为$T(m, n)$，我们有如下的递归关系：

\be
T(m, n) = \log(min(m, n)) + 2 T(\frac{m}{2}, \frac{n}{2})
\ee

不妨设$m > n$，使用telescope方法，若$m = 2^i$、$n = 2^j$，我们有：

\be
\begin{array}{rl}
T(2^i, 2^j) & = j + 2 T(2^{i-1}, 2^{j-1}) \\
            & = \displaystyle \sum_{k=0}^{i-1} 2^k(j - k) \\
            & = O(2^i(j-i)) \\
            & = O(m \log (n/m))
\end{array}
\ee

Richard Bird证明了，这是在$m \times n$的矩形区域内搜索一给定值的最优下界\cite{fp-pearls}。

命令式的实现与此类似，我们在此将其略过。

\begin{Exercise}
\begin{itemize}
\item 参考前面章节快速排序的部分，证明分而治之的$k$选择算法，在平均情况下的性能为$O(n)$。
\item 使用两路划分和三点中值法实现命令式的$k$选择算法。
\item 实现能有效处理大量重复元素的命令式的$k$选择算法。
\item 选择一门编程语言，实现median-of-median的$k$选择算法。
\item 本节给出的$tops(k, L)$使用了列表的连接操作，如$A \cup \{ l_1 \} \cup tops(k - |A| - 1, B)$。这一操作的性能为线性时间，和被连接列表的长度成比例。修改算法，仅用一遍处理就将子列表连接起来。
\item 作者想出了另外一种分而治之的$k$选择问题解法。首先找到前$k$个元素中的最大值，和剩余元素中的最小值，分别记为$x$和$y$，若$x$小于$y$，说明所有的前$k$个元素都小于剩余的元素，它们恰巧是最小的$k$个元素；否则，说明前$k$个元素中的某些元素，需要被交换到后面去。
\begin{algorithmic}[1]
\Procedure{Tops}{$k, A$}
  \State $l \gets 1$
  \State $u \gets |A|$
  \Loop
    \State $i \gets$ \Call{Max-At}{$A[l..k]$}
    \State $j \gets$ \Call{Min-At}{$A[k+1..u]$}
    \If{$A[i] < A[j]$}
      \State break
    \EndIf
    \State \textproc{Exchange} $A[l] \leftrightarrow A[j]$
    \State \textproc{Exchange} $A[k+1] \leftrightarrow A[i]$
    \State $l \gets$ \Call{Partition}{$A, l, k$}
    \State $u \gets$ \Call{Partition}{$A, k+1, u$}
  \EndLoop
\EndProcedure
\end{algorithmic}
请说明这一算法正确与否？性能如何？
\item 使用迭代的方式和递归的方式分别实现二分查找算法，并使用自动的方式进行测试。可以使用生成的随机测试数据，也可以定义一些不变性质，并和编程环境中内置的二分查找工具对比。
\item 任意给定两个已序数组$A$和$B$，寻找它们的中值（median）。要求时间复杂度为$O(\lg (|A| + |B|))$。
\item 使用一门命令式语言，在进行saddleback搜索前，先通过二分查找定位出更精确的搜索区域。
\item 使用一门命令式语言，沿着较短的中线进行二分查找，从而实现改进的二维搜索。
\item 有人给出了这样的二维搜索算法：当搜索一个矩形区域时，由于左下角是最小值，右上角是最大值。若待搜索的值小于最小值或者大于最大值，则无解；否则，从中点将矩形区域分割成4个小矩形，然后进行递归搜索。
\begin{algorithmic}[1]
\Procedure{Search}{$f, z, a, b, c, d$} \Comment{$(a, b)$：左下角 $(c, d)$：右上角}
  \If {$z \leq f(a, b) \lor f(c, d) \geq z$}
    \If{$z = f(a, b)$}
      \State record $(a, b)$ as a solution
    \EndIf
    \If{$z = f(c, d)$}
      \State record $(c, d)$ as a solution
    \EndIf
    \State \Return
  \EndIf
  \State $p \gets \lfloor \frac{a + c}{2} \rfloor$
  \State $q \gets \lfloor \frac{b + d}{2} \rfloor$
  \State \Call{Search}{$f, z, a, q, p, d$}
  \State \Call{Search}{$f, z, p, q, c, d$}
  \State \Call{Search}{$f, z, a, b, p, q$}
  \State \Call{Search}{$f, z, p, b, c, q$}
\EndProcedure
\end{algorithmic}
试分析这一算法的性能。
\end{itemize}
\end{Exercise}

\subsection{信息复用}

人会通过搜索来学习。我们不仅记忆搜索失败的教训，还学习总结成功的模式。这是某种意义上的信息复用，不论这些信息是正面的还是负面的。但难点在于决定记忆哪些信息。记忆太少的信息不足以提高搜索的效率，记忆太多的信息又无法满足存储空间的限制。

本节我们首先介绍两个有趣的问题：Boyer-Moore众数（majority number）问题，和子数组最大和问题。它们都通过复用最少的信息来解决问题。然后，我们介绍两种被广泛使用的字符串匹配算法：KMP（Knuth-Morris-Pratt）算法，和Boyer-Moore算法。

\subsubsection{Boyer-Moore众数问题}
\index{Boyer-Moor众数问题}

人们常常通过投票来进行一些决策，例如选举领袖，接受或者拒绝一项建议。在作者写作本章的时候，有三个国家正在通过投票选举总统，他们都使用计算机来统计投票结果。

假设某个小岛上的国家要通过投票选出新的总统。这个国家的宪法规定，只有赢得半数以上选票的人才可以成为总统。从一个投票结果的序列，例如A, B, A, C, B, B, D, ...我们能否找到一种高效的方法，得知谁当选了总统，或者没有任何人赢得半数以上的选票？

显然可以通过使用一个map，然后遍历一遍选票来解决这个问题。如我们在二叉搜索树一章给出例子那样\footnote{2004年，人们发现了一种概率算法，称为Count-min sketch算法，使用sub-linear空间进行计数\cite{count-min-sketch}。}

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T majority(const T* xs, int n, T fail) {
    map<T, int> m;
    int i, max = 0;
    T r;
    for (i = 0; i < n; ++i)
        ++m[xs[i]];
    for (typename map<T, int>::iterator it = m.begin(); it != m.end(); ++it)
        if (it->second > max) {
            max = it->second;
            r = it->first;
        }
    return max * 2 > n ? r : fail;
}
\end{lstlisting}

这段例子程序首先扫描所有选票，然后通过map累计所有候选人的票数。接着，他遍历map找到得票最多的候选人。若票数超过半数，则此人获胜，否则程序返回一个特殊值表示无人获胜。

下面的伪代码描述了这一算法。

\begin{algorithmic}[1]
\Function{Majority}{$A$}
  \State $M \gets $ empty map
  \For{$\forall a \in A$}
    \State \textproc{Put}($M$, $a, 1 + $ \Call{Get}{$M, a$})
  \EndFor
  \State $max \gets 0$, $m \gets NIL$
  \For{$\forall (k, v) \in M$}
    \If{$max < v$}
      \State $max \gets v$, $m \gets k$
    \EndIf
  \EndFor
  \If{$max > |A| 50\%$}
    \State \Return $m$
  \Else
    \State fail
  \EndIf
\EndFunction
\end{algorithmic}

对于$m$名候选人和$n$张选票，若使用自平衡树实现的map（如红黑树map），这一程序首先需要$O(n \log m)$时间来构建map；若使用Hash表实现的map，则所用时间为$O(n)$。但是Hash表所用的空间会更多。接下来，程序需要$O(m)$的时间来遍历map，然后寻找票数最多的候选人。下表给出了使用不同种类map所需的时间和空间的对比。

\begin{tabular}{|l|l|l|}
\hline
map & 时间 & 空间 \\
\hline
自平衡树 & $O(n \log m)$ & $O(m)$ \\
hashing & $O(n)$ & 最少$O(m)$ \\
\hline
\end{tabular}

Boyer和Moore在1980年发现了一种巧妙的方法，如果存在超过半数的元素，可以只扫描一遍就找到它。并且这一方法只需要$O(1)$的空间\cite{boyer-moore-majority}。

首先我们记录第一张选票投给的候选人为目前的获胜者，所赢得票数为1。在接下来的扫描中，若下一张选票还投给目前的获胜者，就将获胜者的票数加1；否则，下一张选票没有投给目前的获胜者，我们将获胜者的赢得的票数减1。若获胜者的净赢得的票数变为0，说明他不再是获胜者了，我们选择下一张选票上的候选人作为新的获胜者，并继续重复这一扫描过程。

假设选票的序列为：A, B, C, B, B, C, A, B, A, B, B, D, B。下表给出了这一扫描处理的各个步骤。

\begin{tabular}{|l|l|l|}
\hline
获胜者 & 净赢票数 & 扫描位置 \\
\hline
A & 1 & \underline{A}, B, C, B, B, C, A, B, A, B, B, D, B \\
A & 0 & A, \underline{B}, C, B, B, C, A, B, A, B, B, D, B \\
C & 1 & A, B, \underline{C}, B, B, C, A, B, A, B, B, D, B \\
C & 0 & A, B, C, \underline{B}, B, C, A, B, A, B, B, D, B \\
B & 1 & A, B, C, B, \underline{B}, C, A, B, A, B, B, D, B \\
B & 0 & A, B, C, B, B, \underline{C}, A, B, A, B, B, D, B \\
A & 1 & A, B, C, B, B, C, \underline{A}, B, A, B, B, D, B \\
A & 0 & A, B, C, B, B, C, A, \underline{B}, A, B, B, D, B \\
A & 1 & A, B, C, B, B, C, A, B, \underline{A}, B, B, D, B \\
A & 0 & A, B, C, B, B, C, A, B, A, \underline{B}, B, D, B \\
B & 1 & A, B, C, B, B, C, A, B, A, B, \underline{B}, D, B \\
B & 0 & A, B, C, B, B, C, A, B, A, B, B, \underline{D}, B \\
B & 1 & A, B, C, B, B, C, A, B, A, B, B, D, \underline{B} \\
\hline
\end{tabular}

这里关键的一点是：若存在一个超过50\%的众数，则它不可能被其它元素超越落选。但是，如果没有任何候选者赢得半数以上的选票，则最后所记录的“获胜者”并无意义。此时需要再进行一轮扫描进行验证。

下面的算法实现了这一思路。

\begin{algorithmic}[1]
\Function{Majority}{$A$}
  \State $c \gets 0$
  \For{$i \gets$ 1 to $|A|$}
    \If{$c = 0$}
      \State $x \gets A[i]$
    \EndIf
    \If{$A[i] = x$}
      \State $c \gets c + 1$
    \Else
      \State $c \gets c - 1$
    \EndIf
  \EndFor
  \State \Return $x$
\EndFunction
\end{algorithmic}

若存在众数，这一算法首先扫描所有的选票。每扫描一张票，它根据此选票是支持还是反对当前的结果来增减获胜者的净赢票数。若净赢票数变为0，表明当前的获胜者已落选，算法记录下一张选票投给的候选人为新的获胜者，并继续扫描。

这一过程是线性时间$O(n)$的，所用空间仅仅是两个变量。一个用以记录当前的获胜者，另一个记录净赢得的票数。

当众数存在是，虽然上述算法可以将它找出。但当不存在众数时，这一算法仍会输出一个不正确的结果。下面的改进通过增加一轮扫描来进行验证。

\begin{algorithmic}[1]
\Function{Majority}{$A$}
  \State $c \gets 0$
  \For{$i \gets$ 1 to $|A|$}
    \If{$c = 0$}
      \State $x \gets A[i]$
    \EndIf
    \If{$A[i] = x$}
      \State $c \gets c + 1$
    \Else
      \State $c \gets c - 1$
    \EndIf
  \EndFor
  \State $c \gets 0$
  \For{$i \gets 1$ to $|A|$}
    \If{$A[i] = x$}
      \State $c \gets c + 1$
    \EndIf
  \EndFor
  \If{$c > \%50|A|$}
    \State \Return $x$
  \Else
    \State fail
  \EndIf
\EndFunction
\end{algorithmic}

即使增加了验证的过程，这一算法的时间复杂度仍按为$O(n)$，并且所用空间为常数。下面的C++例子程序实现了这一算法\footnote{这是一个更加类似C语言例子，我们只是使用了C++的模板来抽象元素的类型。}。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T majority(const T* xs, int n, T fail) {
    T m;
    int i, c;
    for (i = 0, c = 0; i < n; ++i) {
        if (!c)
            m = xs[i];
        c += xs[i] == m ? 1 : -1;
    }
    for (i = 0, c = 0; i < n; ++i, c += xs[i] == m);
    return c * 2 > n ? m : fail;
}
\end{lstlisting}

Boyer-Moore众数算法也可以用纯函数的方式实现。我们不再使用变量来记录和更新信息，而是使用累积（accumulator）的方法。定义核心算法的函数为$maj(c, n, L)$，它接受一个选票列表$L$，目前的获胜者$c$，和净赢得的票数$n$。若选票列表不为空，则$c$在开始的时候为第一张选票的结果$l_1$，净赢得的票数为1，即$maj(l_1, 1, L')$，其中$L'$是除$l_1$以外的剩余选票。下面是这个函数的详细定义：

\be
maj(c, n, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  c & L = \Phi \\
  maj(c, n + 1, L') & l_1 = c \\
  maj(l_1, 1, L') & n = 0 \land l_1 \neq c \\
  maj(c, n - 1, L') & otherwise
  \end{array}
\right.
\ee

我们还需要定义一个函数来验证所得的结果是否超过半数。最终的算法首先检查选票列表，若为空，则不存在众数，否则它通过Boyer-Moore算法找到一个结果$c$，然后再扫描一遍选票列表计算$c$总共赢得的选票是否过半。

\be
majority(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  fail & L = \Phi \\
  c & c = maj(l_1, 1, L'), |\{x | x \in L, x = c\}| > \%50 |L| \\
  fail & otherwise
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
majority :: (Eq a) => [a] -> Maybe a
majority [] = Nothing
majority (x:xs) = let m = maj x 1 xs in verify m (x:xs)

maj c n [] = c
maj c n (x:xs) | c == x = maj c (n+1) xs
               | n == 0 = maj x 1 xs
               | otherwise = maj c (n-1) xs

verify m xs = if 2 * (length $ filter (==m) xs) > length xs
              then Just m else Nothing
\end{lstlisting} %$

\subsubsection{最大子序列和}
\index{最大和问题}

Jon Bentley给出过另一个类似的趣题\cite{programming-pearls}。给定一个序列，如何找出它的子序列和的最大值？例如，下表所示的序列中，子序列\{19, -12, 1, 9, 18\}的和最大，为35。

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
3 & -13 & 19 & -12 & 1 & 9 & 18 & -16 & 15 & -15 \\
\hline
\end{tabular}

这里，我们只要找出最大和的值。如果所有元素都是整数，显然答案就是全部元素的和。另外一个特殊情况是所有元素都是负数。我们定义空序列的最大和是0。

最简单的方法是穷举：计算出所有子序列的和，然后挑选最大的作为答案。这一方法的复杂度为平方级别。

\begin{algorithmic}[1]
\Function{Max-Sum}{$A$}
  \State $m \gets 0$
  \For{$i \gets 1$ to $|A|$}
    \State $s \gets 0$
    \For{$j \gets i$ to $|A|$}
      \State $s \gets s + A[j]$
      \State $m \gets $ \Call{Max}{$m, s$}
    \EndFor
  \EndFor
  \State \Return $m$
\EndFunction
\end{algorithmic}

穷举法没有复用任何此前已经计算出的结果。借鉴Boyer-Moore众数算法的思路，我们可以一边扫描，一边记录下以当前位置结尾的子序列的最大和。同时我们还需要记录下目前为止所找到的最大和，图\ref{fig:max-sum-invariant}给出了扫描时所保持的不变性质。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/max-sum-invariant.ps}
 \caption{Invariant during scan.}
 \label{fig:max-sum-invariant}
\end{figure}

在任何时候，当我们扫描到第$i$个位置时，目前找到的最大和记为$A$。同时，我们记录下以$i$结尾的子序列的最大和为$B$。$A$和$B$并不一定相等，实际上，我们总保持$B \leq A$的关系。当$B$和下一个元素相加，从而超过$A$时，我们就用这个更大的结果替换$A$。当$B$加上下一个元素后，变为负数时，我们将$B$重新设置为0。下表给出了扫描处理序列$\{3, -13, 19, -12, 1, 9, 18, -16, 15, -15\}$时的各个步骤。

\begin{tabular}{|l|l|r|}
\hline
最大和 & 以$i$结尾的字序列最大和 & 尚未扫描的部分 \\
\hline
0 & 0 & $\{3, -13, 19, -12, 1, 9, 18, -16, 15, -15\}$ \\
3 & 3 & $\{-13, 19, -12, 1, 9, 18, -16, 15, -15\}$ \\
3 & 0 & $\{19, -12, 1, 9, 18, -16, 15, -15\}$ \\
19 & 19 & $\{-12, 1, 9, 18, -16, 15, -15\}$ \\
19 & 7 & $\{1, 9, 18, -16, 15, -15\}$ \\
19 & 8 & $\{9, 18, -16, 15, -15\}$ \\
19 & 17 & $\{18, -16, 15, -15\}$ \\
35 & 35 & $\{-16, 15, -15\}$ \\
35 & 19 & $\{15, -15\}$ \\
35 & 34 & $\{-15\}$ \\
35 & 19 & $\{\}$\\
\hline
\end{tabular}

这一算法可以描述如下：

\begin{algorithmic}[1]
\Function{Max-Sum}{$V$}
  \State $A \gets 0, B \gets 0$
  \For{$i \gets 1$ to  $|V|$}
    \State $B \gets $ \Call{Max}{$B + V[i], 0$}
    \State $A \gets $ \Call{Max}{$A, B$}
  \EndFor
\EndFunction
\end{algorithmic}

也可以用函数式的方式实现这一算法。我们不再更新变量$A$和$B$，而是把它们作为尾递归的累积器。为了找到序列$L$的最大子序列和，我们调用函数$max_{sum}(0, 0, L)$。

\be
max_{sum}(A, B, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \Phi \\
  max_{sum}(A', B', L') & otherwise
  \end{array}
\right.
\ee

其中
\[
\begin{array}{l}
B' = max(l_1 + B, 0) \\
A' = max(A, B')
\end{array}
\]

下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
maxsum = msum 0 0 where
  msum a _ [] = a
  msum a b (x:xs) = let b' = max (x+b) 0
                        a' = max a b'
                    in msum a' b' xs
\end{lstlisting}

\subsubsection{KMP}
\index{KMP} \index{Knuth-Morris-Pratt算法}

字符串搜索是一类很重要的问题。所有的文本编辑器软件都带有字符串搜索功能。在Trie、Patricia和后缀树章节，我们介绍了一些字符串搜索常用的数据结构。本节中，我们介绍两种利用信息复用进行字符串搜索的算法。

有些编程环境提供了内置的字符串搜索工具，但是大多数是用暴力解法，包括ANSI C标准库中的\texttt{strstr}函数，C++标准模板库中的\texttt{find}，以及Java标准库JDK中的\texttt{indexOf}。图\ref{fig:strstr}描述了逐一比较字符的过程。

\begin{figure}[htbp]
 \centering
 \subfloat[Offset为$s = 4$，接着，连续有$q=4$个字符相同，但是第5个字符不同。]{\input{img/strstr.tex}} \\
 \subfloat[比较的起始位置移动到$s = 4 + 2 = 6$。]{\input{img/kmp.tex}}
 \caption{在文本“any ananthous ananym flower”中寻找“ananym”。}
 \label{fig:strstr}
\end{figure}

考虑我们在文本$T$中搜索字符串$P$，如图\ref{fig:strstr} (a)所示，在offset为$s=4$时，处理过程逐一检查$P$和$T$中的字符是否相等。前4个字符都是anan，但是第5个字符在$P$中是y，而在文本$T$中是t，它们不相等。

此时，逐一比较过程立即终止，我们将$s$加1，也就是把$P$向右移动1个位置，然后重新比较ananym和nantho...。实际上，$s$的增量可以超过1。这是因为，我们已经比较过前4个字符anan了，而第5个字符不等。而前两个字符an恰好是已成功比较过的anan的后缀。因此更有效的做法是将$s$增加2，也就是把$P$向右移动两个位置，如图\ref{fig:strstr} (b)所示。这样，我们就复用了前面已经比较的4个字符的信息，从而跳过大量无需比较的位置。

Knuth、Morris和Pratt根据这一思路给出了一个高效的字符串匹配算法\cite{kmp}，人们把三位作者的名字合在一起，称作KMP算法。

简洁起见，我们记文本$T$中的前$k$个字符组成的串为$T_k$，即$T_k$为文本$T$的$k$个字符前缀。

为了把$P$高效向右移动$s$个位置，我们需要定义一个关于$q$的函数，其中$q$是成功匹配的字符个数。例如在图\ref{fig:strstr} (a)中，$q$的值为4，即第5个字符不匹配。

什么情况下向右移动的距离$s$可以大于1呢？如图\ref{fig:kmp-fallback}所示，若可以将$P$向右移动，则一定存在某个$k$，使得$P$中的前$k$个字符和前缀$P_q$的最后$k$个字符相同。也就是说，前缀$P_k$同时是$P_q$的后缀。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/kmp-fallback.ps}
 \caption{$P_k$既是$P_q$的前缀，也是$P_q$的后缀。}
 \label{fig:kmp-fallback}
\end{figure}

当然也有可能不存在这样的前缀，同时也是后缀。如果我们认为空串同时是任何其他字符串的前缀和后缀，则总存在一个解$k=0$。如果存在多个$k$满足，为了避免漏掉任何可能的候选位置，我们需要找到同时既是前缀又是后缀的最大的$k$。我们定义一个\underline{前缀函数}$\pi(q)$，它告诉我们当第$q+1$个字符不匹配时应该回退的位置\cite{CLRS}。

\be
\pi(q) = max \{ k | k < q \land P_k \sqsupset P_q \}
\label{eq:prefix-function}
\ee

其中，$A \sqsupset B$表示“$A$是$B$的后缀”。这一函数的使用方法如下：当我们在文本$T$中，以offset为$s$尝试匹配$P$时，若前$q$个字符都相同，而接下来的字符不同，我们接下来通过$\pi(q)$找到一个回退的位置$q'$，然后重新尝试比较$P[q']$和文本中的字符。根据这一思路，KMP的核心算法可以描述如下：

\begin{algorithmic}[1]
\Function{KMP}{$T, P$}
  \State $n \gets |T|, m \gets |P|$
  \State build prefix function $\pi$ from $P$
  \State $q \gets 0$ \Comment{记录已经匹配的字符个数}
  \For{$i \gets 1$ to $n$}
    \While{$q > 0 \land P[q+1] \neq T[i]$}
      \State $q \gets \pi(q)$
    \EndWhile
    \If{$P[q+1] = T[i]$}
      \State $q \gets q + 1$
    \EndIf
    \If{$q = m$}
      \State found one solution at $i - m$
      \State $q \gets \pi(q)$ \Comment{继续寻找下一个可能的位置}
    \EndIf
  \EndFor
\EndFunction
\end{algorithmic}

虽然式(\ref{eq:prefix-function})给出了前缀函数$\pi(q)$的定义，但是简单寻找最长后缀的效率很低。实际上，我们可以进一步复用信息，来快速构造前缀函数。

最简单的情况是第一个字符就不相等。这种情况下，最长的前缀，同时也是后缀显然是空串，因此$\pi(1) = k = 0$。记最长的前缀为$P_k$。此时，$P_k = P_0$等于空串。

此后，当我们扫描到$P$中的第$q$个字符时，我们总有，前缀函数的所有值$\pi(i)$，$i$在$\{1, 2, ..., q-1 \}$都已经算出并记录下来了，并且目前最长的前缀$P_k$同时也是$P_{q-1}$的后缀。如图\ref{fig:kmp-prefix-func}所示，若$P[q] = P[k+1]$，则我们找到了一个更大的$k$，我们将$k$的最大值加一；否则，若两个字符不等，我们使用$\pi(k)$回退到一个较短的$P_{k'}$，其中$k' = \pi(k)$，然后比较这个新前缀的下一个字符是否和第$q$个字符相等。我们需要重复这一步骤，直到$k$变成0（表示只有空串满足条件），或者和第$q$个字符相等。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/kmp-prefix-func.ps}
 \caption{$P_k$是$P_{q-1}$的后缀，比较$P[q]$和$P[k+1]$。}
 \label{fig:kmp-prefix-func}
\end{figure}

KMP算法中，构建前缀函数的过程可以描述如下：

\begin{algorithmic}[1]
\Function{Build-Prefix-Function}{$P$}
  \State $m \gets |P|, k \gets 0$
  \State $\pi(1) \gets 0$
  \For{$q \gets 2$ to $m$}
    \While{$k > 0 \land P[q] \neq P[k+1]$}
      \State $k \gets \pi(k)$
    \EndWhile
    \If{$P[q] = P[k+1]$}
      \State $k \gets k + 1$
    \EndIf
    \State $\pi(q) \gets k$
  \EndFor
  \State \Return $\pi$
\EndFunction
\end{algorithmic}

下表列出了为字符串“ananym”构建前缀函数的步骤。表中的$k$实际上表示满足式(\ref{eq:prefix-function})的最大$k$。

\begin{tabular}{|c|r|c|l|}
\hline
$q$ & $P_q$ & $k$ & $P_k$ \\
\hline
1 & a & 0 & ``'' \\
2 & an & 0 & ``'' \\
3 & ana & 1 & a \\
4 & anan & 2 & an  \\
5 & anany & 0 & ``'' \\
6 & ananym & 0 & ``'' \\
\hline
\end{tabular}

下面的Python例子程序实现了完整的KMP算法。

\lstset{language=Python}
\begin{lstlisting}
def kmp_match(w, p):
    n = len(w)
    m = len(p)
    fallback = fprefix(p)
    k = 0 # how many elements have been matched so far.
    res = []
    for i in range(n):
        while k > 0 and p[k] != w[i]:
            k = fallback[k] #fall back
        if p[k] == w[i]:
            k = k + 1
        if k == m:
            res.append(i+1-m)
            k = fallback[k-1] # look for next
    return res

def fprefix(p):
    m = len(p)
    t = [0]*m  # fallback table
    k = 0
    for i in range(2, m):
        while k>0 and p[i-1] != p[k]:
            k = t[k-1] #fallback
        if p[i-1] == p[k]:
            k = k + 1
        t[i] = k
    return t
\end{lstlisting}

KMP算法相当于在搜索前对待搜索的字符串进行预处理。因此它可以最大程度地复用已知的匹配结果。

构建前缀函数的分摊性能为$O(m)$，可以使用potential分析法证明\cite{CLRS}。使用同样的方法可以进一步证明搜索算法本身的性能也是线性时间的。因此总体时间性能为$O(m + n)$，同时需要额外的$O(m)$空间来记录前缀函数的表格。

如果不仔细分析，可能会认为不同形式的待搜索字符串会影响KMP的性能。考虑在一个长度为$n$个a的文本“aaa...a”中，搜索长度为$m$个a的字符串“aaa...a”。因为所有的字符都相同，当最后一个字符匹配完成后，我们只能回退一个字符，并且此后不断重复回退1个字符。即使在这种极端情况下，KMP算法依旧是线性时间的（为什么？）。请尝试考虑更多情况，例如$P = aaaa...b$，$T = aaaa...a$，并分析KMP的性能。

\paragraph{纯函数式KMP算法}

用纯函数式的方法实现KMP算法会比较困难。命令式的KMP算法大量使用数组来保存前缀函数的值。虽然可以使用纯函数式的序列数据结构来代替数组，但序列通常使用finger树来实现。与命令式环境中的数组相比，finger树随机访问元素的性能为对数时间\footnote{我们在这里不使用数组。虽然在某些函数式编程环境中，例如Haskell提供了可以在常数时间进行随机访问的数组。}。

Richard Bird给出了一个使用fold fusion定理推导KMP算法的过程（\cite{fp-pearls}第17章）。本节中，我们首先给出一个暴力法的前缀函数构造方法，然后逐步改进得到KMP算法。

在函数式环境中，文本和待搜索的字符串本质上都是单向链表。在扫描过程中，两个列表被分解，每个列表都被分成两部分。如图\ref{fig:fp-strstr}所示，待搜索的字符串的前$j$个字符都相附，接下来要比较$T[i+1]$和$P[j+1]$。如果相等，就将这一字符添加到已成功比较的部分。但是由于字符串由单向链表表示，向尾部添加的性能和其长度$j$成比例。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/fp-strstr.ps}
 \caption{$P$的前$j$个字符都相符，接下来比较$P[j+1]$和$T[i+1]$。}
 \label{fig:fp-strstr}
\end{figure}

记文本的前$i$个字符为$T_p$，表示$T$的前缀，剩余的字符为$T_s$，表示$T$的后缀；同样，记$P$的前$j$个字符为$P_p$，剩余字符为$P_s$；记$T_s$中的第一个字符为$t$，$P_s$中的第一个字符为$p$。我们可以得到如下的“cons”关系。

\[
\begin{array}{l}
T_s = cons(t, T_s') \\
P_s = cons(p, P_s')
\end{array}
\]

若$t = p$，则下面的更新过程需要线性时间：

\[
\begin{array}{l}
T_p' = T_p \cup \{t\} \\
P_p' = P_p \cup \{p\}
\end{array}
\]

我们在队列一章中曾经介绍过一种方法，可以解决这一问题。通过使用一对列表，一个front列表和一个rear列表，可以将线性时间的添加操作转换成常数时间的链接操作。为此，需要将前缀的部分用逆序表达。

\be
\begin{array}{l}
T = T_p \cup T_s = reverse(reverse(T_p)) \cup T_s = reverse(\overleftarrow{T_p}) \cup T_s \\
P = P_p \cup P_s = reverse(reverse(P_p)) \cup P_s = reverse(\overleftarrow{P_p}) \cup P_s \\
\end{array}
\ee

我们分别用$(\overleftarrow{T_p}, T_s)$和$(\overleftarrow{P_p}, P_s)$来表达文本和待搜索的字符串。这样当$t=p$时，就可以用常数时间，快速更新前缀部分。

\be
\begin{array}{l}
\overleftarrow{T_p'} = cons(t, \overleftarrow{T_p}) \\
\overleftarrow{P_p'} = cons(p, \overleftarrow{P_p})
\end{array}
\ee

KMP查找算法开始时，已成功匹配的前缀部分初始化为空串。

\be
search(P, T) = kmp(\pi, (\Phi, P) (\Phi, T))
\ee

其中$\pi$是此前介绍过的前缀函数。除构造前缀函数外的KMP核心算法可以定义如下。

\be
kmp(\pi, (\overleftarrow{P_p}, P_s), (\overleftarrow{T_p}, T_s)) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ |\overleftarrow{T_p}| \} & P_s = \Phi \land T_s = \Phi \\
  \Phi & P_s \neq \Phi \land T_s = \Phi \\
  \{ |\overleftarrow{T_p} \} \cup kmp(\pi, \pi(\overleftarrow{P_p}, P_s), (\overleftarrow{T_p}, T_s)) & P_s = \Phi \land T_s \neq \Phi \\
  kmp(\pi, (\overleftarrow{P_p'}, P_s'), (\overleftarrow{T_p'}, T_s')) & t = p \\
  kmp(\pi, \pi(\overleftarrow{P_p}, P_s), (\overleftarrow{T_p'}, T_s')) & t \neq p \land \overleftarrow{P_p} = \Phi \\
  kmp(\pi, \pi(\overleftarrow{P_p}, P_s), (\overleftarrow{T_p}, T_s)) & t \neq p \land \overleftarrow{P_p} \neq \Phi
  \end{array}
\right.
\ee

第一行表示，若同时扫描完文本和待搜索字串，则获得一个解，同时算法结束。这里我们使用文本中的右侧位置作为搜索到的位置。如果要使用左侧位置，只需要用右侧位置减去待搜索串的长度即可。简单起见，在函数式的解法中，我们使用右侧位置。

第二行表示，若文本已经扫描结束，但是待搜索的字串中仍然有尚未扫描的字符，则不存在解，并且算法结束。

第三行表示，如果带搜索的字串已全部扫描匹配成功，但是文本中仍然存在未扫描的字符，我们得到一个解，但是需要继续搜索。此时我们调用前缀函数$\pi$，获得下一个继续搜索的起始位置。

第四行处理待搜索字串中的下一个字符和文本中的下一个字符相同的情况。此时需要同时向前移动一个字符，然后递归进行搜索。

如果下一个字符不同，并且恰好是待搜索字串的第一个字符，我们只需要移动到文本中的下一个字符，然后重新查找。否则，如果不是待搜索字串中的第一个字符，我们就调用前缀函数$\pi$，获取到回退的位置，以继续进行搜索。

可以用暴力方法构造前缀函数，只要简单地按照式(\ref{eq:prefix-function})的定义即可。

\be
\pi(\overleftarrow{P_p}, P_s) = (\overleftarrow{P_p'}, P_s')
\ee

其中

\[
\begin{array}{l}
P_p' = longest(\{ s | s \in prefixes(P_p), s \sqsupset P_p \}) \\
P_s' = P - P_p'
\end{array}
\]

每次计算回退的位置时，暴力法都简单地穷举所有$P_p$的前缀，检查它是否同时也是$P_p$的后缀，然后选择最长的一个作为结果。这里我们使用了减号表示获取列表的不同部分。

这里需要避免一种特殊情况。由于任何字符串本身都同时是自己的前缀和后缀，即$P_p \sqsubset P_p$和$P_p \sqsupset P_p$总成立，因此不能将$P_p$作为一个候选的前缀。下面给出了穷举前缀算法的定义：

\be
prefixes(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ \Phi \} & L = \Phi \lor |L| = 1 \\
  cons(\Phi, map(\lambda_s \cdot cons(l_1, s), prefixes(L'))) & otherwise
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了对应的字符串查找算法。

\lstset{language=Haskell}
\begin{lstlisting}
kmpSearch1 ptn text = kmpSearch' next ([], ptn) ([], text)

kmpSearch' _ (sp, []) (sw, []) = [length sw]
kmpSearch' _ _ (_, []) = []
kmpSearch' f (sp, []) (sw, ws) = length sw : kmpSearch' f (f sp []) (sw, ws)
kmpSearch' f (sp, (p:ps)) (sw, (w:ws))
    | p == w = kmpSearch' f ((p:sp), ps) ((w:sw), ws)
    | otherwise = if sp ==[] then kmpSearch' f (sp, (p:ps)) ((w:sw), ws)
                  else kmpSearch' f (f sp (p:ps)) (sw, (w:ws))

next sp ps = (sp', ps') where
    prev = reverse sp
    prefix = longest [xs | xs <- inits prev, xs `isSuffixOf` prev]
    sp' = reverse prefix
    ps' = (prev ++ ps) \\ prefix
    longest = maximumBy (compare `on` length)

inits [] = [[]]
inits [_] = [[]]
inits (x:xs) = [] : (map (x:) $ inits xs)
\end{lstlisting} %$

这一算法不仅性能差，而且也很复杂。我们可以对其略作简化。观察KMP搜索过程，它实际上是一个从左向右的对文本进行扫描的过程，因此可以使用fold来表示（参见附录A）。首先，在fold的过程中，我们可以让每一个字符对应一个index，如下：

\be
zip(T, \{1, 2, ... \})
\ee

将文本和自然数zip起来，得到一个列表，每个元素都是一对值。例如文本“The quick brown fox jumps over the lazy dog”这样处理后的结果是：T, 1), (h, 2), (e, 3), ..., (o, 42), (g, 43)。

fold起始时的状态包含两部分，第一部分是待搜索字符串$(P_p, P_s)$，其中前缀起始时为空，后缀为完成的待搜索串，即$(\Phi, P)$。为了方便，我们暂时不用$(\overleftarrow{P_p}, P_s)$的表示法，在最终的定义中我们需要再次改回来。起始状态的另外一部分是已找到的解的列表，它初始为空。fold结束时，这一列表包含所有找到的解。我们需要将其取出，作为最终的结果。这样核心的KMP算法定义可简化如下：

\be
kmp(P, T) = snd(fold(search, ((\Phi, P), \Phi), zip(T, \{1, 2, ... \})))
\ee

这里唯一的“黑盒子”是$search$函数，它接受一个状态，和一个字符——索引对，计算后返回一个新的状态作为结果。记$P_s$中的第一个字符为$p$，剩余的字符为$P_s'$（即$P_s = cons(p, P_s')$），我们有如下的定义：

\be
search(((P_p, P_s), L), (c, i)) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  ((P_p \cup {p}, P_s'), L \cup \{i\}) & p = c \land P_s' = \Phi \\
  ((P_p \cup {p}, P_s'), L) & p = c \land P_s' \neq \Phi \\
  ((P_p, P_s), L) & P_p = \Phi \\
  search((\pi(P_p, P_s), L), (c, i)) & otherwise
  \end{array}
\right.
\ee

如果$P_s$中的第一个字符和当前扫描的字符$c$相等，我们需要检查是否待搜索串种的所有字符都已扫描完毕，如果已完毕，则找到了一个解，我们将这一位置$i$记录到列表$L$中；如果尚未完毕，我们向前移动一个字符，然后继续。如果$p$和$c$不同，我们需要回退到某个位置，然后重新搜索。但是有一个特殊情况，我们不能回退：当$P_p$为空时，我们需要保持当前的状态。

前缀函数$\pi$的定义也可以略微简化。我们要寻找的是一个最长子串，它即是$P_p$前缀，同时也是它后缀。我们可以从右向左扫描。对于任何非空列表$L$，记表中第一个元素为$l_1$，剩余的部分为$L'$，定义函数$init(L)$返回除最后一个元素外的所有其他元素。

\be
init(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & |L| = 1 \\
  cons(l_1, init(L')) & otherwise
  \end{array}
\right.
\ee

注意，这一定义不能处理列表为空的情况。从右向左扫描$P_p$，就是首先检查$init(P_p) \sqsupset P_p$是否成立，如果是，则成功；否则我们接下来检查$init(init(P_p))$是否可以，并且重复这一过程直到最左侧。这样前缀函数的定义就可以简化如下：

\be
\pi(P_p, P_s) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (P_p, P_s) & P_p = \Phi \\
  fallback(init(P_p), cons(last(P_p), P_s)) & otherwise
  \end{array}
\right.
\ee

其中

\be
fallback(A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (A, B) & A \sqsupset P_p \\
  (init(A), cons(last(A), B)) & otherwise
  \end{array}
\right.
\ee

由于空串是任何字符串的后缀，因此函数fallback一定能结束。函数$last(L)$返回一个列表的最后一个元素，它同样是一个线性时间的操作（参见附录A）。但如果我们使用$\overleftarrow{P_p}$的表示法，则获取最后一个元素就是一个常数时间的操作。这一改进的前缀函数的复杂度为线性时间，但和命令式的算法相比，仍然很慢。因为命令式算法可以在常数时间进行前缀函数的检索。下面的Haskell例子程序实现了这一改进。

\lstset{language=Haskell}
\begin{lstlisting}
failure ([], ys) = ([], ys)
failure (xs, ys) = fallback (init xs) (last xs:ys) where
    fallback as bs | as `isSuffixOf` xs = (as, bs)
                   | otherwise = fallback (init as) (last as:bs)

kmpSearch ws txt = snd $ foldl f (([], ws), []) (zip txt [1..]) where
    f (p@(xs, (y:ys)), ns) (x, n) | x == y = if ys==[] then ((xs++[y], ys), ns++[n])
                                             else ((xs++[y], ys), ns)
                                  | xs == [] = (p, ns)
                                  | otherwise = f (failure p, ns) (x, n)
    f (p, ns) e = f (failure p, ns) e
\end{lstlisting} %$

瓶颈在于，在纯函数式的环境中，我们无法使用内置的array来记录前缀函数。实际上，前缀函数可以被看作是一个状态转移函数。它根据字符匹配成功与否将一个状态转移到另一个状态。我们可以将这样的状态转换抽象为一棵树。在支持代数数据类型（algebraic data type）的环境中，例如Haskell，这样的状态树可以定义如下：

\lstset{language=Haskell}
\begin{lstlisting}
data State a = E | S a (State a) (State a)
\end{lstlisting}

一个状态或者为空，或者包含三部分：当前的状态，如果匹配失败后转移到的状态，和匹配成功后转移到的状态。这一定义和二叉树的定义很像。我们将其称为“左侧失败，右侧成功”树。这里的具体状态为$(P_p, P_s)$。

在命令式的KMP算法中，我们通过待搜索字串构造前缀函数。与此类似，我们可以通过待搜索字串构造状态转移树。我们从起始状态$(\Phi, P)$开始，它的两个子状态最初为空。我们调用上面定义的$\pi$获得一个新状态，用它替换掉左侧子节点，然后通过向前前进一个字符得到一个新状态并替换右侧子节点。这里有一种特殊情况，当状态转移到$(P, \Phi)$时，如果匹配成功，我们无法继续前进。这样的节点只含有失败的子状态。下面定义了构造状态转移树的的函数。

\be
build((P_p, P_s), \Phi, \Phi) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  build(\pi(P_p, P_s), \Phi, \Phi) & P_s = \Phi \\
  build((P_p, P_s), L, R) & otherwise
  \end{array}
\right.
\ee

其中

\[
\begin{array}{l}
L = build(\pi(P_p, P_s), \Phi, \Phi) \\
R = build((P_s \cup \{p\}, P_s'), \Phi, \Phi))
\end{array}
\]

其中$p$和$P_s'$的含义和此前相同，$p$是$P_s$中的第一个字符，$P_s'$是剩余部分。最有趣的一点是，build函数永远不会结束。它无休无止地构造一棵无穷树。在strict的编程环境中，调用这样的函数会陷入麻烦。但在支持惰性求值的环境中，只有被使用的节点才会被构造。Lisp方言Scheme和Haskell都可以构造这样的无穷状态树。在命令式环境中，我们通常使用指向祖先节点的指针来实现无穷状态树。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.4]{img/fallback-tr.ps}
 \caption{从字符串“ananym”构造的无穷状态树。}
 \label{fig:fallback-tree}
\end{figure}

图\ref{fig:fallback-tree}描述了从字符串“ananym”对应的无穷状态树。其中最右侧的边对应了字符匹配一直连续成功的特殊情况。此后，由于所有的字符都匹配完毕，所有后继的右侧子树为空。根据这一点，我们可以定义一个辅助函数来判断是否一个状态代表待搜索字符串已经完全匹配成功。

\be
match((P_p, P_s), L, R) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  True & P_s = \Phi \\
  False & otherwise
  \end{array}
\right.
\ee

通过使用状态转移树，我们可以用一个自动机来实现KMP算法。

\be
kmp(P, T) = snd(fold(search, (Tr, []), zip(T, \{1, 2, ... \})))
\ee

其中，$Tr = build((\Phi, P), \Phi, \Phi)$是无穷状态转移树。函数$search$根据匹配成功与否，使用这棵树进行状态转移。及$P_s$中的第一个字符为$p$，剩余部分为$P_s'$，$A$代表已找到的解的位置。

\be
search((((P_p, P_s), L, R), A), (c, i)) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (R, A \cup \{ i \}) & p = c \land match(R) \\
  (R, A) & p = c \land \neg{match(R)} \\
  ((((P_p, P_s), L, R), A) & P_p = \Phi \\
  search((L, A), (c, i)) & otherwise
  \end{array}
\right.
\ee

下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
data State a = E | S a (State a) (State a) -- state, ok-state, fail-state
               deriving (Eq, Show)

build :: (Eq a)=>State ([a], [a]) -> State ([a], [a])
build (S s@(xs, []) E E) = S s (build (S (failure s) E E)) E
build (S s@(xs, (y:ys)) E E) = S s l r where
    l = build (S (failure s) E E) -- fail state
    r = build (S (xs++[y], ys) E E)

matched (S (_, []) _ _) = True
matched _ = False

kmpSearch3 :: (Eq a) => [a] -> [a] -> [Int]
kmpSearch3 ws txt = snd $ foldl f (auto, []) (zip txt [1..]) where
    auto = build (S ([], ws) E E)
    f (s@(S (xs, ys) l r), ns) (x, n)
        | [x] `isPrefixOf` ys = if matched r then (r, ns++[n])
                                else (r, ns)
        | xs == [] = (s, ns)
        | otherwise = f (l, ns) (x, n)
\end{lstlisting} %$

目前的瓶颈在于构造状态转移树的时候，需要调用$\pi$函数计算回退的位置，而前缀函数$\pi$的定义效率很差。这是由于它每次都从右向左穷举所有可能前缀。

由于状态树是无穷的，我们可以使用处理无穷数据结构的常见方法。典型的例子就是斐波那契数列。斐波那契数列的前两个值为0和1，其余的斐波那契数可以通过将前面的两个值相加得到：

\be
\begin{array}{l}
F_0 = 0 \\
F_1 = 1 \\
F_n = F_{n-1} + F_{n-2} \\
\end{array}
\ee

这样，就可以依次列出所有的斐波那契数。

\be
\begin{array}{l}
F_0 = 0 \\
F_1 = 1 \\
F_2 = F_1 + F_0 \\
F_3 = F_2 + F_1 \\
...
\end{array}
\ee

将上述等式左右两侧的所有数字汇集起来，定义无穷斐波那契数列为$F = \{ 0, 1, F_1, F_2, ... \}$，我们有下面的等式：

\be
\begin{array}{rl}
F = & \{ 0, 1, F_1 + F_0, F_2 + F_1, ... \}  \\
  = & \{ 0, 1 \} \cup \{ x + y | x \in \{ F_0, F_1, F_2, ...\}, y \in \{ F_1, F_2, F_3, ... \}\} \\
  = & \{ 0, 1 \} \cup \{ x + y | x \in F, y \in F'\}
\end{array}
\ee

其中$F' = tail(F)$是除第一个元素外的所有斐波那契数。在支持惰性求值的环境中，如Haskell，这一定义可以实现如下。

\lstset{language=Haskell}
\begin{lstlisting}
fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
\end{lstlisting}

无穷斐波那契数列的递归定义可以启发我们找到避免使用函数$\pi$进行回退的方法。记状态转移树为$T$，我们可以定义一个用这棵树匹配字符时的状态转移函数。

\be
trans(T, c) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  root & T = \Phi \\
  R & T = ((P_p, P_s), L, R), c = p \\
  trans(L, c) & otherwise
  \end{array}
\right.
\ee

如果匹配一个字符时节点为空，我们转移到树的根节点。稍后我们会定义根节点。否则，我们比较字符$c$和$P_s$的第一个字符$p$。如果相等，我们就转移到右侧子树表示成功；否则，我们转移到左侧子树表示失败。

通过使用状态转移函数，我们可以定义一个新的状态树构造算法。原理和前面的斐波那契序列类似。

\[
build(T, (P_p, P_s)) = ((P_p, P_s), T, build(trans(T, p), (P_p \cup \{ p \}, P_s')))
\]

等式右侧包含三部分。第一部分是我们正在搜索的状态$(P_p, P_s)$；如果匹配失败，由于$T$本身可以处理任何失败的情况，我们直接使用它作为左侧子树；否则匹配成功，我们前进一个字符，递归构造右侧子树，并调用我们定义的状态转移函数。

这里还必须处理一种特殊情况，如果$P_s$为空，表示匹配了所有的字符。根据上面的定义，将不存在后继的右侧子树。综合起来，我们可以得到下面的构造函数。

\be
build(T, (P_p, P_s)) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  ((P_p, P_s), T, \Phi) & P_s = \Phi \\
  ((P_p, P_s), T, build(trans(T, p), (P_p \cup \{ p \}, P_s'))) & otherwise
  \end{array}
\right.
\ee

最后，我们还需要定义无穷状态转移树的根节点，用以初始化构造过程。

\be
root = build(\Phi, (\Phi, P))
\ee

使用这一根节点定义，我们可以给出一个新的KMP搜索算法。

\be
kmp(P, T) = snd(fold(trans, (root, []), zip(T, \{1, 2, ... \})))
\ee

下面的Haskell例子程序实现了这一KMP算法。

\lstset{language=Haskell}
\begin{lstlisting}
kmpSearch ws txt = snd $ foldl tr (root, []) (zip txt [1..]) where
    root = build' E ([], ws)
    build' fails (xs, []) = S (xs, []) fails E
    build' fails s@(xs, (y:ys)) = S s fails succs where
        succs = build' (fst (tr (fails, []) (y, 0))) (xs++[y], ys)
    tr (E, ns) _ = (root, ns)
    tr ((S (xs, ys) fails succs), ns) (x, n)
        | [x] `isPrefixOf` ys = if matched succs then (succs, ns++[n]) else (succs, ns)
        | otherwise = tr (fails, ns) (x, n)
\end{lstlisting} %$

图\ref{fig:lazy-fallback-tree}给出了在文本“anal”中搜索“anaym”的前4步。由于前三步的字符都匹配成功，所以这3个状态的左侧子树都没有被构造。它们被标记为“?”。在第4步，字符匹配失败，因此无需构造右侧的子树。同时，我们必须根据$trans(right(right(right(T))), n)$的结果构造左侧的子树，其中函数$right(T)$返回树$T$的右侧子树。根据构造过程和状态转移的定义，这一结果最终展开到一个具体的状态$((a, nanym), L, R)$。具体的推导过程留给读者作为练习。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.4]{img/lazy-fallback-tr.ps}
 \caption{在文本“anal”中搜索字符串“ananym”，按需构造构造状态转移树。}
 \label{fig:lazy-fallback-tree}
\end{figure}

这一算法的实现依赖于惰性求值。所有被转移到的状态都是按需构造。构造过程的分摊复杂度为$O(m)$，算法的整体分摊性能为$O(m+n)$。读者可以参考\cite{fp-pearls}了解详细的证明。

在我们此前介绍的很多形况中，函数式算法通常比较简洁。但是在KMP搜索中，命令式算法却更加简单、直观。这主要是由于我们通过无穷状态转移树来模拟内置数组造成的。

\subsubsection{Boyer-Moore}
\index{Boyer-Moore算法}

Boyer-Moore字符串匹配算法是1977年发现的另一种高效的字符串查找方法\cite{boyer-moore}。它的思想来自于下面的一些事实。

\paragraph{不良字符启发条件}

当匹配待搜索的字符串时，即使从左边开始有若干字符都匹配成功，如果最后一个字符不想等，最终结果仍然失败，如图\ref{fig:bad-char}所示。而且，即使我们将待搜索字符串向右侧平移1到2个单位，匹配仍然会失败。实际上，待查找的字符串“ananym”的长度为5，最后一个字符是‘m’，但是文本中对应的字符是‘h’。它根本没有出现在待搜索的字符串中。我们据此可以直接向右侧平移5个单位。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/strstr.ps}
 \caption{因为字符‘h’没有出现在待搜索的字符串中，向右侧平移少于待搜索串长度的距离都会匹配失败。}
 \label{fig:bad-char}
\end{figure}

从这点可以得到\underline{不良字符规则}。我们可以对待搜索字符串进行预处理。如果文本中的字符集已知，我们可以找到所有不存在于待搜索串种的不良字符。在后继的扫描过程中，只要遇到不良字符，我们就可以立即向右侧移动一个待搜索串长度的距离。接下来的问题是，如果文本中不匹配的字符存在于待搜索串中要如何处理？为了不漏掉任何可能的解，我们只能向右少量移动，然后重新搜索，如图\ref{fig:good-char}所示。

\begin{figure}[htbp]
 \centering
 \subfloat[待搜索串的最后一个字符‘e’和‘p’不匹配。但是‘p’出现在待搜索串中。]{\includegraphics[scale=0.5]{img/good-char.ps}} \\
 \subfloat[我们只能向右侧平移2个字符，然后重新检查。]{\includegraphics[scale=0.5]{img/good-char-2.ps}}
 \caption{如果不匹配的字符出现在待搜索串中，需要向右侧平移。}
 \label{fig:good-char}
\end{figure}

不匹配的字符很可能多次出现在待搜索串中。记待搜索串的长度为$|P|$，该字符出现的位置依次为$p_1, p_2, ..., p_i$。此时，我们需要用最后一个位置来计算平移的距离，以避免漏掉任何可能的解。

\be
s = |P| - p_i
\ee

根据这一公式，待搜索串种的最后一个字符对应的平移距离为0。在实现时，需要跳过这种情况。另外，由于平移的距离是根据待搜索串最后一个字符计算的（从$|P|$减去相应的值），当从右向左扫描时，无论在哪里发生了不匹配，我们都要检查待搜索串中最后一个字符正对的文本中的字符，是否出现在不良字符表中。如图\ref{fig:good-char-2}所示。

\begin{figure}[htbp]
 \centering
 \subfloat[]{\includegraphics[scale=0.5]{img/good-char-3.ps}}
 \subfloat[]{\includegraphics[scale=0.5]{img/good-char-4.ps}}
 \caption{即使字符‘i’和‘a’在中间位置匹配失败，我们要使用字符‘e’来查找平移的距离。得到结果6（根据第一个‘e’出现的位置计算，需要跳过第二个‘e’出现的位置以避免平移距离为0）。}
 \label{fig:good-char-2}
\end{figure}

在实际中，即使只使用不良字符规则也能够得到简单、快速的字符串查找算法，被称为Boyer-Moore-Horspool算法\cite{boyer-moore-horspool}。

\begin{algorithmic}[1]
\Procedure{Boyer-Moore-Horspool}{$T, P$}
  \For{$\forall c \in \Sigma$}
    \State $\pi[c] \gets |P|$
  \EndFor
  \For{$i \gets 1$ to $|P|-1$} \Comment{跳过最后一个位置}
    \State $\pi[P[i]] \gets |P| - i$
  \EndFor
  \State $s \gets 0$
  \While{$s + |P| \leq |T|$}
    \State $i \gets |P|$
    \While{$i \geq 1 \land P[i] = T[s+i]$} \Comment{从右侧开始扫描}
      \State $i \gets i - 1$
    \EndWhile
    \If{$i < 1$}
      \State found one solution at $s$
      \State $s \gets s + 1$ \Comment{继续寻找下一个解}
    \Else
      \State $s \gets s + \pi[T[s + |P|]]$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}

记字符集为$\Sigma$，我们首先将平移表的所有值都初始化为待搜索串的长度$|P|$。然后，我们从左向右处理待搜索串，更新相应的平移距离。如果某个字符在待搜索串中多次出现，在右侧后出现的值将覆盖此前的值。开始查找时，我们将文本和待搜索串的左侧对齐。但是对于每个对齐的位置$s$，我们都从右向左扫描，直到发生匹配失败，或者检查完待搜索串种的所有字符。后者说明我们发现了一个解；而对于前者，我们查找$\pi$并向右侧平移相应的距离。

下面的Python例子程序实现了这一算法。

\lstset{language=Python}
\begin{lstlisting}
def bmh_match(w, p):
    n = len(w)
    m = len(p)
    tab = [m for _ in range(256)]  # table to hold the bad character rule.
    for i in range(m-1):
        tab[ord(p[i])] = m - 1 - i
    res = []
    offset = 0
    while offset + m <= n:
        i = m - 1
        while i >= 0 and p[i] == w[offset+i]:
            i = i - 1
        if i < 0:
            res.append(offset)
            offset = offset + 1
        else:
            offset = offset + tab[ord(w[offset + m - 1])]
    return res
\end{lstlisting}

算法首先使用$O(|\Sigma| + |P|)$的时间构造平移表格。如果字符集很小，则性能主要由待搜索串的长度和文本的长度决定。显然，最坏的情况下，文本和待搜索串中的所有字符都相同，例如在文本“aa......a”($n$个字符‘a’，记为$a^n$)中搜索“aa...a”($m$个字符‘a’，记为$a^m$)。此时性能为$O(mn)$。当待搜索的字符较长，并且有常数个解的时候，算法的性能良好。为线性时间。这一结论和后面介绍的完整的Boyer-Moore算法在最好情况下的性能相同。

\paragraph{良好后缀启发条件}

考虑在文本“bbbababbabab...”中搜索“abbabab”，如图\ref{fig:bad-char-2}所示。根据bad字符规则，应该向右侧平移2个单位。

\begin{figure}[htbp]
 \centering
 \subfloat[]{\includegraphics[scale=0.5]{img/bad-char-1.ps}}
 \subfloat[]{\includegraphics[scale=0.5]{img/bad-char-2.ps}}
 \caption{根据不良字符规则，应向右平移2个单位，这样，下一个字符‘b’的位置相互对齐。}
 \label{fig:bad-char-2}
\end{figure}

实际上，我们可以做的更好。在匹配失败前，我们已经从右向左成功匹配了6个字符“bbabab”。由于“ab”既是待搜索串的前缀，也是已匹配部分的后缀，我们可以向右平移对齐这个后缀，如图\ref{fig:good-suffix-case1-1}所示。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/good-suffix-case1-1.ps}
 \caption{由于前缀“ab”也是已匹配部分的后缀，我们可以向右平移使得“ab”对齐。}
 \label{fig:good-suffix-case1-1}
\end{figure}

这和KMP算法中的预处理部分非常类似，但是我们不能总跳过这样多的字符。考虑如图\ref{fig:good-suffix-case2-1}所示的例子。在失败前，我们已匹配了“bab”。虽然前缀“ab”也是“bab”的后缀，我们却不能平移这么远。这是因为“bab”也在其它位置出现过，即待搜索串的第3个字符的位置。为了避免漏掉可能的解，我们只能向右平移2个单位。

\begin{figure}[htbp]
 \centering
 \subfloat[]{\includegraphics[scale=0.5]{img/good-suffix-case2-1.ps}}
 \subfloat[]{\includegraphics[scale=0.5]{img/good-suffix-case2-2.ps}}
 \caption{已匹配的部分“bab”也在待搜索串的其它位置出现（从第3个字符到第5个字符）。我们只能向右平移2个单位以避免漏掉可能的解。}
 \label{fig:good-suffix-case2-1}
\end{figure}

以上两种情况组成了\underline{良好后缀规则}，如图\ref{fig:good-suffix-cases}所示。

\begin{figure}[htbp]
 \centering
 \subfloat[情况1，已匹配的子串中，有一部分也同时是待搜索串的前缀。]{\includegraphics[scale=0.4]{img/good-suffix-case1.eps}} \hspace{.01\textwidth}
 \subfloat[情况2，匹配部分的后缀，也出现在待搜索串的其它位置。]{\includegraphics[scale=0.4]{img/good-suffix-case2.eps}}
 \caption{文本中浅灰色的部分代表已匹配的子串；深灰色的部分表示待搜索串种相同的内容。}
 \label{fig:good-suffix-cases}
\end{figure}

良好后缀规则用来处理多个字符已成功匹配的情况。如果下面任何一种情况发生，都可以向右平移一定的距离。

\begin{itemize}
\item 情况1，如果已匹配部分的某个后缀同时也是待搜索字串的前缀，并且这一后缀不出现在待搜索字符串的其他位置，我们可以将待搜索串向右侧平移，对齐这一前缀；
\item 情况2，如果已匹配部分的某个后缀也出现在待搜索串的其它位置，我们可以将待搜索串向右侧平移，使得最右侧出现的位置对齐。
\end{itemize}

在扫描的过程中，只要可能，要优先使用第2种情况，如果发现已匹配的后缀没有出现过，然后再检查情况1。由于良好后缀规则的两种情况都仅仅依赖于待搜索字符串，我们可以在搜索前进行预处理，构造出用于后继查询的表格。

简单起见，记$P$中从第$i$个字符开始的后缀为$\overline{P_i}$。即$\overline{P_i}$为子串$P[i]P[i+1]...P[m]$。

对于情况1，我们可以检查$P$的每个后缀，包括$\overline{P_m}$, $\overline{P_{m-1}}$, $\overline{P_{m-2}}$, ...,  $\overline{P_2}$，看它是否同时是$P$的前缀。可以通过从右向左进行一轮扫描实现。

对于情况2，我们可以检查$P$的每个前缀，包括$P_1$, $P_2$, ..., $P_{m-1}$，看它的最长后缀是否也是$P$的后缀。可以通过从左向右的另一轮扫描实现。

\begin{algorithmic}[1]
\Function{Good-Suffix}{$P$}
  \State $m \gets |P|$
  \State $\pi_s \gets \{0, 0, ..., 0\}$ \Comment{初始化一个长度为$m$的表格}
  \State $l \gets 0$ \Comment{最后的后缀也同时是$P$的前缀}
  \For{$i \gets m-1$ down-to $1$} \Comment{第一轮循环处理情况1}
    \If{$\overline{P_i} \sqsubset P$} \Comment{$\sqsubset$代表左侧是右侧的前缀}
      \State $l \gets i$
    \EndIf
    \State $\pi_s[i] \gets l$
  \EndFor
  \For{$i \gets 1$ to $m$} \Comment{第二轮循环处理情况2}
    \State $s \gets$ \Call{Suffix-Length}{$P_i$}
    \If{$s \neq 0 \land P[i - s] \neq P[m - s]$}
      \State $\pi_s[m - s] \gets m - i$
    \EndIf
  \EndFor
  \State \Return $\pi_s$
\EndFunction
\end{algorithmic}

这一算法构造良好后缀规则表$\pi_s$。它首先检查$P$的每个后缀，从最短的开始，到最长的结束。如果后缀$\overline{P_i}$同时是$P$的前缀，就将此后缀记录下来，并将其用于表格中所有的项，知道我们发现另一个后缀$\overline{P_j}$，$j < i$并且同时是$P$的前缀。

然后，这一算法逐一检查$P$的所有前缀，从最短的开始，到最长的结束。它调用函数\textproc{Suffix-Length}($P_i$)，来计算$P_i$中最长的一个同时是$P$前缀的后缀的长度。如果长度$s$不等于0，说明存在一个子串，同时也是待搜索串的后缀。它表明发生了情况2。算法修改表格$\pi_s$从右侧数的第$s$项的值。为了避免再次找到已匹配的后缀，我们需要检查$P[i - s]$和$P[m - s]$是否相等。

函数\textproc{Suffix-Length}的实现如下。

\begin{algorithmic}[1]
\Function{Suffix-Length}{$P_i$}
  \State $m \gets |P|$
  \State $j \gets 0$
  \While{$P[m - j] = P[i - j] \land j < i$}
    \State $j \gets j + 1$
  \EndWhile
  \State \Return $j$
\EndFunction
\end{algorithmic}

下面的Python例子程序实现了良好后缀规则。

\lstset{language=Python}
\begin{lstlisting}
def good_suffix(p):
    m = len(p)
    tab = [0 for _ in range(m)]
    last = 0
    # first loop for case 1
    for i in range(m-1, 0, -1): # m-1, m-2, ..., 1
        if is_prefix(p, i):
            last = i
        tab[i - 1] = last
    # second loop for case 2
    for i in range(m):
        slen = suffix_len(p, i)
        if slen != 0 and p[i - slen] != p[m - 1 - slen]:
            tab[m - 1 - slen] = m - 1 - i
    return tab

# test if p[i..m-1] `is prefix of` p
def is_prefix(p, i):
    for j in range(len(p) - i):
        if p[j] != p [i+j]:
            return False
    return True

# length of the longest suffix of p[..i], which is also a suffix of p
def suffix_len(p, i):
    m = len(p)
    j = 0
    while p[m - 1 - j] == p[i - j] and j < i:
        j = j + 1
    return j
\end{lstlisting}

当匹配失败时，不良字符规则和良好后缀规则可能同时适用。Boyer-Moore算法比较这两种规则的结果，并选择较大的平移值以获得更快的速度。不良字符规则的表格可以按照如下的实现构造。

\begin{algorithmic}[1]
\Function{Bad-Character}{$P$}
  \For{$\forall c \in \Sigma$}
    \State $\pi_b[c] \gets |P|$
  \EndFor
  \For{$i \gets 1$ to $|P|-1$}
    \State $\pi_b[P[i]] \gets |P| - i$
  \EndFor
  \State \Return $\pi_b$
\EndFunction
\end{algorithmic}

下面的Python例子程序实现了不良字符规则表的构造算法。

\lstset{language=Python}
\begin{lstlisting}
def bad_char(p):
    m = len(p)
    tab = [m for _ in range(256)]
    for i in range(m-1):
        tab[ord(p[i])] = m - 1 - i
    return tab
\end{lstlisting}

最终的Boyer-Moore算法首先从待搜索串构造出两个规则表，将待搜索串和文本的左侧对齐，对每个对齐位置，都进行从右向左的扫描。如果不匹配发生，就尝试使用两种规则，并选择较大的距离向右侧平移。

\begin{algorithmic}[1]
\Function{Boyer-Moore}{$T, P$}
  \State $n \gets |T|, m \gets |P|$
  \State $\pi_b \gets$ \Call{Bad-Character}{$P$}
  \State $\pi_s \gets$ \Call{Good-Suffix}{$P$}
  \State $s \gets 0$
  \While{$s + m \leq n$}
    \State $i \gets m$
    \While{$i \geq 1 \land P[i] = T[s + i]$}
      \State $i \gets i - 1$
    \EndWhile
    \If{$i < 1$}
      \State found one solution at $s$
      \State $s \gets s + 1$ \Comment{继续寻找下一个解}
    \Else
      \State $s \gets s + max(\pi_b[T[s + m]], \pi_s[i])$
    \EndIf
  \EndWhile
\EndFunction
\end{algorithmic}

下面的Python例子程序，完整地实现了Boyer-Moore算法。

\lstset{language=Python}
\begin{lstlisting}
def bm_match(w, p):
    n = len(w)
    m = len(p)
    tab1 = bad_char(p)
    tab2 = good_suffix(p)
    res = []
    offset = 0
    while offset + m <= n:
        i = m - 1
        while i >= 0 and p[i] == w[offset + i]:
            i = i - 1
        if i < 0:
            res.append(offset)
            offset = offset + 1
        else:
            offset = offset + max(tab1[ord(w[offset + m - 1])], tab2[i])
    return res
\end{lstlisting}

最初发表的Boyer-Moore算法，在最坏的情况下，只有当待搜索串不出现在文本中时，性能才是$O(n+m)$\cite{boyer-moore}。在1977年，Knuth、Morris和Pratt证明了这一结论。但是，当待搜索串出现在文本中时，如前所述，Boyer-Moore算法在最坏情况下的性能为$O(nm)$。

我们在此略过Boyer-Moore算法的纯函数式实现，读者可以参考Richard Birds在\cite{fp-pearls}的第16章，给出的纯函数式的Boyer-Moore算法。

\begin{Exercise}
\begin{itemize}
\item 证明Boyer-Moore众数算法的正确性。
\item 对于任意列表，寻找其中出现最多的元素。是否存在分而治之的解法？是否存在分而治之的数据结构，例如map可供使用？
% see /others/problems/problems/majority-elem folder.
\item 如何找到一个列表中出现次数超过1/3的元素？如何找到一个列表中出现次数超过1/m的元素？
% use a map with capacity of m, the keys are the elements, the values are the occurrence.
% scans the list and fill the map, when the map is full (contains m different keys), reduce
% all values by 1, and remove the keys with zero occurrence.
% After the scan completion, the elements left in the map are possible answers, We need
% verify them by another round of scan.
% The overall complexity is O(n) if m is small (such as 3), and O(n \lg m) for tree based map.
\item 如果空数组不算合法的子数组，如何解决子数组最大和问题？
%% int maxsum'(int* xs, int n) {
%%     int i, m, sum;
%%     for (m = sum = xs[0], i = 1; i < n; ++i) {
%%         sum = max(sum + xs[i], xs[i]);
%%         m = max(m, sum);
%%     }
%%     return m;
%% }
\item Bentley在\cite{programming-pearls}中给出了一个分而治之的方法求子数组最大和。复杂度为$O(n \log n)$。思路是将列表在中点分成两份。我们可以递归地找出前半部分的最大和，和后半部分的最大和；但是我们还需要找出跨越中点部分的最大和，方法是从中点开始向左右两侧扫描：
\begin{algorithmic}[1]
\Function{Max-Sum}{$A$}
  \If{$A = \Phi$}
    \State \Return 0
  \ElsIf{$|A| = 1$}
    \State \Return \Call{Max}{$0, A[1]$}
  \Else
    \State $m \gets \lfloor \frac{|A|}{2} \rfloor$
    \State $a \gets$ \textproc{Max-From}(\Call{Reverse}{$A[1...m]$})
    \State $b \gets$ \Call{Max-From}{$A[m+1...|A|]$}
    \State $c \gets$ \Call{Max-Sum}{$A[1...m]$}
    \State $d \gets$ \Call{Max-Sum}{$A[m+1...|A|$}
    \State \Return \textproc{Max}($a+b, c, d$)
  \EndIf
\EndFunction
\Statex
\Function{Max-From}{$A$}
  \State $sum \gets 0, m \gets 0$
  \For{$i \gets 1$ to $|A|$}
    \State $sum \gets sum + A[i]$
    \State $m \gets $ \Call{Max}{$m, sum$}
  \EndFor
  \State \Return $m$
\EndFunction
\end{algorithmic}
易知，这一方法的存在性能关系$T(n) = 2T(n/2) + O(n)$。选择一门编程语言，实现这一算法。
\item 给定$n$个非负整数，用以表示一个一维等高地图，每个高度条的宽度都为1，计算降雨后这一地形的积水数量。图\ref{fig:rain-fill}给出了一个例子。
\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{scan/rain-fill/img/rain-fill.eps}
 \caption{灰色的区域表示积水。}
 \label{fig:rain-fill}
\end{figure}
例如，等高地图数据为$\{0,1,0,2,1,0,1,3,2,1,2,1\}$，则积水数量为6。
\item 解释在看起来“最坏”的情况下，为何KMP算法的性能仍然为线性？
\item 使用逆序的$P_p$以避免线性时间的添加操作，改进实现纯函数式的KMP算法。
\item 在文本“anal”中搜索字符串“ananym”，试推导树$left(right(right(right(T))))$的状态。
\end{itemize}
\end{Exercise}

\section{解的搜索}

计算机程序可以用于解答某些趣题。在人工智能的早期阶段，人们发展出了搜索解的许多方法。和序列搜索、字符串匹配不同，问题的解并不一定直接存在于一个候选答案集中。往往需要一边构造解，一边进行尝试。某些问题可解，同时也存在大量无解的问题。即使是有解的问题，也通常存在多个解。例如，一个迷宫可能存在多种走出的路线。人们往往需要求出某种意义下的最优解。

\subsection{深度优先搜索（DFS）和广度优先搜索（BFS）}
\index{DFS} \index{Deep-first search}
DFS和BFS分别代表深度优先搜索和广度优先搜索。它们通常作为图搜索算法加以介绍。图是一个很大的题目，超出了本书讲述的基本算法的范围。本节中，我们主要介绍如何使用DFS和BFS解决某些趣题，而不会正式介绍图的概念。

\subsubsection{迷宫}
\index{Maze problem}
迷宫的历史悠久，广受欢迎、是老少皆宜的一类趣题。图\ref{fig:maze}给出了一个迷宫的例子。在某些公园，甚至还建有真正的迷宫供人游玩。在1990年代末，机器老鼠走迷宫的竞赛一度在世界上流行。

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/maze.eps}
 \caption{一个迷宫的例子}
 \label{fig:maze}
\end{figure}

There are multiple methods to solve maze puzzle. We'll introduce an effective, yet not the best one in this
section. There are some well known sayings about how to find the way out in maze, while not all of them are
true.

For example, one method states that, wherever you have multiple ways, always turn right. This doesn't work
as shown in figure \ref{fig:maze-loop}. The obvious solution is first to go along the top horizontal line,
then turn right,
and keep going ahead at the 'T' section. However, if we always turn right, we'll endless
loop around the inner big block.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.6]{img/maze-loop.eps}
 \caption{It leads to loop way if always turns right.}
 \label{fig:maze-loop}
\end{figure}

This example tells us that the decision when there are multiple choices matters the solution. Like the
fairy tale we read in our childhood, we can take some bread crumbs in a maze. When there
are multiple ways, we can simply select one, left a piece of bread crumbs to mark this attempt.
If we enter a died end, we go back to the last place where
we've made a decision by back-tracking the bread crumbs. Then we can alter to another way.

At any time, if we find there have been already bread crumbs left, it means we have entered a loop, we
must go back and try different ways. Repeat these try-and-check steps, we can either find the way
out, or give the `no solution' fact. In the later case, we back-track to the start point.

One easy way to describe a maze, is by a $m \times n$ matrix, each element is either 0 or 1, which
indicates if there is a way at this cell. The maze
illustrated in figure \ref{fig:maze-loop} can be defined as the following matrix.

\[
\begin{matrix}
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
1 & 1 & 1 & 1 & 1 & 0
\end{matrix}
\]

Given a start point $s=(i, j)$, and a goal $e=(p, q)$, we need find all solutions, that are the
paths from $s$ to $e$.

There is an obviously recursive exhaustive search method. That in order to find all paths from
$s$ to $e$, we can check all connected points to $s$, for every such point $k$, we recursively
find all paths from $k$ to $e$. This method can be illustrated as the following.

\begin{itemize}
\item Trivial case, if the start point $s$ is as same as the target point $e$, we are done;
\item Otherwise, for every connected point $k$ to $s$, recursively find the paths from $k$
to $e$; If $e$ can be reached via $k$, put section $s$-$k$ in front of each path between $k$
and $e$.
\end{itemize}

However, we have to left 'bread crumbs' to avoid repeatedly trying the same attempts. This is because
otherwise in the recursive case, we start from $s$, find a connected point $k$, then we further
try to find paths from $k$ to $e$. Since $s$ is connected to $k$ as well, so in the next
recursion, we'll try to find paths from $s$ to $e$ again. It turns to be the very same origin
problem, and we are trapped in infinite recursions.

Our solution is to initialize an empty list, use it to record all the points we've visited so
far. For every connected point, we look up the list to examine if it has already been visited.
We skip all the visited candidates and only try those new ones. The corresponding algorithm can be
defined like this.

\be
solveMaze(m, s, e) = solve(s, \{ \Phi \})
\label{eq:solve-maze-reversed}
\ee

Where $m$ is the matrix which defines a maze, $s$ is the start point, and $e$ is the end point.
Function $solve$ is defined in the context of $solveMaze$, so that the maze and the end point
can be accessed. It can be realized recursively like what we described above\footnote{Function
$concat$ can flatten a list of lists. For example.
$concat(\{\{a, b, c\}, \{x, y, z\}\}) = \{a, b, c, x, y, z\}$. Refer to appendix A for detail.}.

\be
solve(s, P) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ \{ s\} \cup p | p \in P \} & s = e \\
  \begin{array}{rl}
  concat(\{ & solve(s', \{ \{ s\} \cup p | p \in P \}) | \\
            & s' \in adj(s), \lnot visited(s')\})
  \end{array} & otherwise
  \end{array}
\right.
\ee

Note that $P$ also serves as an accumulator. Every connected point is recorded in all the
possible paths to the current position. But they are stored in reversed order, that is
the newly visited point is put to the head of all the lists, and the starting point is the last
one. This is because the appending operation is linear ($O(n)$, where $n$ is
the number of elements stored in a list), while linking to the head is just constant time.
We can output the result in correct order by reversing all possible solutions
in equation (\ref{eq:solve-maze-reversed})\footnote{the detailed definition of $reverse$ can
be found in the appendix A.}:

\be
solveMaze(m, s, e) = map(reverse, solve(s, \{ \Phi \}))
\ee

We need define functions $adj(p)$ and $visited(p)$, which finds all the connected
points to $p$, and tests if point $p$ has been visited respectively.
Two points are connected if and only if they are next cells horizontally
or vertically in the maze matrix, and both have zero value.

\be
\begin{array}{ll}
adj((x, y)) = \{ (x', y') | & (x', y') \in \{ (x-1, y), (x+1, y), (x, y-1), (x, y+1)\}, \\
 & 1 \leq x' \leq M, 1 \leq y' \leq N, m_{x' y'} = 0\} \\
\end{array}
\ee

Where $M$ and $N$ are the widths and heights of the maze.

Function $visited(p)$ examines if point $p$ has been recorded in any lists in $P$.

\be
visited(p, P) = \exists path \in P, p \in path
\ee

The following Haskell example code implements this algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
solveMaze m from to = map reverse $ solve from [[]] where
    solve p paths | p == to = map (p:) paths
                  | otherwise = concat [solve p' (map (p:) paths) |
                                        p' <- adjacent p,
                                        not $ visited p' paths]
    adjacent (x, y) = [(x', y') |
                    (x', y') <- [(x-1, y), (x+1, y), (x, y-1), (x, y+1)],
                    inRange (bounds m) (x', y'),
                    m ! (x', y') == 0]
    visited p paths = any (p `elem`) paths
\end{lstlisting} %$

For a maze defined as matrix like below example, all the solutions can be given
by this program.

\lstset{language=Haskell}
\begin{lstlisting}
mz = [[0, 0, 1, 0, 1, 1],
      [1, 0, 1, 0, 1, 1],
      [1, 0, 0, 0, 0, 0],
      [1, 1, 0, 1, 1, 1],
      [0, 0, 0, 0, 0, 0],
      [0, 0, 0, 1, 1, 0]]

maze  = listArray ((1,1), (6, 6)) . concat

solveMaze (maze mz) (1,1) (6, 6)
\end{lstlisting}

As we mentioned, this is a style of 'exhaustive search'. It recursively searches all the connected
points as candidates. In a real maze solving game, a robot mouse competition for instance,
it's enough to just find a route. We can adapt to a method close to what described at the
beginning of this section. The robot mouse always tries the first connected point, and skip
the others until it gets stuck. We need some data structure to
store the 'bread crumbs', which help to remember the decisions being made. As we always attempt
to find the way on top of the latest decision, it is the last-in, first-out manner.
A stack can be used to realize it.

At the very beginning, only the starting point $s$ is stored in the stack. we pop it out,
find, for example, points $a$, and $b$, are connected to $s$. We push the two possible
paths: $\{a, s\}$ and $\{b, s\}$ to the stack. Next we pop $\{a, s\}$ out, and examine
connected points to $a$. Then all the paths with 3 steps will be pushed back. We
repeat this process. At anytime, each element stored in the stack is a path, from the
starting point to the farthest place can arrive in the reversed order.
This can be illustrated in figure \ref{fig:dfs-stack}.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/dfs-stack.ps}
 \caption{The stack is initialized with a singleton list of the starting point $s$. $s$ is connected with point $a$ and $b$. Paths $\{a, s\}$ and $\{b, s\}$ are pushed back. In some step, the path ended with point $p$ is popped.
$p$ is connected with points $i$, $j$, and $k$. These 3 points are expanded as different options and pushed
back to the stack. The candidate path ended with $q$ won't be examined unless all the options above fail.}
 \label{fig:dfs-stack}
\end{figure}

The stack can be realized with a list. The latest option is picked from the head, and the new
candidates are also added to the head.
The maze puzzle can be solved by using such a list of paths:

\be
solveMaze'(m, s, e) = reverse(solve'(\{\{s\}\}))
\ee

As we are searching the first, but not all the solutions, $map$ isn't used here. When the stack is empty, it
means that we've tried all the options and failed to find a way out. There is no solution; otherwise,
the top option is popped, expanded with all the adjacent points which haven't been visited before, and pushed
back to the stack. Denote the stack as $S$, if it isn't empty, the top element is $s_1$, and the new stack
after the top being popped as $S'$. $s_1$ is a list of points represents path $P$.
Denote
the first point in this path as $p_1$, and the rest as $P'$. The solution can be formalized as
the following.

\be
solve'(S) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & S = \Phi \\
  s_1 & s_1 = e \\
  solve'(S') & C = \{ c | c \in adj(p_1), c \not\in P' \} = \Phi \\
  solve'(\{ \{p\}\cup P | p \in C\} \cup S) & otherwise, C \neq \Phi
  \end{array}
\right.
\ee

Where the $adj$ function is defined above. This updated maze solution can be implemented with the
below example Haskell program \footnote{The same code of $adjacent$ function is skipped}.

\lstset{language=Haskell}
\begin{lstlisting}
dfsSolve m from to = reverse $ solve [[from]] where
    solve [] = []
    solve (c@(p:path):cs)
        | p == to = c -- stop at the first solution
        | otherwise = let os = filter (`notElem` path) (adjacent p) in
                          if os == []
                          then solve cs
                          else solve ((map (:c) os) ++ cs)
\end{lstlisting} %$

It's quite easy to modify this algorithm to find all solutions.
When we find a path in the second clause, instead
of returning it immediately, we record it and go on checking the rest memorized
options in the stack till until the stack becomes empty.
We left it as an exercise to the reader.

The same idea can also be realized imperatively. We maintain a stack to store all possible paths
from the starting point. In each iteration, the top option path is popped,
if the farthest position is the end point, a solution is found; otherwise, all the adjacent, not
visited yet points are appended as new paths and pushed back to the stack.
This is repeated
till all the candidate paths in the stacks are checked.

We use the same notation to represent the stack $S$. But the paths will be stored as arrays
instead of list in imperative settings as the former is more effective. Because of this
the starting point is the first element in the path array, while the farthest reached
place is the right most element. We use $p_n$ to represent \textproc{Last}($P$) for
path $P$. The imperative algorithm can be given as below.

\begin{algorithmic}[1]
\Function{Solve-Maze}{$m, s, e$}
  \State $S \gets \Phi$
  \State \Call{Push}{$S, \{ s \}$}
  \State $L \gets \Phi$ \Comment{the result list}
  \While{$S \neq \Phi$}
    \State $P \gets$ \Call{Pop}{$S$}
    \If{$e = p_n$}
      \State \Call{Add}{$L, P$}
    \Else
      \For{$\forall p \in $ \Call{Adjacent}{$m, p_n$}}
        \If{$p \notin P$}
          \State \Call{Push}{$S, P \cup \{ p \}$}
        \EndIf
      \EndFor
    \EndIf
  \EndWhile
  \State \Return $L$
\EndFunction
\end{algorithmic}

The following example Python program implements this maze solving algorithm.

\lstset{language=Python}
\begin{lstlisting}
def solve(m, src, dst):
    stack = [[src]]
    s = []
    while stack != []:
        path = stack.pop()
        if path[-1] == dst:
            s.append(path)
        else:
            for p in adjacent(m, path[-1]):
                if not p in path:
                    stack.append(path + [p])
    return s

def adjacent(m, p):
    (x, y) = p
    ds = [(0, 1), (0, -1), (1, 0), (-1, 0)]
    ps = []
    for (dx, dy) in ds:
        x1 = x + dx
        y1 = y + dy
        if 0 <= x1 and x1 < len(m[0]) and
           0 <= y1 and y1 < len(m) and m[y][x] == 0:
            ps.append((x1, y1))
    return ps
\end{lstlisting}

And the same maze example given above can be solved by this program like the following.

\lstset{language=Python}
\begin{lstlisting}
mz = [[0, 0, 1, 0, 1, 1],
      [1, 0, 1, 0, 1, 1],
      [1, 0, 0, 0, 0, 0],
      [1, 1, 0, 1, 1, 1],
      [0, 0, 0, 0, 0, 0],
      [0, 0, 0, 1, 1, 0]]

solve(mz, (0, 0), (5,5))
\end{lstlisting}

It seems that in the worst case, there are 4 options (up, down, left, and right) at
each step, each option is pushed to the stack and eventually examined during backtracking.
Thus the complexity is bound to $O(4^n)$. The actual time won't be so large
because we filtered out the places which have been visited before.
In the worst case, all the reachable
points are visited exactly once. So the time is bound to $O(n)$, where $n$ is the
number of points connected in total. As a stack is used to store candidate solutions,
the space complexity is $O(n^2)$.

\subsubsection{Eight queens puzzle}
\index{8 queens puzzle}
The eight queens puzzle is also a famous problem. Although cheese has very long history, this
puzzle was first published in 1848 by Max Bezzel\cite{wiki-8-queens}.
Queen in the cheese game is quite powerful.
It can attack any other pieces in the same row, column and diagonal at any distance.
The puzzle is to find a solution to put 8 queens in the board, so that none of
them attack each other. Figure \ref{fig:8-queens-puzzle} (a) illustrates the places can
be attacked by a queen and \ref{fig:8-queens-puzzle} (b) shows a solution of 8 queens puzzle.

\begin{figure}[htbp]
 \centering
 \subfloat[A queen piece.]{\includegraphics[scale=1]{img/queen.eps}}
 \subfloat[An example solution]{\includegraphics[scale=1]{img/queens-example.eps}}
 \caption{The eight queens puzzle.}
 \label{fig:8-queens-puzzle}
\end{figure}

It's obviously that the puzzle can be solved by brute-force, which takes $P^8_{64}$ times.
This number is about $4 \times 10^{10}$. It can be easily improved by observing that,
no two queens can be in the same row, and each queen must be put on one column between
1 to 8. Thus we can represent the arrangement as a permutation of $\{1,2,3,4,5,6,7,8\}$.
For instance, the arrangement $\{6,2,7,1,3,5,8,4\}$ means, we put the first queen at row 1,
column 6, the second queen at row 2 column 2, ..., and the last queen at row 8, column 4.
By this means, we need only examine $8! = 40320$ possibilities.

We can find better solutions than this. Similar to the maze puzzle, we put queens one by
one from the first row. For the first queen, there are 8 options, that we can put it
at one of the eight columns. Then for the next queen, we again examine the 8 candidate
columns. Some of them are not valid because those positions will be attacked by the first
queen. We repeat this process, for the $i$-th queen, we examine the 8 columns
in row $i$, find which columns are safe. If none column is valid, it means all the
columns in this row will be attacked by some queen we've previously arranged, we have
to backtrack as what we did in the maze puzzle. When all the 8 queens are successfully
put to the board, we find a solution. In order to find all the possible solutions, we
need record it and go on to examine other candidate columns and perform back tracking if
necessary. This process terminates when all the columns in the first row have been
examined. The below equation starts the search.

\be
solve(\{\Phi\}, \Phi)
\ee

In order to manage the candidate attempts, a stack $S$ is used as same as in the maze
puzzle. The stack is initialized with one empty element.
And a list $L$ is used to record all possible solutions.
Denote the top element in the stack as $s_1$. It's actually an intermediate state of
assignment, which is a partial permutation of 1 to 8.
after pops $s_1$, the stack becomes $S'$. The $solve$ function can be defined as the following.

\be
solve(S, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L & S = \Phi \\
  solve(S', \{s_1\} \cup L) & |s_1| = 8 \\
  solve(\left \{
      \begin{array}{rl}
        \{i\} \cup s_1 | & i \in [1,8], \\
                         & i \notin s_1, \\
                         & safe(i, s_1)
      \end{array}
      \right \} \cup S', L) & otherwise
  \end{array}
\right.
\ee

If the stack is empty, all the possible candidates have been examined, it's not possible
to backtrack any more. $L$ has been accumulated all found solutions and returned as the result;
Otherwise, if the length of the top element in the stack is 8,
a valid solution is found. We add it to $L$, and go on finding other solutions; If
the length is less than 8, we need try to put the next queen.
Among all the columns from 1 to 8, we
pick those not already occupied by previous queens (through the $i \notin s_1$ clause), and
must not be attacked in diagonal direction (through the $safe$ predication). The valid
assignments will be pushed to the stack for the further searching.

Function $safe(x, C)$ detects if the assignment of a queen in position $x$ will be attacked
by other queens in $C$ in diagonal direction. There are 2 possible cases,
$45^{\circ}$ and $135^{\circ}$ directions. Since the row of this new
queen is $y = 1 + |C|$, where $|C|$ is the length of $C$, the $safe$ function can be
defined as the following.

\be
safe(x, C) = \forall (c, r) \in zip(reverse(C), \{1, 2, ...\}), |x - c| \neq |y - r|
\ee

Where $zip$ takes two lists, and pairs every elements in them to a new list. Thus
If $C = \{ c_{i-1}, c_{i-2}, ..., c_2, c_1\}$ represents the column of the first $i-1$
queens has been assigned, the above function will check list of pairs
$\{(c_1, 1), (c_2, 2), ..., (c_{i-1}, i-1)\}$ with position $(x, y)$ forms any
diagonal lines.

Translating this algorithm into Haskell gives the below example program.

\lstset{language=Haskell}
\begin{lstlisting}
solve = dfsSolve [[]] [] where
    dfsSolve [] s = s
    dfsSolve (c:cs) s
             | length c == 8 = dfsSolve cs (c:s)
             | otherwise = dfsSolve ([(x:c) | x <- [1..8] \\ c,
                               not $ attack x c] ++ cs) s
    attack x cs = let y = 1 + length cs in
                  any (\(c, r) -> abs(x - c) == abs(y - r)) $
                      zip (reverse cs) [1..]
\end{lstlisting}

Observing that the algorithm is tail recursive, it's easy to transform it into
imperative realization. Instead of using list, we use array to represent queens assignment.
Denote the stack as $S$, and the possible solutions as $A$. The imperative
algorithm can be described as the following.

\begin{algorithmic}[1]
\Function{Solve-Queens}{}
  \State $S \gets \{\Phi\}$
  \State $L \gets \Phi$ \Comment{The result list}
  \While{$S \neq \Phi$}
    \State $A \gets$ \Call{Pop}{$S$} \Comment{$A$ is an intermediate assignment}
    \If{$|A|=8$}
      \State \Call{Add}{$L, A$}
    \Else
      \For{$i \gets 1$ to $8$}
        \If{\Call{Valid}{$i, A$}}
          \State \Call{Push}{$S, A \cup \{i\}$}
        \EndIf
      \EndFor
    \EndIf
  \EndWhile
  \State \Return $L$
\EndFunction
\end{algorithmic}

The stack is initialized with the empty assignment. The main process repeatedly
pops the top candidate from the stack. If there are still queens left, the algorithm
examines possible columns in the next row from 1 to 8. If a column is safe, that
it won't be attacked by any previous queens, this column will be appended
to the assignment, and pushed back to the stack. Different from the functional
approach, since array, but not list, is used, we needn't reverse the
solution assignment any more.

Function \textproc{Valid} checks if column $x$ is safe with previous queens put
in $A$. It filters out the columns have already been occupied, and calculates
if any diagonal lines are formed with existing queens.

\begin{algorithmic}[1]
\Function{Valid}{$x, A$}
  \State $y \gets 1 + |A|$
  \For{$i \gets 1$ to $|A|$}
    \If{$x = i \lor |y-i| = |x - A[i]|$}
      \State \Return False
    \EndIf
  \EndFor
  \State \Return True
\EndFunction
\end{algorithmic}

The following Python example program implements this imperative algorithm.

\lstset{language=Python}
\begin{lstlisting}
def solve():
    stack = [[]]
    s = []
    while stack != []:
        a = stack.pop()
        if len(a) == 8:
            s.append(a)
        else:
            for i in range(1, 9):
                if valid(i, a):
                    stack.append(a+[i])
    return s

def valid(x, a):
    y = len(a) + 1
    for i in range(1, y):
        if x == a[i-1] or abs(y - i) == abs(x - a[i-1]):
            return False
    return True
\end{lstlisting}

Although there are 8 optional columns for each queen, not all of them
are valid and thus further expanded. Only those columns haven't been occupied by
previous queens are tried. The algorithm only examines 15720, which is far
less than $8^8 = 16777216$, possibilities \cite{wiki-8-queens}.

It's quite easy to extend the algorithm, so that it can solve $N$ queens puzzle, where
$N \geq 4$. However, the time cost increases fast. The backtrack algorithm is just
slightly better than the one permuting the sequence of 1 to 8 (which is bound to $o(N!)$).
Another extension to the algorithm is based on the fact that the chess board is
square, which is symmetric both vertically and horizontally. Thus a solution can
generate other solutions by rotating and flipping. These aspects are left as
exercises to the reader.

\subsubsection{Peg puzzle}
\index{Peg puzzle}
I once received a puzzle of the leap frogs. It said to be homework
for 2nd grade student in China. As illustrated in figure \ref{fig:leapfrog}, there
are 6 frogs in 7 stones. Each frog can either hop to the next stone if it is not
occupied, or leap over one frog to another empty stone. The frogs on the left side
can only move to the right, while the ones on the right side can only move to the
left. These rules are described in figure \ref{fig:pegrules}

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.4]{img/leapfrogs.eps}
 \caption{The leap frogs puzzle.}
 \label{fig:leapfrog}
\end{figure}

The goal of this puzzle is to arrange the frogs to jump according to the rules, so
that the positions of the 3 frogs on the left are finally exchange with the ones on the right.
If we denote the frog on the left as 'A', on the right as 'B', and the empty stone as 'O'. The
puzzle is to find a solution to transform from 'AAAOBBB' to 'BBBOAAA'.

\begin{figure}[htbp]
 \centering
 \subfloat[Jump to the next stone]{\includegraphics[scale=0.3]{img/leapfrog1.eps}} \hspace{0.02\textwidth}
 \subfloat[Jump over to the right]{\includegraphics[scale=0.3]{img/leapfrog2.eps}} \hspace{0.02\textwidth}
 \subfloat[Jump over to the left]{\includegraphics[scale=0.3]{img/leapfrog3.eps}}
 \caption{Moving rules.}
 \label{fig:pegrules}
\end{figure}

This puzzle is just a special form of the peg puzzles. The number of pegs is not
limited to 6. it can be 8 or other bigger even numbers. Figure \ref{fig:pegpuzzles}
shows some variants.

\begin{figure}[htbp]
 \centering
 \subfloat[Solitaire]{\includegraphics[scale=0.5]{img/solitaire.eps}} \hspace{0.02\textwidth}
 \subfloat[Hop over]{\includegraphics[scale=0.7]{img/hop-over.eps}} \hspace{0.02\textwidth}
 \subfloat[Draught board]{\includegraphics[scale=0.3]{img/draught-board.eps}}
 \caption{Variants of the peg puzzles from http://home.comcast.net/~stegmann/jumping.htm}
 \label{fig:pegpuzzles}
\end{figure}

We can solve this puzzle by programing. The idea is similar to the 8 queens puzzle.
Denote the positions from the left most stone as 1, 2, ..., 7.
In ideal cases, there are 4 options to arrange the move. For example when start, the frog
on 3rd stone can hop right to the empty stone; symmetrically, the frog on the 5th stone
can hop left; Alternatively, the frog on the 2nd stone can leap right, while the frog on
the 6th stone can leap left.

We can record the state and try one of these 4 options at every step. Of course not all of them
are possible at any time. If get stuck, we can backtrack and try other options.

As we restrict the left side frogs only moving to the right, and the right frogs only
moving to the left. The moves are not reversible. There won't be any repetition cases as
what we have to deal with in the maze puzzle. However, we still need record the steps
in order to print them out finally.

In order to enforce these restriction, let A, O, B in representation 'AAAOBBB' be -1, 0,
and 1 respectively. A state $L$ is a list of elements, each element is one of these 3
values. It starts from $\{-1, -1, -1, 0, 1, 1, 1\}$. $L[i]$ access the $i$-th element,
its value indicates if the $i$-th stone is empty, occupied by a frog from left side, or
occupied by a frog from right side. Denote the position of the vacant stone as $p$.
The 4 moving options can be stated as below.

\begin{itemize}
\item Leap left: $p < 6$ and $L[p+2] > 0$, swap $L[p] \leftrightarrow L[p+2]$;
\item Hop left: $p < 7$ and $L[p+1] > 0$, swap $L[p] \leftrightarrow L[p+1]$;
\item Leap right: $p > 2$ and $L[p-2] < 0$, swap $L[p-2] \leftrightarrow L[p]$;
\item Hop right: $p > 1$ and $L[p-1] < 0$, swap $L[p-1] \leftrightarrow L[p]$.
\end{itemize}

Four functions $leap_l(L), hop_l(L), leap_r(L)$ and $hop_r(L)$ are defined accordingly.
If the state $L$ does not satisfy the move restriction, these function return $L$
unchanged, otherwise, the changed state $L'$ is returned accordingly.

We can also explicitly maintain a stack $S$ to the attempts as
well as the historic movements. The stack is initialized with a singleton list
of starting state. The solution is accumulated to a list $M$, which is empty at the beginning:

\be
solve(\{\{-1, -1, -1, 0, 1, 1, 1\}\}, \Phi)
\ee

As far as the stack isn't empty, we pop one intermediate attempt. If
the latest state is equal to $\{1, 1, 1, 0, -1, -1, -1\}$, a solution is found.
We append the series of moves till this state to the result list $M$; otherwise,
We expand to next possible state by trying all four possible moves, and push
them back to the stack for further search. Denote the top element in the stack
$S$ as $s_1$, and the latest state in $s_1$ as $L$. The algorithm can be
defined as the following.

\be
solve(S, M) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  M & S = \Phi \\
  solve(S', \{reverse(s_1)\} \cup M) & L = \{1, 1, 1, 0, -1, -1, -1\} \\
  solve(P \cup S', M) & otherwise
  \end{array}
\right.
\ee

Where $P$ are possible moves from the latest state $L$:
\[
P = \{ L'  | L' \in \{leap_l(L), hop_l(L), leap_r(L), hop_r(L)\}, L \neq L'\}
\]

Note that the starting state is stored as the last element, while the
final state is the first. That is the reason why we reverse it when adding
to solution list.

Translating this algorithm to Haskell gives the following example program.

\lstset{language=Haskell}
\begin{lstlisting}
solve = dfsSolve [[[-1, -1, -1, 0, 1, 1, 1]]] [] where
    dfsSolve [] s = s
    dfsSolve (c:cs) s
             | head c == [1, 1, 1, 0, -1, -1, -1] = dfsSolve cs (reverse c:s)
             | otherwise = dfsSolve ((map (:c) $ moves $ head c) ++ cs) s

moves s = filter (/=s) [leapLeft s, hopLeft s, leapRight s, hopRight s] where
    leapLeft [] = []
    leapLeft (0:y:1:ys) = 1:y:0:ys
    leapLeft (y:ys) = y:leapLeft ys
    hopLeft [] = []
    hopLeft (0:1:ys) = 1:0:ys
    hopLeft (y:ys) = y:hopLeft ys
    leapRight [] = []
    leapRight (-1:y:0:ys) = 0:y:(-1):ys
    leapRight (y:ys) = y:leapRight ys
    hopRight [] = []
    hopRight (-1:0:ys) = 0:(-1):ys
    hopRight (y:ys) = y:hopRight ys
\end{lstlisting}

Running this program finds 2 symmetric solutions, each takes 15 steps. One solution
is list in the below table.

\begin{tabular}{|c||c|c|c|c|c|c|c|}
\hline
step & -1 & -1 & -1 & 0 & 1 & 1 & 1 \\
\hline
1 & -1 & -1 & 0 & -1 & 1 & 1 & 1 \\
2 & -1 & -1 & 1 & -1 & 0 & 1 & 1 \\
3 & -1 & -1 & 1 & -1 & 1 & 0 & 1 \\
4 & -1 & -1 & 1 & 0 & 1 & -1 & 1 \\
5 & -1 & 0 & 1 & -1 & 1 & -1 & 1 \\
6 & 0 & -1 & 1 & -1 & 1 & -1 & 1 \\
7 & 1 & -1 & 0 & -1 & 1 & -1 & 1 \\
8 & 1 & -1 & 1 & -1 & 0 & -1 & 1 \\
9 & 1 & -1 & 1 & -1 & 1 & -1 & 0 \\
10 & 1 & -1 & 1 & -1 & 1 & 0 & -1 \\
11 & 1 & -1 & 1 & 0 & 1 & -1 & -1 \\
12 & 1 & 0 & 1 & -1 & 1 & -1 & -1 \\
13 & 1 & 1 & 0 & -1 & 1 & -1 & -1 \\
14 & 1 & 1 & 1 & -1 & 0 & -1 & -1 \\
15 & 1 & 1 & 1 & 0 & -1 & -1 & -1 \\
\hline
\end{tabular}

Observe that the algorithm is in tail recursive manner, it can also be realized imperatively.
The algorithm can be more generalized, so that it
solve the puzzles of $n$ frogs on each side. We represent the start
state \{-1, -1, ..., -1, 0, 1, 1, ..., 1\} as $s$, and the mirrored end state as $e$.

\begin{algorithmic}[1]
\Function{Solve}{$s, e$}
  \State $S \gets \{\{s\}\}$
  \State $M \gets \Phi$
  \While{$S \neq \Phi$}
    \State $s_1 \gets$ \Call{Pop}{$S$}
    \If{$s_1[1] = e$}
      \State \textproc{Add}($M$, \Call{Reverse}{$s_1$})
    \Else
      \For{$\forall m \in$ \Call{Moves}{$s_1[1]$}}
        \State \textproc{Push}($S$, $\{m\} \cup s_1$)
      \EndFor
    \EndIf
  \EndWhile
  \State \Return $M$
\EndFunction
\end{algorithmic}

The possible moves can be also generalized with procedure \textproc{Moves} to handle
arbitrary number of frogs. The following
Python program implements this solution.

\lstset{language=Python}
\begin{lstlisting}
def solve(start, end):
    stack = [[start]]
    s = []
    while stack != []:
        c = stack.pop()
        if c[0] == end:
            s.append(reversed(c))
        else:
            for m in moves(c[0]):
                stack.append([m]+c)
    return s

def moves(s):
    ms = []
    n = len(s)
    p = s.index(0)
    if p < n - 2 and s[p+2] > 0:
        ms.append(swap(s, p, p+2))
    if p < n - 1 and s[p+1] > 0:
        ms.append(swap(s, p, p+1))
    if p > 1 and s[p-2] < 0:
        ms.append(swap(s, p, p-2))
    if p > 0 and s[p-1] < 0:
        ms.append(swap(s, p, p-1))
    return ms

def swap(s, i, j):
    a = s[:]
    (a[i], a[j]) = (a[j], a[i])
    return a
\end{lstlisting}

For 3 frogs in each side, we know that it takes 15 steps to exchange them.
It's interesting to examine the table that how many steps are needed
along with the number of frogs in each side. Our program
gives the following result.

\begin{tabular}{c|c|c|c|c|c|c}
number of frogs & 1 & 2 & 3  & 4  & 5 & ... \\
\hline
number of steps & 3 & 8 & 15 & 24 & 35 & ...
\end{tabular}

It seems that the number of steps are all square numbers minus one.
It's natural to guess that the number of steps for $n$ frogs in one side is
$(n+1)^2 - 1$. Actually we can prove it is true.

Compare to the final state and the start state, each frog moves ahead $n+1$
stones in its opposite direction. Thus total $2n$ frogs move $2n(n+1)$ stones.
Another important fact is that each frog on the left has to meet every one
on the right one time. And leap will happen when meets. Since the frog moves two
stones ahead by leap, and there are total $n^2$ meets happen, so that all these
meets cause moving $2n^2$ stones ahead. The rest moves are not leap, but hop.
The number of hops are $2n(n+1) - 2n^2 = 2n$. Sum up all $n^2$ leaps and $2n$
hops, the total number of steps are $n^2 + 2n = (n+1)^2 -1$.

\subsubsection{Summary of DFS}
Observe the above three puzzles, although they vary in many aspects, their
solutions show quite similar common structures. They all have some starting
state. The maze starts from the entrance point; The 8 queens puzzle starts
from the empty board; The leap frogs start from the state of 'AAAOBBB'.
The solution is a kind of searching, at each attempt, there are several
possible ways. For the maze puzzle, there are four different directions to try;
For the 8 queens puzzle, there are eight columns to choose; For the leap
frogs puzzle, there are four movements of leap or hop. We don't know how
far we can go when make a decision, although the final state is clear.
For the maze, it's the exit point; For the 8 queens puzzle, we are done
when all the 8 queens being assigned on the board; For the leap frogs
puzzle, the final state is that all frogs exchanged.

We use a common approach to solve them. We repeatedly select one possible
candidate to try, record where we've achieved; If we get stuck, we backtrack
and try other options. We are sure by using this strategy, we can either
find a solution, or tell that the problem is unsolvable.

Of course there can be some variation, that we can stop when find one answer,
or go on searching all the solutions.

If we draw a tree rooted at the starting state, expand it so that every
branch stands for a different attempt, our searching process
is in a manner, that it searches deeper and deeper. We won't consider any
other options in the same depth unless the searching fails so that we've
to backtrack to upper level of the tree. Figure \ref{fig:dfs-tree} illustrates
the order we search a state tree. The arrow indicates how we go down
and backtrack up. The number of the nodes shows the order we visit them.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/dfs-tree.eps}
 \caption{Example of DFS search order.}
 \label{fig:dfs-tree}
\end{figure}

This kind of search strategy is called 'DFS' (Deep-first-search). We widely
use it unintentionally. Some programming environments, Prolog for instance,
adopt DFS as the default evaluation model. A maze is given by a set
of rule base, such as:

\lstset{language=Prolog}
\begin{lstlisting}
c(a, b). c(a, e).
c(b, c). c(b, f).
c(e, d), c(e, f).
c(f, c).
c(g, d). c(g, h).
c(h, f).
\end{lstlisting}

Where predicate $c(X, Y)$ means place $X$ is connected with $Y$. Note that
this is a directed predicate, we can make $Y$ to be connected with $X$ as well
by either adding a symmetric rule, or create a undirected predicate. Figure
\ref{fig:directed-graph} shows such a directed graph. Given two places $X$ and $Y$, Prolog can
tell if they are connected by the following program.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/directed-graph.ps}
 \caption{A directed graph.}
 \label{fig:directed-graph}
\end{figure}

\lstset{language=Prolog}
\begin{lstlisting}
go(X, X).
go(X, Y) :- c(X, Z), go(Z, Y)
\end{lstlisting}

This program says that, a place is connected with itself. Given two different places $X$ and
$Y$, if $X$ is connected with $Z$, and $Z$ is connected with $Y$, then $X$ is connected with $Y$.
Note that, there might be multiple choices for $Z$. Prolog selects a candidate, and go on further
searching. It only tries other candidates if the recursive searching fails. In that case, Prolog
backtracks and tries other alternatives. This is exactly what DFS does.

DFS is quite straightforward when we only need a solution, but don't care if the solution takes
the fewest steps. For example, the solution it gives, may not be the shortest path for the maze. We'll
see some more puzzles next. They demands to find the solution with the minimum attempts.


\subsubsection{The wolf, goat, and cabbage puzzle}
\index{The wolf, goat, and cabbage puzzle}
This puzzle says that a farmer wants to cross a river with a wolf, a goat, and a bucket of cabbage.
There is a boat. Only the farmer can drive it. But the boat is small. it can only hold one of the
wolf, the goat, and the bucket of cabbage with the farmer at a time. The farmer has to pick
them one by one to the other side of the river. However, the wolf would eat the goat, and the
goat would eat the cabbage if the farmer is absent. The puzzle asks to find the fast solution
so that they can all safely go cross the river.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/wgc-puzzle.eps}
 \caption{The wolf, goat, cabbage puzzle}
 \label{fig:wgc-puzzle}
\end{figure}

The key point to this puzzle is that the wolf does not eat the cabbage. The farmer can safely
pick the goat to the other side. But next time, no matter if he pick the wolf or the cabbage
to cross the river, he has to take one back to avoid the conflict. In order to find the
fast the solution, at any time, if the farmer has multiple options, we can examine all of them
in parallel, so that these different decisions compete. If we count the number of the
times the farmer cross the river without considering the direction, that crossing the river
back and forth means 2 times, we are actually checking the complete possibilities after 1 time,
2 times, 3 times, ... When we find a situation, that they all arrive at the other bank,
we are done. And this solution wins the competition, which is the fast solution.

The problem is that we can't examine all the possible solutions in parallel ideally.
Even with a super computer equipped with many CPU cores, the setup is too expensive to
solve such a simple puzzle.

Let's consider a lucky draw game. People blindly pick from a box with colored balls. There
is only one black ball, all the others are white. The one who pick the black ball wins the
game; Otherwise, he must return the ball to the box and wait for the next chance.
In order to be fair enough, we can setup a rule that no one can try the second time before
all others have tried. We can line people to a queue. Every time the first guy
pick a ball, if he does not win, he then stands at the tail of the queue to wait for the
second try. This queue helps to ensure our rule.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=1]{img/lucky-draw.eps}
 \caption{A lucky-draw game, the $i$-th person goes from the queue, pick a ball, then join the queue at tail if he fails to pick the black ball.}
 \label{fig:luck-draw}
\end{figure}

We can use the quite same idea to solve our puzzle. The two banks of the river can be
represented as two sets $A$ and $B$. $A$ contains the wolf, the goat, the cabbage and
the farmer; while $B$ is empty. We take an element from one set to the other
each time. The two sets can't hold conflict things if the farmer is absent. The goal is
to exchange the contents of $A$ and $B$ with fewest steps.

We initialize a queue with state $A = \{w, g, c, p\}, B=\Phi$ as the only element.
As far as the queue isn't empty, we pick the first element from the head, expand it
with all possible options, and put these new expanded candidates to the tail of the queue.
If the first element on the head is the final goal, that $A=\Phi, B=\{w, g, c, p\}$,
we are done. Figure \ref{fig:bfs-tree} illustrates the idea of this search order.
Note that as all possibilities in the same level are examined, there is no need for
back-tracking.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/bfs-tree.eps}
 \caption{Start from state 1, check all possible options 2, 3, and 4 for next step; then all nodes in level 3, ...}
 \label{fig:bfs-tree}
\end{figure}

There is a simple way to treat the set. A four bits binary number can be used, each
bit stands for a thing, for example, the wolf $w=1$, the goat $g=2$, the cabbage
$c=4$, and the farmer $p=8$. That 0 stands for the empty set, 15 stands for a
full set. Value 3, solely means there are a wolf and a goat on the river bank. In
this case, the wolf will eat the goat. Similarly, value 6 stands for another
conflicting case. Every time, we move the highest bit (which is 8), or together
with one of the other bits (4 or 2, or 1) from one number to the other. The
possible moves can be defined as below.

\be
mv(A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{(A - 8 - i, B + 8 + i) | i \in \{0, 1, 2, 4\}, i = 0 \lor A \overline{\land} i \neq 0 \} & B < 8 \\
  \{(A + 8 + i, B - 8 - i) | i \in \{0, 1, 2, 4\}, i = 0 \lor B \overline{\land} i \neq 0 \} & Otherwise
  \end{array}
\right.
\ee

Where $\overline{\land}$ is the bitwise-and operation.

the solution can be given by reusing the queue defined in previous chapter. Denote
the queue as $Q$, which is initialed with a singleton list \{(15, 0)\}. If $Q$ is
not empty, function $DeQ(Q)$ extracts the head element $M$, the updated queue
becomes $Q'$. $M$ is a list of pairs, stands for a series of movements between the
river banks. The first element in $m_1=(A', B')$ is the latest state. Function
$EnQ'(Q, L)$ is a slightly different enqueue operation.
It pushes all the possible moving sequences in $L$ to the tail of the
queue one by one and returns the updated queue.
With these notations, the solution function is defined like below.

\be
solve(Q) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & Q = \Phi \\
  reverse(M) & A' = 0 \\
  solve(EnQ'(Q', \left \{
    \begin{array}{rl}
      \{m\} \cup M | & m \in mv(m_1), \\
                     & valid(m, M)
    \end{array}
    \right \})) & otherwise
  \end{array}
\right.
\ee

Where function $valid(m, M)$ checks if the new moving candidate $m=(A'', B'')$ is valid.
That neither $A''$ nor $B''$ is 3 or 6, and $m$ hasn't been tried before in $M$
to avoid any repeatedly attempts.

\be
valid(m, M) = A'' \neq 3, A'' \neq 6, B'' \neq 3, B'' \neq 6, m \notin M
\ee

The following example Haskell program implements this solution. Note that it
uses a plain list to represent the queue for illustration purpose.

\lstset{language=Haskell}
\begin{lstlisting}
import Data.Bits

solve = bfsSolve [[(15, 0)]] where
    bfsSolve :: [[(Int, Int)]] -> [(Int, Int)]
    bfsSolve [] = [] -- no solution
    bfsSolve (c:cs) | (fst $ head c) == 0 = reverse c
                    | otherwise = bfsSolve (cs ++ map (:c)
                                      (filter (`valid` c) $ moves $ head c))
    valid (a, b) r = not $ or [ a `elem` [3, 6], b `elem` [3, 6],
                                (a, b) `elem` r]

moves (a, b) = if b < 8 then trans a b else map swap (trans b a) where
    trans x y = [(x - 8 - i, y + 8 + i)
                     | i <-[0, 1, 2, 4], i == 0 || (x .&. i) /= 0]
    swap (x, y) = (y, x)
\end{lstlisting}

This algorithm can be easily modified to find all the possible solutions, but
not just stop after finding the first one. This is left as the exercise to the
reader. The following shows the two best solutions to this puzzle.

Solution 1:

\begin{tabular}{l|c|l}
Left & river & Right \\
\hline
wolf, goat, cabbage, farmer &   & \\
wolf, cabbage &   & goat, farmer \\
wolf, cabbage, farmer &   & goat \\
cabbage &   & wolf, goat, farmer \\
goat, cabbage, farmer &   & wolf \\
goat &   & wolf, cabbage, farmer \\
goat, farmer &   & wolf, cabbage \\
 &  & wolf, goat, cabbage, farmer
\end{tabular}

Solution 2:

\begin{tabular}{l|c|l}
Left & river & Right \\
\hline
 wolf, goat, cabbage, farmer & & \\
 wolf, cabbage & & goat, farmer \\
 wolf, cabbage, farmer & & goat \\
 wolf & & goat, cabbage, farmer \\
 wolf, goat, farmer & & cabbage \\
 goat & & wolf, cabbage, farmer \\
 goat, farmer & & wolf, cabbage \\
 & & wolf, goat, cabbage, farmer
\end{tabular}

This algorithm can also be realized imperatively. Observing that our
solution is in tail recursive manner, we can translate it directly
to a loop. We use a list $S$ to hold all the solutions can be found.
The singleton list $\{(15, 0)\}$ is pushed to queue when initializing.
As long as the queue isn't empty, we extract the head $C$ from the
queue by calling \textproc{DeQ} procedure. Examine if it reaches the
final goal, if not, we expand all the possible moves and push to the
tail of the queue for further searching.

\begin{algorithmic}[1]
\Function{Solve}{}
  \State $S \gets \Phi$
  \State $Q \gets \Phi$
  \State \Call{EnQ}{$Q, \{(15, 0)\}$}
  \While{$Q \neq \Phi$}
    \State $C \gets $ \Call{DeQ}{$Q$}
    \If{$c_1 = (0, 15)$}
      \State \textproc{Add}($S$, \Call{Reverse}{$C$})
    \Else
      \For{$\forall m \in $ \Call{Moves}{$C$}}
        \If{\Call{Valid}{$m, C$}}
          \State \Call{EnQ}{$Q, \{m\} \cup C$}
        \EndIf
      \EndFor
    \EndIf
  \EndWhile
  \State \Return $S$
\EndFunction
\end{algorithmic}

Where \textproc{Moves}, and \textproc{Valid} procedures are as
same as before. The following Python example program implements
this imperative algorithm.

\lstset{language=Python}
\begin{lstlisting}
def solve():
    s = []
    queue = [[(0xf, 0)]]
    while queue != []:
        cur = queue.pop(0)
        if cur[0] == (0, 0xf):
            s.append(reverse(cur))
        else:
            for m in moves(cur):
                queue.append([m]+cur)
    return s

def moves(s):
    (a, b) = s[0]
    return valid(s, trans(a, b) if b < 8 else swaps(trans(b, a)))

def valid(s, mv):
    return [(a, b) for (a, b) in mv
        if a not in [3, 6] and b not in [3, 6] and (a, b) not in s]

def trans(a, b):
    masks = [ 8 | (1<<i) for i in range(4)]
    return [(a ^ mask, b | mask) for mask in masks if a & mask == mask]

def swaps(s):
    return [(b, a) for (a, b) in s]
\end{lstlisting}

There is a minor difference between the program and the pseudo code, that the function
to generate candidate moving options filters the invalid cases inside it.

Every time, no matter the farmer drives the boat back and forth, there are
$m$ options for him to choose, where $m$ is the number of objects on the
river bank the farmer drives from. $m$ is always less than 4, that the
algorithm won't take more than $n^4$ times at step $n$. This estimation
is far more than the actual time, because we avoid trying all invalid
cases. Our solution examines all the possible moving in the worst case.
Because we check recorded steps to avoid repeated attempt, the
algorithm takes about $O(n^2)$ time to search for $n$ possible steps.

\subsubsection{Water jugs puzzle}
\index{Water jugs puzzle}
This is a popular puzzle in classic AI. The history of it should be very long.
It says that there are two jugs, one is 9 quarts, the other is 4 quarts.
How to use them to bring up from the river exactly 6 quarts of water?

There are varies versions of this puzzle, although the volume
of the jugs, and the target volume of water differ. The solver is said to
be young Blaise Pascal when he was a child, the French mathematician,
scientist in one story, and Sim\`{e}on Denis Poisson in another story.
Later in the popular Hollywood movie `Die-Hard 3', actor Bruce Willis
and Samuel L. Jackson were also confronted with this puzzle.

P\`{o}lya gave a nice way to solve this problem backwards in \cite{how-to-solve-it}.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/jugs-start.eps}
 \caption{Two jugs with volume of 9 and 4.}
 \label{fig:jugs-start}
\end{figure}

Instead of thinking from the starting state as shown in figure \ref{fig:jugs-start}.
P\`{o}lya pointed out that there will be 6 quarts of water in the bigger jugs
at the final stage, which indicates the second last step, we can fill the
9 quarts jug, then pour out 3 quarts from it. In order to achieve this, there
should be 1 quart of water left in the smaller jug as shown in figure
\ref{fig:jugs-r1}.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/jugs-r1.eps}
 \caption{The last two steps.}
 \label{fig:jugs-r1}
\end{figure}

It's easy to see that fill the 9 quarters jug, then pour to the 4 quarters
jug twice can bring 1 quarters of water. As shown in figure \ref{fig:jugs-r2}.
At this stage, we've found the solution. By reversing our findings, we
can give the correct steps to bring exactly 6 quarters of water.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/jugs-r2.eps}
 \caption{Fill the bigger jugs, and pour to the smaller one twice.}
 \label{fig:jugs-r2}
\end{figure}

P\`{o}lya's methodology is general. It's still hard to solve it without
concrete algorithm. For instance, how to bring up 2 gallons of water from
899 and 1147 gallon jugs?

There are 6 ways to deal with 2 jugs in total. Denote the smaller jug as $A$,
the bigger jug as $B$.

\begin{itemize}
\item Fill jug $A$ from the river;
\item Fill jug $B$ from the river;
\item Empty jug $A$;
\item Empty jug $B$;
\item Pour water from jug $A$ to $B$;
\item Pour water from jug $B$ to $A$.
\end{itemize}

The following sequence shows an example. Note that in this example, we
assume that $a < b < 2a$.

\begin{tabular}{l|l|l}
$A$ & $B$ & operation \\
\hline
0 & 0 & start \\
a & 0 & fill $A$ \\
0 & a & pour $A$ into $B$ \\
a & a & fill $A$ \\
2a - b & b & pour $A$ into $B$ \\
2a - b & 0 & empty $B$ \\
0 & 2a - b & pour $A$ into $B$ \\
a & 2a - b & fill $A$ \\
3a - 2b & b & pour $A$ into $B$ \\
... & ... & ... \\
\end{tabular}

No matter what the above operations are taken, the amount of water in
each jug can be expressed as $xa + yb$, where $a$ and $b$ are volumes
of jugs, for some integers $x$ and $y$. All the amounts of water we
can get are linear combination of $a$ and $b$. We can immediately
tell given two jugs, if a goal $g$ is solvable or not.

For instance, we can't bring 5 gallons of water with two jugs of volume
4 and 6 gallon. The number theory ensures that, the 2 water jugs puzzle
can be solved if and only if $g$ can be divided by the greatest common
divisor of $a$ and $b$. Written as:

\be
gcd(a, b) | g
\ee

Where $ m | n $ means $n$ can be divided by $m$. What's more, if $a$
and $b$ are relatively prime, which means $gcd(a, b) = 1$, it's possible
to bring up any quantity $g$ of water.

Although $gcd(a, b)$ enables us to determine if the puzzle is solvable, it
doesn't give us the detailed pour sequence. If we can find some integer
$x$ and $y$, so that $g = xa + yb$. We can arrange a sequence of operations
(even it may not be the best solution) to solve it. The idea is that,
without loss of generality, suppose $x > 0, y < 0$, we need fill jug $A$ by
$x$ times, and empty jug $B$ by $y$ times in total.

Let's take $a=3$, $b=5$, and $g=4$ for example, since $4 = 3 \times 3 - 5$,
we can arrange a sequence like the following.

\begin{tabular}{l|l|l}
$A$ & $B$ & operation \\
\hline
0 & 0 & start \\
3 & 0 & fill $A$ \\
0 & 3 & pour $A$ into $B$ \\
3 & 3 & fill $A$ \\
1 & 5 & pour $A$ into $B$ \\
1 & 0 & empty $B$ \\
0 & 1 & pour $A$ into $B$ \\
3 & 1 & fill $A$ \\
0 & 4 & pour $A$ into $B$ \\
\end{tabular}

In this sequence, we fill $A$ by 3 times, and empty $B$ by 1 time. The procedure
can be described as the following:

Repeat $x$ times:
\begin{enumerate}
\item Fill jug $A$;
\item Pour jug $A$ into jug $B$, whenever $B$ is full, empty it.
\end{enumerate}

So the only problem left is to find the $x$ and $y$. There is a powerful tool
in number theory called, {\em Extended Euclid algorithm}, which can achieve this.
Compare to the classic Euclid GCD algorithm, which can only give the greatest
common divisor, The extended Euclid algorithm can give a pair of $x, y$ as well,
so that:

\be
(d, x, y) = gcd_{ext}(a, b)
\ee

where $d = gcd(a, b)$ and $ax + by = d$. Without loss of generality, suppose
$a < b$, there exits quotation $q$ and remainder $r$ that:

\be
b = a q + r
\ee

Since $d$ is the common divisor, it can divide both $a$ and $b$, thus $d$ can
divide $r$ as well. Because $r$ is less than $a$, we can scale down the
problem by finding GCD of $a$ and $r$:

\be
(d, x', y') = gcd_{ext}(r, a)
\label{eq:recursive-ext-gcd}
\ee

Where $d = x' r + y' a$ according to the definition of the extended Euclid
algorithm. Transform $b = a q + r$ to $r = b - a q$, substitute $r$ in above
equation yields:

\be
\begin{array}{rcl}
d & = & x' (b - a q) + y' a \\
  & = & (y' - x' q) a + x' b
\end{array}
\ee

This is the linear combination of $a$ and $b$, so that we have:

\be
\left \{
  \begin{array}{l}
  x = y' - x' \displaystyle \frac{b}{a} \\
  y = x'
  \end{array}
\right.
\ee

Note that this is a typical recursive relationship. The edge case happens
when $a=0$.

\be
gcd(0, b) = b = 0 a + 1 b
\ee

Summarize the above result, the extended Euclid algorithm can be defined
as the following:

\be
gcd_{ext}(a, b) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (b, 0, 1) & a = 0 \\
  (d, y' - x' \displaystyle \frac{b}{a}, x') & otherwise
  \end{array}
\right.
\ee

Where $d$, $x'$, $y'$ are defined in equation (\ref{eq:recursive-ext-gcd}).

The 2 water jugs puzzle is almost solved, but there are still two detailed
problems need to be tackled. First, extended Euclid algorithm gives the linear
combination for the greatest common divisor $d$. While the target volume
of water $g$ isn't necessarily equal to $d$. This can be easily solved
by multiplying $x$ and $y$ by $m$ times, where $m = g / gcd(a, b)$;
Second, we assume $x>0$, to form a procedure to fill jug $A$ with $x$ times.
However, the extended Euclid algorithm doesn't ensure $x$ to be positive.
For instance $gcd_{ext}(4, 9) = (1, -2, 1)$. Whenever we get a negative
$x$, since $d = x a + y b$, we can continuously add $b$ to $x$, and decrease
$y$ by $a$ till $a$ is greater than zero.

At this stage, we are able to give the complete solution to the 2 water jugs
puzzle. Below is an example Haskell program.

\lstset{language=Haskell}
\begin{lstlisting}
extGcd 0 b = (b, 0, 1)
extGcd a b = let (d, x', y') = extGcd (b `mod` a) a in
               (d, y' - x' * (b `div` a), x')

solve a b g | g `mod` d /= 0 = [] -- no solution
            | otherwise = solve' (x * g `div` d)
    where
      (d, x, y) = extGcd a b
      solve' x | x < 0 = solve' (x + b)
               | otherwise = pour x [(0, 0)]
      pour 0 ps = reverse ((0, g):ps)
      pour x ps@((a', b'):_) | a' == 0 = pour (x - 1) ((a, b'):ps) -- fill a
                             | b' == b = pour x ((a', 0):ps) -- empty b
                             | otherwise = pour x ((max 0 (a' + b' - b),
                                                    min (a' + b') b):ps)
\end{lstlisting}

Although we can solve the 2 water jugs puzzle with extended Euclid algorithm, the
solution may not be the best. For instance, when we are going to bring 4 gallons
of water from 3 and 5 gallons jugs. The extended Euclid algorithm produces the
following sequence:

\begin{verbatim}
[(0,0),(3,0),(0,3),(3,3),(1,5),(1,0),(0,1),(3,1),
(0,4),(3,4),(2,5),(2,0),(0,2),(3,2),(0,5),(3,5),
(3,0),(0,3),(3,3),(1,5),(1,0),(0,1),(3,1),(0,4)]
\end{verbatim}

It takes 23 steps to achieve the goal, while the best solution only need 6 steps:

\begin{verbatim}
[(0,0),(0,5),(3,2),(0,2),(2,0),(2,5),(3,4)]
\end{verbatim}

Observe the 23 steps, and we can find that jug $B$ has already contained 4 gallons
of water at the 8-th step. But the algorithm ignores this fact and goes on executing
the left 15 steps. The reason is that the linear combination $x$ and $y$ we find
with the extended Euclid algorithm are not the only numbers satisfying
$g = x a + b y$. For all these numbers, the smaller $|x| + |y|$, the less steps
are needed. There is an exercise to addressing this problem in this section.

The interesting problem is how to find the best solution? We have two approaches,
one is to find $x$ and $y$ to minimize $|x| + |y|$; the other is to adopt the quite
similar idea as the wolf-goat-cabbage puzzle. We focus on the latter in this
section. Since there are at most 6 possible
options: fill $A$, fill $B$, pour $A$ into $B$, pour $B$ into $A$, empty $A$ and
empty $B$, we can try them in parallel, and check which decision can lead to the
best solution. We need record all the states we've achieved to avoid any potential
repetition. In order to realize this parallel approach with reasonable resources,
a queue can be used to arrange our attempts. The elements stored in this queue
are series of pairs $(p, q)$, where $p$ and $q$ represent the volume of waters
contained in each jug. These pairs record the sequence of our operations from
the beginning to the latest. We initialize the queue with the singleton list
contains the starting state $\{ (0, 0) \}$.

\be
solve(a, b, g) = solve' \{ \{ (0, 0) \} \}
\ee

Every time, when the queue isn't empty, we pick a sequence from the head of the
queue. If this sequence
ends with a pair contains the target volume $g$, we find a solution, we can
print this sequence by reversing it; Otherwise, we expand the latest pair
by trying all the possible 6 options, remove any duplicated states, and add
them to the tail of the queue. Denote the queue as $Q$, the first sequence
stored on the head of the queue as $S$, the latest pair in $S$ as $(p, q)$,
and the rest of pairs as $S'$. After popping the head element, the queue
becomes $Q'$. This algorithm can be defined like below:

\be
solve'(Q) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & Q = \Phi \\
  reverse(S) & p = g \lor q = g \\
  solve'(EnQ'(Q', \{\{s'\} \cup S' | s' \in try(S)\})) & otherwise
  \end{array}
\right.
\ee

Where function $EnQ'$ pushes a list of sequence to the queue one by one.
Function $try(S)$ will try all possible 6 options to generate new
pairs of water volumes:

\be
try(S) = \{s' | s' \in \left \{ \begin{array}{l}
  fillA(p, q), fillB(p, q), \\
  pourA(p, q), pourB(p, q), \\
  emptyA(p, q), emptyB(p, q)
  \end{array}
  \right \}, s' \notin S' \}
\ee

It's intuitive to define the 6 options. For fill operations, the
result is that the volume of the filled jug is full; for empty
operation, the result volume is empty; for pour operation, we
need test if the jug is big enough to hold all the water.

\be
\begin{array}{ll}
fillA(p, q) = (a, q) & fillB(p, q) = (p, b) \\
emptyA(p, q) = (0, q) & emptyB(p, q) = (p, 0) \\
pourA(p, q) = (max(0, p + q - b), min(x + y, b)) & \\
pourB(p, q) = (min(x + y, a), max(0, x + y - a)) &
\end{array}
\ee

The following example Haskell program implements this method:

\lstset{language=Haskell}
\begin{lstlisting}
solve' a b g = bfs [[(0, 0)]] where
  bfs [] = []
  bfs (c:cs) | fst (head c) == g || snd (head c) == g = reverse c
             | otherwise = bfs (cs ++ map (:c) (expand c))
  expand ((x, y):ps) = filter (`notElem` ps) $ map (\f -> f x y)
                           [fillA, fillB, pourA, pourB, emptyA, emptyB]
  fillA _ y = (a, y)
  fillB x _ = (x, b)
  emptyA _ y = (0, y)
  emptyB x _ = (x, 0)
  pourA x y = (max 0 (x + y - b), min (x + y) b)
  pourB x y = (min (x + y) a, max 0 (x + y - a))
\end{lstlisting} %$

This method always returns the fast solution. It can also be realized in
imperative approach. Instead of storing the complete sequence of operations
in every element in the queue, we can store the unique state in a
global history list, and use links to track the operation sequence, this
can save spaces.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{img/water-jugs.ps}
  \caption{All attempted states are stored in a global list.}
  \label{fig:water-jugs}
\end{figure}

The idea is illustrated in figure \ref{fig:water-jugs}. The initial state
is (0, 0). Only `fill A' and `fill B' are possible. They are tried and
added to the record list; Next we can try and record `fill B' on top of (3, 0),
which yields new state (3, 5). However, when try `empty A' from state (3, 0),
we would return to the start state (0, 0). As this previous state has been
recorded, it is ignored. All the repeated states are in gray color in this
figure.

With such settings, we needn't remember the operation sequence in each element
in the queue explicitly. We can add a `parent' link to each node in figure
\ref{fig:water-jugs}, and use it to back-traverse to the starting point from
any state. The following example ANSI C code shows such a definition.

\lstset{language=C}
\begin{lstlisting}
struct Step {
    int p, q;
    struct Step* parent;
};

struct Step* make_step(int p, int q, struct Step* parent) {
    struct Step* s = (struct Step*) malloc(sizeof(struct Step));
    s->p = p;
    s->q = q;
    s->parent = parent;
    return s;
}
\end{lstlisting}

Where $p, q$ are volumes of water in the 2 jugs. For any state $s$,
define functions $p(s)$ and $q(s)$ return these 2 values, the imperative algorithm
can be realized based on this idea as below.

\begin{algorithmic}[1]
\Function{Solve}{$a, b, g$}
  \State $Q \gets \Phi$
  \State \Call{Push-and-record}{$Q$, (0, 0)}
  \While{$Q \neq \Phi$}
    \State $s \gets$ \Call{Pop}{$Q$}
    \If{$p(s) = g \lor q(s) = g$}
      \State \Return $s$
    \Else
      \State $C \gets$ \Call{Expand}{$s$}
      \For{$\forall c \in C$}
        \If{$c \neq s \land \lnot$ \Call{Visited}{$c$}}
          \State \Call{Push-and-record}{$Q, c$}
        \EndIf
      \EndFor
    \EndIf
  \EndWhile
  \State \Return NIL
\EndFunction
\end{algorithmic}

Where \textproc{Push-and-record} does not only push an element to the queue, but also
record this element as visited, so that we can check if an element has been visited
before in the future. This can be implemented with a list. All push operations
append the new elements to the tail. For pop operation, instead of removing the element
pointed by head, the head pointer only advances to the next one. This list contains
historic data which has to be reset explicitly. The following ANSI C
code illustrates this idea.

\lstset{language=C}
\begin{lstlisting}
struct Step *steps[1000], **head, **tail = steps;

void push(struct Step* s) { *tail++ = s; }

struct Step* pop() { return *head++; }

int empty() { return head == tail; }

void reset() {
    struct Step **p;
    for (p = steps; p != tail; ++p)
        free(*p);
    head = tail = steps;
}
\end{lstlisting}

In order to test a state has been visited, we can traverse the list to compare $p$ and $q$.

\lstset{language=C}
\begin{lstlisting}
int eq(struct Step* a, struct Step* b) {
    return a->p == b->p && a->q == b->q;
}

int visited(struct Step* s) {
    struct Step **p;
    for (p = steps; p != tail; ++p)
        if (eq(*p, s)) return 1;
    return 0;
}
\end{lstlisting}

The main program can be implemented as below:

\lstset{language=C}
\begin{lstlisting}
struct Step* solve(int a, int b, int g) {
    int i;
    struct Step *cur, *cs[6];
    reset();
    push(make_step(0, 0, NULL));
    while (!empty()) {
        cur = pop();
        if (cur->p == g || cur->q == g)
            return cur;
        else {
            expand(cur, a, b, cs);
            for (i = 0; i < 6; ++i)
                if(!eq(cur, cs[i]) && !visited(cs[i]))
                    push(cs[i]);
        }
    }
    return NULL;
}
\end{lstlisting}

Where function \texttt{expand} tries all the 6 possible options:

\lstset{language=C}
\begin{lstlisting}
void expand(struct Step* s, int a, int b, struct Step** cs) {
    int p = s->p, q = s->q;
    cs[0] = make_step(a, q, s); /*fill A*/
    cs[1] = make_step(p, b, s); /*fill B*/
    cs[2] = make_step(0, q, s); /*empty A*/
    cs[3] = make_step(p, 0, s); /*empty B*/
    cs[4] = make_step(max(0, p + q - b), min(p + q, b), s); /*pour A*/
    cs[5] = make_step(min(p + q, a), max(0, p + q - a), s); /*pour B*/
}
\end{lstlisting}

And the result steps is back tracked in reversed order, it can be output
with a recursive function:

\lstset{language=C}
\begin{lstlisting}
void print(struct Step* s) {
    if (s) {
        print(s->parent);
        printf("%d, %d\n", s->p, s->q);
    }
}
\end{lstlisting}

\subsubsection{Kloski}
\index{Kloski puzzle}
Kloski is a block sliding puzzle. It appears in many countries. There are different
sizes and layouts. Figure \ref{fig:klotski-cn} illustrates a traditional Kloski game in China.

\begin{figure}[htbp]
 \centering
 \subfloat[Initial layout of blocks]{\includegraphics[scale=0.5]{img/klotski-cn1.eps}} \hspace{.01\textwidth}
 \subfloat[Block layout after several movements]{\includegraphics[scale=0.5]{img/klotski-cn2.eps}}
 \caption{`Huarong Dao', the traditional Kloski game in China.}
 \label{fig:klotski-cn}
\end{figure}

In this puzzle, there are 10 blocks, each is labeled with text or icon. The smallest block has
size of 1 unit square, the biggest one is $2 \times 2$ units size. Note there is a slot of 2 units
wide at the middle-bottom of the board. The biggest block represents a king in ancient time, while
the others are enemies. The goal is to move the biggest block to the slot, so
that the king can escape. This game is named as `Huarong Dao', or `Huarong Escape' in China.
Figure \ref{fig:klotski-jp} shows the similar Kloski puzzle in Japan. The biggest block means
daughter, while the others are her family members. This game is named as `Daughter in the box'
in Japan (Japanese name: {\em hakoiri musume}).

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/klotski-jp.eps}
 \caption{`Daughter in the box', the Kloski game in Japan.}
 \label{fig:klotski-jp}
\end{figure}

In this section, we want to find a solution, which can slide blocks from the initial state to
the final state with the minimum movements.

The intuitive idea to model this puzzle is to use a $5 \times 4$ matrix representing the
board. All pieces are labeled with a number. The following matrix $M$, for example,
shows the initial state of the puzzle.

\[
M = \left [
  \begin{array}{cccc}
  1 & 10 & 10 & 2 \\
  1 & 10 & 10 & 2 \\
  3 & 4 & 4 & 5 \\
  3 & 7 & 8 & 5 \\
  6 & 0 & 0 & 9
  \end{array}
\right ]
\]

In this matrix, the cells of value $i$ mean the $i$-th piece covers this cell. The special
value 0 represents a free cell. By using sequence 1, 2, ... to identify pieces, a special
layout can be further simplified as an array $L$. Each element is a list of cells covered by
the piece indexed with this element.
For example, $L[4] = \{(3, 2), (3, 3)\}$ means the $4$-th piece covers cells at
position $(3, 2)$ and $(3, 3)$, where $(i, j)$ means the cell at row $i$ and column $j$.

The starting layout can be written as the following Array.

\[
\begin{array}{l}
\{ \{(1, 1), (2, 1) \},
 \{(1, 4), (2, 4) \},
 \{(3, 1), (4, 1) \},
 \{(3, 2), (3, 3) \},
 \{(3, 4), (4, 4) \}, \\
 \{(5, 1) \},
 \{(4, 2) \},
 \{(4, 3) \},
 \{(5, 4) \},
 \{(1, 2), (1, 3), (2, 2), (2, 3) \} \}
\end{array}
\]

When moving the Kloski blocks, we need examine all the 10 blocks, checking each block if
it can move up, down, left and right. it seems that this approach would lead to a very huge
amount of possibilities, because each step might have $10 \times 4$ options, there will be
about $40^n$ cases in the $n$-th step.

Actually, there won't be so much options. For example, in the first step, there are only 4
valid moving: the 6-th piece moves right; the 7-th and 8-th move down; and the 9-th moves left.

All others are invalid moving. Figure \ref{fig:klotski-valid-move} shows how to test if the
moving is possible.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.4]{img/klotski-valid-move.eps}
 \caption{Left: both the upper and the lower 1 are OK; Right: the upper 1 is OK, the lower 1 conflicts with 2.}
 \label{fig:klotski-valid-move}
\end{figure}

The left example illustrates sliding block labeled with 1 down. There are two cells covered by
this block. The upper 1 moves to the cell previously occupied by this same block, which is also
labeled with 1; The lower 1 moves to a free cell, which is labeled with 0;

The right example, on the other hand, illustrates invalid sliding. In this case, the upper cells
could move to the cell occupied by the same block. However, the lower cell labeled with 1 can't
move to the cell occupied by other block, which is labeled with 2.

In order to test the valid moving, we need examine all the cells a block will cover.
If they are labeled
with 0 or a number as same as this block, the moving is valid. Otherwise it conflicts with some
other block. For a layout $L$, the corresponding matrix is $M$, suppose we want to move the $k$-th
block with $(\Delta x, \Delta y)$, where $|\Delta x| \leq 1, |\Delta y| \leq 1$. The following
equation tells if the moving is valid:

\be
\begin{array}{rl}
valid(L, k, \Delta x, \Delta y): & \\
\forall (i, j) \in L[k] \Rightarrow & i' = i + \Delta y, j' = j + \Delta x, \\
& (1, 1) \leq (i', j') \leq (5, 4), M_{i'j'} \in \{k, 0\}
\end{array}
\ee

Another important point to solve Kloski puzzle, is about how to avoid repeated attempts.
The obvious case is that after a series of sliding, we end up a matrix which have been
transformed from. However, it is not enough to only avoid the same matrix. Consider the following
two metrics. Although $M_1 \neq M_2$, we need drop options to $M_2$, because they are
essentially the same.

\[
\begin{array}{cc}
M_1 = \left [
  \begin{array}{cccc}
  1 & 10 & 10 & 2 \\
  1 & 10 & 10 & 2 \\
  3 & 4 & 4 & 5 \\
  3 & 7 & 8 & 5 \\
  6 & 0 & 0 & 9
  \end{array}
\right ] &
M_2 = \left [
  \begin{array}{cccc}
  2 & 10 & 10 & 1 \\
  2 & 10 & 10 & 1 \\
  3 & 4 & 4 & 5 \\
  3 & 7 & 6 & 5 \\
  8 & 0 & 0 & 9
  \end{array}
\right ]
\end{array}
\]

This fact tells us, that we should compare the layout, but not merely matrix to avoid
repetition. Denote the corresponding layouts as $L_1$ and $L_2$ respectively, it's
easy to verify that $||L_1|| = ||L_2||$, where $||L||$ is the normalized layout, which
is defined as below:

\be
||L|| = sort(\{sort(l_i) | \forall l_i \in L\})
\ee

In other words, a normalized layout is ordered for all its elements, and every element
is also ordered. The ordering can be defined as that $(a, b) \leq (c, d) \Leftrightarrow
a n + b \leq c n + d$, where $n$ is the width of the matrix.

Observing that the Kloski board is symmetric, thus a layout can be mirrored from
another one. Mirrored layout is also a kind of repeating, which should be avoided.
The following $M_1$ and $M_2$ show such an example.

\[
\begin{array}{cc}
M_1 = \left [
  \begin{array}{cccc}
  10 & 10 & 1 & 2 \\
  10 & 10 & 1 & 2 \\
  3 & 5 & 4 & 4 \\
  3 & 5 & 8 & 9 \\
  6 & 7 & 0 & 0
  \end{array}
\right ] &
M_2 = \left [
  \begin{array}{cccc}
  3 & 1 & 10 & 10 \\
  3 & 1 & 10 & 10 \\
  4 & 4 & 2 & 5 \\
  7 & 6 & 2 & 5 \\
  0 & 0 & 9 & 8
  \end{array}
\right ]
\end{array}
\]

Note that, the normalized layouts are symmetric to each other.
It's easy to get a mirrored layout like this:

\be
mirror(L) = \{\{(i, n - j + 1) | \forall (i, j) \in l \} | \forall l \in L \}
\ee

We find that the matrix representation is useful in validating the moving, while the
layout is handy to model the moving and avoid repeated attempt. We can use the similar
approach to solve the Kloski puzzle. We need a queue, every element in the queue
contains two parts: a series of moving and the latest layout led by the moving.
Each moving is in form of $(k, (\Delta y, \Delta x))$, which means moving the $k$-th
block, with $\Delta y$ in row, and $\Delta x$ in column in the board.

The queue contains the starting layout when initialized.
Whenever this queue isn't empty, we pick the first one from the head,
checking if the biggest block is on target, that $L[10] = \{(4, 2), (4, 3), (5, 2), (5, 3)\}$.
If yes, then we are done; otherwise, we try to move every block with 4 options:
left, right, up, and down, and store all the possible, unique new layout to the
tail of the queue. During this searching, we need record all the normalized
layouts we've ever found to avoid any duplication.

Denote the queue as $Q$, the historic layouts as $H$, the first layout on the
head of the queue as $L$, its corresponding matrix as $M$. and the moving sequence
to this layout as $S$. The algorithm can be defined as the following.

\be
solve(Q, H) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & Q = \Phi \\
  reverse(S) & L[10] = \{(4, 2), (4, 3), (5, 2), (5, 3)\} \\
  solve(Q', H') & otherwise
  \end{array}
\right.
\ee

The first clause says that if the queue is empty, we've tried all the possibilities
and can't find a solution; The second clause finds a solution, it returns the moving
sequence in reversed order; These are two edge cases. Otherwise, the algorithm
expands the current layout, puts all the valid new layouts to the tail of the
queue to yield $Q'$, and updates the normalized layouts to $H'$. Then it performs
recursive searching.

In order to expand a layout to valid unique new layouts, we can define a function
as below:

\be
\begin{array}{rl}
expand(L, H) = \{(k, (\Delta y, \Delta x) | & \forall k \in \{1, 2, ..., 10\}, \\
  &  \forall (\Delta y, \Delta x) \in \{(0, -1), (0, 1), (-1, 0), (1, 0)\}, \\
  &  valid(L, k, \Delta x, \Delta y), unique(L', H)\}
\end{array}
\ee

Where $L'$ is the the new layout by moving the $k$-th block with $(\Delta y, \Delta x)$
from $L$, $M'$ is the corresponding matrix, and $M''$ is the matrix to the mirrored
layout of $L'$. Function unique is defined like this:

\be
unique(L', H) = M' \notin H \land M'' \notin H
\ee

We'll next show some example Haskell Kloski programs. As array isn't mutable in
the purely functional settings, tree based map is used to represent layout
\footnote{Alternatively, finger tree based sequence shown in previous chapter
can be used}. Some type synonyms are defined as below:

\lstset{language=Haskell}
\begin{lstlisting}
import qualified Data.Map as M
import Data.Ix
import Data.List (sort)

type Point = (Integer, Integer)
type Layout = M.Map Integer [Point]
type Move = (Integer, Point)

data Ops = Op Layout [Move]
\end{lstlisting}

The main program is almost as same as the $sort(Q, H)$ function defined above.

\lstset{language=Haskell}
\begin{lstlisting}
solve :: [Ops] -> [[[Point]]]-> [Move]
solve [] _ = [] -- no solution
solve (Op x seq : cs) visit
    | M.lookup 10 x == Just [(4, 2), (4, 3), (5, 2), (5, 3)] = reverse seq
    | otherwise = solve q visit'
  where
    ops = expand x visit
    visit' = map (layout . move x) ops ++ visit
    q = cs ++ [Op (move x op) (op:seq) | op <- ops ]
\end{lstlisting}

Where function \texttt{layout} gives the normalized form by sorting. \texttt{move} returns
the updated map by sliding the $i$-th block with $(\Delta y, \Delta x)$.

\lstset{language=Haskell}
\begin{lstlisting}
layout = sort . map sort . M.elems

move x (i, d) = M.update (Just . map (flip shift d)) i x

shift (y, x) (dy, dx) = (y + dy, x + dx)
\end{lstlisting}

Function \texttt{expand} gives all the possible new options. It can be directly translated
from $expand(L, H)$.

\lstset{language=Haskell}
\begin{lstlisting}
expand :: Layout -> [[[Point]]] -> [Move]
expand x visit = [(i, d) | i <-[1..10],
                           d <- [(0, -1), (0, 1), (-1, 0), (1, 0)],
                           valid i d, unique i d] where
  valid i d = all (\p -> let p' = shift p d in
                    inRange (bounds board) p' &&
                    (M.keys $ M.filter (elem p') x) `elem` [[i], []])
              (maybe [] id $ M.lookup i x)
  unique i d = let mv = move x (i, d) in
               all (`notElem` visit) (map layout [mv, mirror mv])
\end{lstlisting}

Note that we also filter out the mirrored layouts. The \texttt{mirror} function is given
as the following.

\lstset{language=Haskell}
\begin{lstlisting}
mirror = M.map (map (\ (y, x) -> (y, 5 - x)))
\end{lstlisting}

This program takes several minutes to produce the best solution, which takes 116 steps.
The final 3 steps are shown as below:

\begin{verbatim}
...

['5', '3', '2', '1']
['5', '3', '2', '1']
['7', '9', '4', '4']
['A', 'A', '6', '0']
['A', 'A', '0', '8']

['5', '3', '2', '1']
['5', '3', '2', '1']
['7', '9', '4', '4']
['A', 'A', '0', '6']
['A', 'A', '0', '8']

['5', '3', '2', '1']
['5', '3', '2', '1']
['7', '9', '4', '4']
['0', 'A', 'A', '6']
['0', 'A', 'A', '8']

total 116 steps
\end{verbatim}

The Kloski solution can also be realized imperatively. Note that the $solve(Q, H)$ is
tail-recursive, it's easy to transform the algorithm with looping. We can also link one layout
to its parent, so that the moving sequence can be recorded globally.
This can save some spaces, as the queue needn't store the moving information in every element.
When output the result, we only need back-tracking to the starting layout from the
last one.

Suppose function \textproc{Link}($L', L$) links a new layout $L'$ to its parent layout
$L$.
The following algorithm takes a starting layout, and searches for best moving sequence.

\begin{algorithmic}[1]
\Function{Solve}{$L_0$}
  \State $H \gets ||L_0||$
  \State $Q \gets \Phi$
  \State \textproc{Push}($Q$, \Call{Link}{$L_0$, NIL})
  \While{$Q \neq \Phi$}
    \State $L \gets $ \Call{Pop}{$Q$}
    \If{$L[10] = \{(4, 2), (4, 3), (5, 2), (5, 3)\}$}
      \State \Return $L$
    \Else
      \For{each $L' \in$ \Call{Expand}{$L, H$}}
        \State \textproc{Push}($Q$, \Call{Link}{$L', L$})
        \State \Call{Append}{$H, ||L'||$}
      \EndFor
    \EndIf
  \EndWhile
  \State \Return NIL \Comment{No solution}
\EndFunction
\end{algorithmic}

The following example Python program implements this algorithm:

\lstset{language=Python}
\begin{lstlisting}
class Node:
    def __init__(self, l, p = None):
        self.layout = l
        self.parent = p

def solve(start):
    visit = [normalize(start)]
    queue = [Node(start)]
    while queue != []:
        cur = queue.pop(0)
        layout = cur.layout
        if layout[-1] == [(4, 2), (4, 3), (5, 2), (5, 3)]:
            return cur
        else:
            for brd in expand(layout, visit):
                queue.append(Node(brd, cur))
                visit.append(normalize(brd))
    return None # no solution
\end{lstlisting}

Where \texttt{normalize} and \texttt{expand} are implemented as below:

\lstset{language=Python}
\begin{lstlisting}
def normalize(layout):
    return sorted([sorted(r) for r in layout])

def expand(layout, visit):
    def bound(y, x):
        return 1 <= y and y <= 5 and 1 <= x and x <= 4
    def valid(m, i, y, x):
        return m[y - 1][x - 1] in [0, i]
    def unique(brd):
        (m, n) = (normalize(brd), normalize(mirror(brd)))
        return all(m != v and n != v for v in visit)
    s = []
    d = [(0, -1), (0, 1), (-1, 0), (1, 0)]
    m = matrix(layout)
    for i in range(1, 11):
        for (dy, dx) in d:
            if all(bound(y + dy, x + dx) and valid(m, i, y + dy, x + dx)
                    for (y, x) in layout[i - 1]):
                brd = move(layout, (i, (dy, dx)))
                if unique(brd):
                    s.append(brd)
    return s
\end{lstlisting}

Like most programming languages, arrays are indexed from 0 but not 1 in Python.
This has to be handled properly. The rest functions including \texttt{mirror},
\texttt{matrix}, and \texttt{move} are implemented as the following.

\lstset{language=Python}
\begin{lstlisting}
def mirror(layout):
    return [[(y, 5 - x) for (y, x) in r] for r in layout]

def matrix(layout):
    m = [[0]*4 for _ in range(5)]
    for (i, ps) in zip(range(1, 11), layout):
        for (y, x) in ps:
            m[y - 1][x - 1] = i
    return m

def move(layout, delta):
    (i, (dy, dx)) = delta
    m = dup(layout)
    m[i - 1] = [(y + dy, x + dx) for (y, x) in m[i - 1]]
    return m

def dup(layout):
    return [r[:] for r in layout]
\end{lstlisting}

It's possible to modify this Kloski algorithm, so that it does not only stop at the
fast solution, but also search all the solutions. In such case, the computation time
is bound to the size of a space $V$, where $V$ holds all the layouts can be transformed
from the starting layout. If all these layouts are stored globally, with a parent field
point to the predecessor, the space requirement of this algorithm is also bound to $O(V)$.

\subsubsection{Summary of BFS}
\index{BFS} \index{Breadth-first search}
The above three puzzles, the wolf-goat-cabbage puzzle, the water jugs puzzle, and the
Kloski puzzle show some common solution structure. Similar to the DFS problems, they
all have the starting state and the end state.
The wolf-goat-cabbage puzzle starts with the wolf, the goat,
the cabbage and the farmer all in one side, while the other side is empty. It ends up
in a state that they all moved to the other side.  The water jugs
puzzle starts with two empty jugs, and ends with either jug contains a certain volume
of water. The Kloski puzzle starts from a layout and ends to another layout that
the biggest block begging slided to a given position.

All problems specify a set of rules which can transfer from one state to another.
Different form the DFS approach, we try all the possible options `in parallel'. We won't
search further until all the other alternatives in the same step have been examined.
This method ensures that the solution with the minimum steps can be found before those
with more steps. Review and compare the two figures we've drawn before shows the difference
between these two approaches. For the later one, because we expand the searching horizontally,
it is called as Breadth-first search (BFS for short).

\begin{figure}[htbp]
 \centering
 \subfloat[Depth First Search]{ \includegraphics[scale=0.5]{img/dfs-tree.eps}}
 \subfloat[Breadth First Search]{ \includegraphics[scale=0.5]{img/bfs-tree.eps}}
 \caption{Search orders for DFS and BFS.}
 \label{fig:dfs-bfs-tree}
\end{figure}

As we can't perform search really in parallel, BFS realization typically utilizes a queue
to store the search options. The candidate with less steps pops from the head, while
the new candidate with more steps is pushed to the tail of the queue. Note that the
queue should meet constant time enqueue and dequeue requirement, which we've explained
in previous chapter of queue. Strictly speaking, the example functional programs shown above
don't meet this criteria. They use list to mimic queue, which can only provide
linear time pushing. Readers can replace them with the functional queue we explained
before.

BFS provides a simple method to search for optimal solutions in terms of the number
of steps. However, it can't search for more general optimal solution.
Consider another directed graph as shown in figure \ref{fig:weighted-dag}, the length of
each section varies. We can't use BFS to find the shortest route from one city to
another.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/weighted-dag.ps}
 \caption{A weighted directed graph.}
 \label{fig:weighted-dag}
\end{figure}

Note that the shortest route from city $a$ to city $c$ isn't the one with the fewest steps
$a \to b \to c$. The total length of this route is 22; But the route with more steps
$a \to e \to f \to c$ is the best. The length of it is 20. The coming sections
introduce other algorithms to search for optimal solution.

\subsection{Search the optimal solution}
Searching for the optimal solution is quite important in many aspects. People need the `best'
solution to save time, space, cost, or energy. However, it's not easy to find the
best solution with limited resources. There have been many optimal problems can
only be solved by brute-force. Nevertheless, we've found that, for some of them,
There exists special simplified ways to search the optimal solution.

\subsubsection{Grady algorithm}
\index{Grady algorithm}

\paragraph{Huffman coding}
\index{Huffman coding}
Huffman coding is a solution to encode information with the shortest length of
code. Consider the popular ASCII code, which uses 7 bits to encode characters, digits,
and symbols. ASCII code can represent $2^7 = 128$ different symbols. With 0, 1 bits,
we need at least $\log_2 n$ bits to distinguish $n$ different symbols. For text with
only case insensitive English letters, we can define a code table like below.

\begin{tabular}{l|l||l|l}
char & code & char & code \\
\hline
A & 00000 & N & 01101 \\
B & 00001 & O & 01110 \\
C & 00010 & P & 01111 \\
D & 00011 & Q & 10000 \\
E & 00100 & R & 10001 \\
F & 00101 & S & 10010 \\
G & 00110 & T & 10011 \\
H & 00111 & U & 10100 \\
I & 01000 & V & 10101 \\
J & 01001 & W & 10110 \\
K & 01010 & X & 10111 \\
L & 01011 & Y & 11000 \\
M & 01100 & Z & 11001 \\
\hline
\end{tabular}

With this code table, text `INTERNATIONAL' is encoded to 65 bits.

\begin{verbatim}
00010101101100100100100011011000000110010001001110101100000011010
\end{verbatim}

Observe the above code table, which actually maps the letter `A' to 'Z' from
0 to 25. There are 5 bits to represent every code. Code zero is forced
as '00000' but not '0' for example. Such kind of coding method, is called
fixed-length coding.

Another coding method is variable-length coding. That we can use just one bit `0'
for `A', two bits `10' for C, and 5 bits `11001' for `Z'. Although this approach
can shorten the total length of the code for `INTERNATIONAL' from 65 bits
dramatically, it causes problem when decoding. When processing a sequence of
bits like `1101', we don't know if it means `1' followed by `101', which stands
for `BF'; or `110' followed by `1', which is `GB', or `1101' which is `N'.

The famous Morse code is variable-length coding system. That the most used letter `E'
is encoded as a dot, while `Z' is encoded as two dashes and two dots. Morse code
uses a special pause separator to indicate the termination of a code, so that
the above problem won't happen. There is another solution to avoid ambiguity.
Consider the following code table.

\begin{tabular}{l|l||l|l}
char & code & char & code \\
\hline
A & 110 & E & 1110 \\
I & 101 & L & 1111 \\
N & 01 & O & 000 \\
R & 001 & T & 100 \\
\hline
\end{tabular}

Text `INTERNATIONAL' is encoded to 38 bits only:

\begin{verbatim}
10101100111000101110100101000011101111
\end{verbatim}

If decode the bits against the above code table, we won't meet any ambiguity symbols.
This is because there is no code for any symbol is the prefix of another one. Such
code is called {\em prefix-code}. (You may wonder why it isn't called as non-prefix code.)
By using prefix-code, we needn't separators at all. So that the length of the code
can be shorten.

This is a very interesting problem. Can we find a prefix-code table, which produce
the shortest code for a given text? The very same problem was given to David A. Huffman
in 1951, who was still a student in MIT\cite{Huffman}. His professor Robert M. Fano
told the class that those who could solve this problem needn't take the final exam.
Huffman almost gave up and started preparing the final exam when he found the most
efficient answer.

The idea is to create the coding table according to the frequency of the symbol appeared
in the text. The more used symbol is assigned with the shorter code.

It's not hard to process some text, and calculate the occurrence for each symbol.
So that we have a symbol set, each one is augmented with a weight. The weight can
be the number which indicates the frequency this symbol occurs. We can use the
number of occurrence, or the probabilities for example.

Huffman discovered that a binary tree can be used to generate prefix-code. All
symbols are stored in the leaf nodes. The codes are generated by traversing
the tree from root. When go left, we add a zero; and when go right we add a one.

Figure \ref{fig:huffman-tr} illustrates a binary tree. Taking symbol 'N' for example,
starting from the root, we first go left, then right and arrive at 'N'. Thus the
code for 'N' is '01'; While for symbol 'A', we can go right, right, then left.
So 'A' is encode to '110'. Note that this approach ensures none code is the prefix
of the other.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.5]{img/huffman-tr.ps}
 \caption{An encoding tree.}
 \label{fig:huffman-tr}
\end{figure}

Note that this tree can also be used directly for decoding. When scan a series of
bits, if the bit is zero, we go left; if the bit is one, we go right. When arrive
at a leaf, we decode a symbol from that leaf. And we restart from the root of the
tree for the coming bits.

Given a list of symbols with weights, we need build such a binary tree, so that the
symbol with greater weight has shorter path from the root. Huffman developed a bottom-up
solution. When start, all symbols are put into a leaf node. Every time, we pick two
nodes, which has the smallest weight, and merge them into a branch node. The weight
of this branch is the sum of its two children. We repeatedly pick the two smallest
weighted nodes and merge till there is only one tree left. Figure \ref{fig:huffman-build}
illustrates such a building process.

\begin{figure}[htbp]
 \centering
 \subfloat[1. ]{ \includegraphics[scale=0.4]{img/huffman-tr1.ps}}
 \subfloat[2. ]{ \includegraphics[scale=0.4]{img/huffman-tr2.ps}}
 \subfloat[3. ]{ \includegraphics[scale=0.4]{img/huffman-tr3.ps}} \\
 \subfloat[4. ]{ \includegraphics[scale=0.4]{img/huffman-tr4.ps}}
 \subfloat[5. ]{ \includegraphics[scale=0.4]{img/huffman-tr5.ps}} \\
 \subfloat[6. ]{ \includegraphics[scale=0.3]{img/huffman-tr6.ps}} \\
 \subfloat[7. ]{ \includegraphics[scale=0.3]{img/huffman-tr.ps}}
 \caption{Steps to build a Huffman tree.}
 \label{fig:huffman-build}
\end{figure}

We can reuse the binary tree definition to formalize Huffman coding. We augment
the weight information, and the symbols are only stored in leaf nodes. The following
C like definition, shows an example.

\lstset{language=C}
\begin{lstlisting}
struct Node {
    int w;
    char c;
    struct Node *left, *right;
};
\end{lstlisting}

Some limitation can be added to the definition, as empty tree isn't allowed.
A Huffman tree is either a leaf, which contains a symbol and its weight; or
a branch, which only holds total weight of all leaves. The following Haskell
code, for instance, explicitly specifies these two cases.

\lstset{language=Haskell}
\begin{lstlisting}
data HTr w a = Leaf w a | Branch w (HTr w a) (HTr w a)
\end{lstlisting}

When merge two Huffman trees $T_1$ and $T_2$ to a bigger one, These two trees
are set as its children. We can select either one as the left, and the other
as the right. the weight of the result tree $T$ is the sum of its two children.
so that $w = w_1 + w_2$. Define $T_1 < T_2$ if $w_1 < w_2$, One possible Huffman
tree building algorithm can be realized as the following.

\be
build(A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  T_1 & A = \{ T_1 \} \\
  build(\{ merge(T_a, T_b) \} \cup A') & otherwise
  \end{array}
\right.
\ee

$A$ is a list of trees. It is initialized as leaves for all symbols and their
weights. If there is only one tree in this list, we are done, the tree is the final
Huffman tree. Otherwise, The two smallest tree $T_a$ and $T_b$ are extracted,
and the rest trees are hold in list $A'$. $T_a$ and $T_b$ are merged to one bigger
tree, and put back to the tree list for further recursive building.

\be
(T_a, T_b, A') = extract(A)
\ee

We can scan the tree list to extract the 2 nodes with the smallest weight. Below
equation shows that when the scan begins, the first 2 elements are compared and
initialized as the two minimum ones. An empty accumulator is passed as the last
argument.

\be
extract(A) = extract'(min(T_1, T_2), max(T_1, T_2), \{T_3, T_4, ... \}, \Phi)
\ee

For every tree, if its weight is less than the smallest two we've ever found, we
update the result to contain this tree. For any given tree list $A$, denote the
first tree in it as $T_1$, and the rest trees except $T_1$ as $A'$. The scan
process can be defined as the following.

\be
extract'(T_a, T_b, A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (T_a, T_b, B) & A = \Phi \\
  extract'(T_a', T_b', A', \{T_b\} \cup A) & T_1 < T_b\\
  extract'(T_a, T_b, A', \{T_1\} \cup A) & otherwise
  \end{array}
\right.
\ee

Where $T_a' = min(T_1, T_a)$, $T_b' = max(T_1, T_a)$ are the updated two trees with
the smallest weights.

The following Haskell example program implements this Huffman tree building algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
build [x] = x
build xs = build ((merge x y) : xs') where
  (x, y, xs') = extract xs

extract (x:y:xs) = min2 (min x y) (max x y) xs [] where
  min2 x y [] xs = (x, y, xs)
  min2 x y (z:zs) xs | z < y = min2 (min z x) (max z x) zs (y:xs)
                     | otherwise = min2 x y zs (z:xs)
\end{lstlisting}

This building solution can also be realized imperatively. Given an array of Huffman
nodes, we can use the last two cells to hold the nodes with the smallest weights.
Then we scan the rest of the array from right to left. Whenever there is a node with
the smaller weight, this node will be exchanged with the bigger one of the last two.
After all nodes have been examined, we merge the trees in the last two cells, and drop
the last cell. This shrinks the array by one. We repeat this process till there is only
one tree left.

\begin{algorithmic}[1]
\Function{Huffman}{$A$}
  \While{$|A|>1$}
    \State $n \gets |A|$
    \For{$i \gets n - 2 $ down to $1$}
      \If{$A[i] <$ \Call{Max}{$A[n], A[n-1]$}}
        \State \textproc{Exchange} $A[i]$ $\leftrightarrow$ \Call{Max}{$A[n], A[n-1]$}
      \EndIf
    \EndFor
    \State $A[n-1] \gets$ \Call{Merge}{$A[n], A[n-1]$}
    \State \Call{Drop}{$A[n]$}
  \EndWhile
  \State \Return $A[1]$
\EndFunction
\end{algorithmic}

The following C++ example program implements this algorithm. Note that this algorithm
needn't the last two elements being ordered.

\lstset{language=C++}
\begin{lstlisting}
typedef vector<Node*> Nodes;

bool lessp(Node* a, Node* b) { return a->w < b->w; }

Node* max(Node* a, Node* b) { return lessp(a, b) ? b : a; }

void swap(Nodes& ts, int i, int j, int k) {
    swap(ts[i], ts[ts[j] < ts[k] ? k : j]);
}

Node* huffman(Nodes ts) {
    int n;
    while((n = ts.size()) > 1) {
        for (int i = n - 3; i >= 0; --i)
            if (lessp(ts[i], max(ts[n-1], ts[n-2])))
                swap(ts, i, n-1, n-2);
        ts[n-2] = merge(ts[n-1], ts[n-2]);
        ts.pop_back();
    }
    return ts.front();
}
\end{lstlisting}

The algorithm merges all the leaves, and it need scan the list in each iteration. Thus the performance
is quadratic. This algorithm can be improved. Observe that each time, only the two trees with the
smallest weights are merged. This reminds us the heap data structure. Heap ensures to access
the smallest element fast. We can put all the leaves in a heap. For binary heap, this is typically
a linear operation. Then we extract the minimum element twice, merge them, then put the bigger
tree back to the heap. This is $O(\lg n)$ operation if binary heap is used. So the total performance
is $O(n \lg n)$, which is better than the above algorithm. The next algorithm extracts the
node from the heap, and starts Huffman tree building.

\be
build(H) = reduce(top(H), pop(H))
\ee

This algorithm stops when the heap is empty; Otherwise, it extracts another nodes from the heap for
merging.

\be
reduce(T, H) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  T & H = \Phi \\
  build(insert(merge(T, top(H)), pop(H))) & otherwise
  \end{array}
\right.
\ee

Function $build$ and $reduce$ are mutually recursive. The following Haskell example program
implements this algorithm by using heap defined in previous chapter.

\lstset{language=Haskell}
\begin{lstlisting}
huffman' :: (Num a, Ord a) => [(b, a)] -> HTr a b
huffman' = build' . Heap.fromList . map (\(c, w) -> Leaf w c) where
  build' h = reduce (Heap.findMin h) (Heap.deleteMin h)
  reduce x Heap.E = x
  reduce x h = build' $ Heap.insert (Heap.deleteMin h) (merge x (Heap.findMin h))
\end{lstlisting} %$

The heap solution can also be realized imperatively. The leaves are firstly transformed to
a heap, so that the one with the minimum weight is put on the top. As far as there are more
than 1 elements in the heap, we extract the two smallest, merge them to a bigger one, and
put back to the heap. The final tree left in the heap is the result Huffman tree.

\begin{algorithmic}[1]
\Function{Huffman'}{$A$}
  \State \Call{Build-Heap}{$A$}
  \While{$|A| > 1$}
    \State $T_a \gets$ \Call{Heap-Pop}{$A$}
    \State $T_b \gets$ \Call{Heap-Pop}{$B$}
    \State \textproc{Heap-Push}($A$, \Call{Merge}{$T_a, T_b$})
  \EndWhile
  \State \Return \Call{Heap-Pop}{$A$}
\EndFunction
\end{algorithmic}

The following example C++ code implements this heap solution. The heap used here
is provided in the standard library. Because the max-heap, but not min-heap would
be made by default, a greater predication is explicitly passed as argument.

\lstset{language=C++}
\begin{lstlisting}
bool greaterp(Node* a, Node* b) { return b->w < a->w; }

Node* pop(Nodes& h) {
    Node* m = h.front();
    pop_heap(h.begin(), h.end(), greaterp);
    h.pop_back();
    return m;
}

void push(Node* t, Nodes& h) {
    h.push_back(t);
    push_heap(h.begin(), h.end(), greaterp);
}

Node* huffman1(Nodes ts) {
    make_heap(ts.begin(), ts.end(), greaterp);
    while (ts.size() > 1) {
        Node* t1 = pop(ts);
        Node* t2 = pop(ts);
        push(merge(t1, t2), ts);
    }
    return ts.front();
}
\end{lstlisting}

When the symbol-weight list has been already sorted, there exists a linear time method
to build the Huffman tree. Observe that during the Huffman tree building, it produces
a series of merged trees with weight in ascending order. We can use a queue to
manage the merged trees. Every time, we pick the two trees with the smallest weight
from both the queue and the list, merge them and push the result to the queue.
All the trees in the list will be processed, and there will be only one tree
left in the queue. This tree is the result Huffman tree. This process starts
by passing an empty queue as below.

\be
build'(A) = reduce'(extract''(\Phi, A))
\ee

Suppose $A$ is in ascending order by weight, At any time, the tree with the
smallest weight is either the header of the queue, or the first element of the
list. Denote the header of the queue is $T_a$, after pops it, the queue is $Q'$;
The first element in $A$ is $T_b$, the rest elements are hold in $A'$.
Function $extract''$ can be defined like the following.

\be
extract''(Q, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (T_b, (Q, A')) & Q = \Phi \\
  (T_a, (Q', A)) & A = \Phi \lor T_a < T_b \\
  (T_b, (Q, A')) & otherwise
  \end{array}
\right.
\ee

Actually, the pair of queue and tree list can be viewed as a special heap.
The tree with the minimum weight is continuously extracted and merged.

\be
\begin{array}{l}
reduce'(T, (Q, A)) = \\
\left \{
  \begin{array}
  {r@{\quad:\quad}l}
  T & Q = \Phi \land A = \Phi \\
  reduce'(extract''(push(Q'', merge(T, T')), A'')) & otherwise
  \end{array}
\right.
\end{array}
\ee

Where $(T', (Q'', A'')) = extract''(Q, A)$, which means extracting another
tree. The following Haskell example program shows the implementation of
this method. Note that this program explicitly sort the leaves, which isn't
necessary if the leaves are ordered. Again, the list, but not a real
queue is used here for illustration purpose. List isn't good at pushing
new element, please refer to the chapter of queue for details about
it.

\lstset{language=Haskell}
\begin{lstlisting}
huffman'' :: (Num a, Ord a) => [(b, a)] -> HTr a b
huffman'' = reduce . wrap . sort . map (\(c, w) -> Leaf w c) where
  wrap xs = delMin ([], xs)
  reduce (x, ([], [])) = x
  reduce (x, h) = let (y, (q, xs)) = delMin h in
                  reduce $ delMin (q ++ [merge x y], xs)
  delMin ([], (x:xs)) = (x, ([], xs))
  delMin ((q:qs), []) = (q, (qs, []))
  delMin ((q:qs), (x:xs)) | q < x = (q, (qs, (x:xs)))
                          | otherwise = (x, ((q:qs), xs))
\end{lstlisting} %$

This algorithm can also be realized imperatively.

\begin{algorithmic}[1]
\Function{Huffman''}{$A$} \Comment{$A$ is ordered by weight}
  \State $Q \gets \Phi$
  \State $T \gets$ \Call{Extract}{$Q, A$}
  \While{$Q \neq \Phi \lor A \neq \Phi$}
    \State \textproc{Push}($Q$, \textproc{Merge}($T$, \Call{Extract}{$Q, A$}))
    \State $T \gets$ \Call{Extract}{$Q, A$}
  \EndWhile
  \State \Return $T$
\EndFunction
\end{algorithmic}

Where function \textproc{Extract}($Q, A$) extracts the tree with the smallest weight from
the queue and the list. It mutates the queue and tree if necessary. Denote
the head of the queue is $T_a$, and the first element of the list as $T_b$.

\begin{algorithmic}[1]
\Function{Extract}{$Q, A$}
  \If{$Q \neq \Phi \land (A = \Phi \lor T_a < T_b)$}
    \State \Return \Call{Pop}{$Q$}
  \Else
    \State \Return \Call{Detach}{$A$}
  \EndIf
\EndFunction
\end{algorithmic}

Where procedure \textproc{Detach}($A$), removes the first element from $A$,
and returns this element as result. In most imperative settings, as detaching
the first element is slow linear operation for array, we can store the trees
in descending order by weight, and remove the last element. This is a fast
constant time operation. The below C++ example code shows this idea.

\lstset{language=C++}
\begin{lstlisting}
Node* extract(queue<Node*>& q, Nodes& ts) {
    Node* t;
    if (!q.empty() && (ts.empty() || lessp(q.front(), ts.back()))) {
        t = q.front();
        q.pop();
    } else {
        t = ts.back();
        ts.pop_back();
    }
    return t;
}

Node* huffman2(Nodes ts) {
    queue<Node*> q;
    sort(ts.begin(), ts.end(), greaterp);
    Node* t = extract(q, ts);
    while (!q.empty() || !ts.empty()) {
        q.push(merge(t, extract(q, ts)));
        t = extract(q, ts);
    }
    return t;
}
\end{lstlisting}

Note that the sorting isn't necessary if the trees have already been ordered.
It can be a linear time reversing in case the trees are in ascending order by
weight.

There are three different Huffman man tree building methods explained. Although
they follow the same approach developed by Huffman, the result trees varies.
Figure \ref{fig:huffman-vars} shows the three different Huffman trees built
with these methods.

\begin{figure}[htbp]
 \centering
 \subfloat[Created by scan method.]{ \includegraphics[scale=0.3]{img/huffman-tr-v1.ps}}
 \subfloat[Created by heap method.]{ \includegraphics[scale=0.3]{img/huffman-tr.ps}} \\
 \subfloat[Linear time building for sorted list.]{ \includegraphics[scale=0.3]{img/huffman-tr-v3.ps}} \\
 \caption{Variation of Huffman trees for the same symbol list.}
 \label{fig:huffman-vars}
\end{figure}

Although these three trees are not identical. They are all able to generate the
most efficient code. The formal proof is skipped here. The detailed information
can be referred to \cite{Huffman} and Section 16.3 of \cite{CLRS}.

The Huffman tree building is the core idea of Huffman coding. Many things can be
easily achieved with the Huffman tree. For example, the code table can be generated
by traversing the tree. We start from the root with the empty prefix $p$.
For any branches,
we append a zero to the prefix if turn left, and append a one if turn right. When
a leaf node is arrived, the symbol represented by this node and the prefix are put
to the code table. Denote the symbol of a leaf node as $c$, the children for tree
$T$ as $T_l$ and $T_r$
respectively. The code table association list can be built with $code(T, \Phi)$,
which is defined as below.

\be
code(T, p) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{(c, p)\} & leaf(T) \\
  code(T_l, p \cup \{0\}) \cup code(T_r, p \cup \{1\}) & otherwise
  \end{array}
\right.
\ee

Where function $leaf(T)$ tests if tree $T$ is a leaf or a branch node.
The following Haskell example program generates a map as the code table according to
this algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
code tr = Map.fromList $ traverse [] tr where
  traverse bits (Leaf _ c) = [(c, bits)]
  traverse bits (Branch _ l r) = (traverse (bits ++ [0]) l) ++
                                 (traverse (bits ++ [1]) r)
\end{lstlisting} %$

The imperative code table generating algorithm is left as exercise.
The encoding process can scan the text, and look up the code table to output the bit
sequence. The realization is skipped here.

The decoding process is realized by looking up the Huffman tree according to the bit
sequence. We start from the root, whenever a zero is received, we turn left, otherwise
if a one is received, we turn right. If a leaf node is arrived, the symbol represented
by this leaf is output, and we start another looking up from the root. The decoding
process ends when all the bits are consumed. Denote the bit sequence as
$B = \{b_1, b_2, ...\}$, all bits except the first one are hold in $B'$, below
definition realizes the decoding algorithm.

\be
decode(T, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{c\} & B = \Phi \land leaf(T) \\
  \{c\} \cup decode(root(T), B) & leaf(T) \\
  decode(T_l, B') & b_1 = 0 \\
  decode(T_r, B') & otherwise
  \end{array}
\right.
\ee

Where $root(T)$ returns the root of the Huffman tree. The following Haskell example
code implements this algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
decode tr cs = find tr cs where
  find (Leaf _ c) [] = [c]
  find (Leaf _ c) bs = c : find tr bs
  find (Branch _ l r) (b:bs) = find (if b == 0 then l else r) bs
\end{lstlisting}

Note that this is an on-line decoding algorithm with linear time performance. It consumes
one bit per time. This can be clearly noted from the below imperative
realization, where the index keeps increasing by one.

\begin{algorithmic}[1]
\Function{Decode}{$T, B$}
  \State $W \gets \Phi$
  \State $n \gets |B|, i \gets 1$
  \While{$i < n$}
    \State $R \gets T$
    \While{$\lnot$ \Call{Leaf}{$R$}}
      \If{$B[i] = 0$}
        \State $R \gets$ \Call{Left}{$R$}
      \Else
        \State $R \gets$ \Call{Right}{$R$}
      \EndIf
      \State $i \gets i + 1$
    \EndWhile
    \State $W \gets W \cup$ \Call{Symbol}{$R$}
  \EndWhile
  \State \Return $W$
\EndFunction
\end{algorithmic}

This imperative algorithm can be implemented as the following example C++ program.

\lstset{language=C++}
\begin{lstlisting}
string decode(Node* root, const char* bits) {
    string w;
    while (*bits) {
        Node* t = root;
        while (!isleaf(t))
            t = '0' == *bits++ ? t->left : t->right;
        w += t->c;
    }
    return w;
}
\end{lstlisting}

Huffman coding, especially the Huffman tree building shows an interesting strategy.
Each time, there are multiple options for merging. Among the trees in the list,
Huffman method always selects two trees with the smallest weight. This is the
best choice at that merge stage. However, these series of {\em local} best options
generate a global optimal prefix code.

It's not always the case that the local optimal choice also leads to the global optimal
solution. In most cases, it doesn't. Huffman coding is a special one. We call
the strategy that always choosing the local best option as {\em greedy} strategy.

Greedy method works for many problems. However, it's not easy to tell if the greedy
method can be applied to get the global optimal solution. The generic formal proof
is still an active research area. Section 16.4 in \cite{CLRS} provides a good
treatment for Matroid tool, which covers many problems that greedy algorithm
can be applied.

\paragraph{Change-making problem}
\index{Change-making problem}
We often change money when visiting other countries. People tend to use
credit card more often nowadays than before, because it's quite convenient
to buy things without considering much about changes. If we changed
some money in the bank, there are often some foreign money left by the
end of the trip. Some people like to change them to coins for collection.
Can we find a solution, which can change the given amount of money with the
least number of coins?

Let's use USA coin system for example. There are 5 different
coins: 1 cent, 5 cent, 25 cent, 50 cent, and 1 dollar. A dollar is equal
to 100 cents. Using the greedy method introduced above, we can always
pick the largest coin which is not greater than the remaining amount of
money to be changed. Denote list $C = \{1, 5, 25, 50, 100\}$, which stands
for the value of coins. For any given money $X$, the
change coins can be generated as below.

\be
change(X, C) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & X = 0 \\
  \{ c_m \} \cup change(X-c_m, C) &
      \begin{array}{l}
        otherwise, \\
        c_m = max(\{c \in C, c \leq X\})
      \end{array}
  \end{array}
\right.
\ee

If $C$ is in descending order, $c_m$ can be found as the first
one not greater than $X$. If we want to change 1.42 dollar, This function
produces a coin list of $\{100, 25, 5, 5, 5, 1, 1\}$. The output coins
list can be easily transformed to contain pairs
$\{(100, 1), (25, 1), (5, 3), (1, 2)\}$. That we need one dollar, a quarter,
three coins of 5 cent, and 2 coins of 1 cent to make the change. The
following Haskell example program outputs result as such.

\lstset{language=Haskell}
\begin{lstlisting}
solve x = assoc . change x where
  change 0 _ = []
  change x cs = let c = head $ filter (<= x) cs in c : change (x - c) cs

assoc = (map (\cs -> (head cs, length cs))) . group
\end{lstlisting} %$

As mentioned above, this program assumes the coins are in descending order, for
instance like below.

\lstset{language=Haskell}
\begin{lstlisting}
solve 142 [100, 50, 25, 5, 1]
\end{lstlisting}

This algorithm is tail recursive, it can be transformed to
a imperative looping.

\begin{algorithmic}[1]
\Function{Change}{$X, C$}
  \State $R \gets \Phi$
  \While{$X \neq 0$}
    \State $c_m = max(\{c \in C, c \leq X\})$
    \State $R \gets \{c_m\} \cup R$
    \State $X \gets X - c_m$
  \EndWhile
  \State \Return $R$
\EndFunction
\end{algorithmic}

The following example Python program implements this imperative version
and manages the result with a dictionary.

\lstset{language=Python}
\begin{lstlisting}
def change(x, coins):
    cs = {}
    while x != 0:
        m = max([c for c in coins if c <= x])
        cs[m] = 1 + cs.setdefault(m, 0)
        x = x - m
    return cs
\end{lstlisting}

For a coin system like USA, the greedy approach can find the optimal solution.
The amount of coins is the minimum. Fortunately, our greedy method works in most
countries. But it is not always true. For example, suppose a country have
coins of value 1, 3, and 4 units. The best change for value 6, is to use two coins
of 3 units, however, the greedy method gives a result of three coins: one coin of 4,
two coins of 1. Which isn't the optimal result.

\paragraph{Summary of greedy method}
As shown in the change making problem, greedy method doesn't always give the
best result. In order to find the optimal solution, we need dynamic programming
which will be introduced in the next section.

However, the result is often good enough in practice. Let's take the word-wrap
problem for example. In modern software editors and browsers, text spans to
multiple lines if the length of the content is too long to be hold. With word-wrap
supported, user needn't hard line breaking. Although dynamic programming can wrap
with the minimum number of lines, it's overkill. On the contrary, greedy
algorithm can wrap with lines approximate to the optimal result with quite
effective realization as below. Here it wraps text $T$, not to exceeds line
width $W$, with space $s$ between each word.

\begin{algorithmic}[1]
\State $L \gets W$
\For{$w \in T$}
  \If{$|w| + s > L$}
    \State Insert line break
    \State $L \gets W - |w|$
  \Else
    \State $L \gets L - |w| - s$
  \EndIf
\EndFor
\end{algorithmic}

For each word $w$ in the text, it uses a greedy strategy to put as many words in
a line as possible unless it exceeds the line width. Many word processors use a
similar algorithm to do word-wrapping.

There are many cases, the strict optimal result, but not the approximate one is
necessary. Dynamic programming can help to solve such problems.

\subsubsection{Dynamic programming}
\index{Dynamic programming}

In the change-making problem, we mentioned the greedy method can't always give
the optimal solution. For any coin system, are there any way to find the best
changes?

Suppose we have find the best solution which makes $X$ value of money. The coins
needed are contained in $C_m$. We can partition these coins into two collections,
$C_1$ and $C_2$. They make money of $X_1$, and $X_2$ respectively. We'll prove
that $C_1$ is the optimal solution for $X_1$, and $C_2$ is
the optimal solution for $X_2$.

\begin{proof}
For $X_1$, Suppose there exists another solution $C_1'$, which uses less coins
than $C_1$. Then changing solution $C_1' \cup C_2$ uses less coins to make
$X$ than $C_m$. This is conflict with the fact that $C_m$ is the optimal
solution to $X$. Similarity, we can prove $C_2$ is the optimal solution to
$X_2$.
\end{proof}

Note that it is not true in the reverse situation. If we arbitrary select a
value $Y<X$, divide the original problem to find the optimal solutions for
sub problems $Y$ and $X-Y$. Combine the two optimal solutions doesn't
necessarily yield optimal solution for $X$. Consider this example.
There are coins with value 1, 2, and 4. The optimal solution for making
value 6, is to use 2 coins of value 2, and 4; However, if we divide
$6 = 3 + 3$, since each 3 can be made with optimal solution $3 = 1 + 2$,
the combined solution contains 4 coins (1 + 1 + 2 + 2).

If an optimal problem can be divided into several sub optimal problems,
we call it has optimal substructure. We see that the change-making
problem has optimal substructure. But the dividing has to be done
based on the coins, but not with an arbitrary value.

The optimal substructure can be expressed recursively as the following.

\be
change(X) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & X = 0 \\
  least(\{c \cup change(X-c) | c \in C, c \leq X\}) & otherwise
  \end{array}
\right.
\ee

For any coin system $C$, the changing result for zero is empty;
otherwise, we check every candidate coin $c$, which is not greater
then value $X$, and recursively find the best solution for $X-c$;
We pick the coin collection which contains the least coins as the
result.

Below Haskell example program implements this top-down recursive
solution.

\lstset{language=Haskell}
\begin{lstlisting}
change _ 0 = []
change cs x = minimumBy (compare `on` length)
                [c:change cs (x - c) | c <- cs, c <= x]
\end{lstlisting}

Although this program outputs correct answer \texttt{[2, 4]} when evaluates
\texttt{change [1, 2, 4] 6}, it performs very bad when changing 1.42 dollar
with USA coins system. It failed to find the answer within 15 minutes in a computer
with 2.7GHz CPU and 8G memory.

The reason why it's slow is because there are a lot of duplicated computing
in the top-down recursive solution. When it computes $change(142)$, it
needs to examine $change(141), change(137), change(117), change(92)$, and $change(42)$.
While $change(141)$ next computes to smaller values by deducing with 1, 2,
25, 50 and 100 cents.
it will eventually meets value 137, 117, 92, and 42 again. The search
space explodes with power of 5.

This is quite similar to compute Fibonacci numbers in a top-down recursive
way.

\be
F_n = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  1 & n = 1 \lor n = 2 \\
  F_{n-1} + F_{n-2} & otherwise
  \end{array}
\right.
\ee

When we calculate $F_8$ for example, we recursively calculate $F_7$ and $F_6$.
While when we calculate $F_7$, we need calculate $F_6$ again, and $F_5$, ...
As shown in the below expand forms, the calculation is doubled every time,
and the same value is calculate again and again.

\[
\begin{array}{rl}
F_8 = & F_7 + F_6 \\
   = & F_6 + F_5 + F_5 + F_4 \\
   = & F_5 + F_4 + F_4 + F_3 + F_4 + F_3 + F_3 + F_2 \\
   = & ...
\end{array}
\]

In order to avoid duplicated computation, a table $F$ can be maintained
when calculating the Fibonacci numbers.
The first two elements are filled as 1, all others are left blank. During
the top-down recursive calculation, If need $F_k$, we first look up this
table for the $k$-th cell, if it isn't blank, we use that value directly.
Otherwise we need further calculation. Whenever a value is calculated,
we store it in the corresponding cell for looking up in the future.

\begin{algorithmic}[1]
\State $F \gets \{1, 1, NIL, NIL, ...\}$
\Function{Fibonacci}{$n$}
  \If{$n > 2 \land F[n] = NIL$}
    \State $F[n] \gets$ \Call{Fibonacci}{$n-1$} $+$ \Call{Fibonacci}{$n-2$}
  \EndIf
  \State \Return $F[n]$
\EndFunction
\end{algorithmic}

By using the similar idea, we can develop a new top-down change-making
solution. We use a table $T$ to maintain the best changes, it is initialized
to all empty coin list. During the top-down recursive computation,
we look up this table for smaller changing values. Whenever a intermediate
value is calculated, it is stored in the table.

\begin{algorithmic}[1]
\State $T \gets \{\Phi, \Phi, ...\}$
\Function{Change}{$X$}
  \If{$X > 0 \land T[X] = \Phi$}
    \For{$c \in C$}
      \If{$c \leq X$}
        \State $C_m \gets \{c\} \cup$ \Call{Change}{$X-c$}
        \If{$T[X] = \Phi \lor |C_m| < |T[X]|$}
          \State $T[X] \gets C_m$
        \EndIf
      \EndIf
    \EndFor
  \EndIf
  \State \Return $T[X]$
\EndFunction
\end{algorithmic}

The solution to change 0 money is definitely empty $\Phi$, otherwise, we look up
$T[X]$ to retrieve the solution to change $X$ money. If it is empty, we need
recursively calculate it. We examine all coins in the coin system $C$ which is
not greater than $X$. This is the sub problem of making changes for money $X-c$.
The minimum amount of coins plus one coin of $c$ is stored in $T[X]$ finally
as the result.

The following example Python program implements this algorithm just takes 8000 ms
to give the answer of changing 1.42 dollar in US coin system.

\lstset{language=Python}
\begin{lstlisting}
tab = [[] for _ in range(1000)]

def change(x, cs):
    if x > 0 and tab[x] == []:
        for s in [[c] + change(x - c, cs) for c in cs if c <= x]:
            if tab[x] == [] or len(s) < len(tab[x]):
                tab[x] = s
    return tab[x]
\end{lstlisting}

Another solution to calculate Fibonacci number, is to compute them in order
of $F_1, F_2, F_3, ..., F_n$. This is quite natural when people write down
Fibonacci series.

\begin{algorithmic}[1]
\Function{Fibo}{$n$}
  \State $F = \{1, 1, NIL, NIL, ...\}$
  \For{$i \gets 3$ to $n$}
    \State $F[i] \gets F[i-1] + F[i-2]$
  \EndFor
  \State \Return $F[n]$
\EndFunction
\end{algorithmic}

We can use the quite similar idea to solve the change making problem.
Starts from zero money, which can be changed by an empty list of coins, we next
try to figure out how to change money of value 1. In US coin system for example,
A cent can be used; The next values of 2, 3, and 4, can be changed by
two coins of 1 cent, three coins of 1 cent, and 4 coins of 1 cent.
At this stage, the solution table looks like below

\begin{tabular}{|c|c|c|c|c|}
\hline
0 & 1 & 2 & 3 & 4 \\
\hline
$\Phi$ & $\{1\}$ & $\{1, 1\}$ & $\{1, 1, 1\}$ & $\{1, 1, 1, 1\}$ \\
\hline
\end{tabular}

The interesting case happens for changing value 5. There are two options,
use another coin of 1 cent, which need 5 coins in total; The other way
is to use 1 coin of 5 cent, which uses less coins than the former.
So the solution table can be extended to this.

\begin{tabular}{|c|c|c|c|c|c|}
\hline
0 & 1 & 2 & 3 & 4 & 5 \\
\hline
$\Phi$ & $\{1\}$ & $\{1, 1\}$ & $\{1, 1, 1\}$ & $\{1, 1, 1, 1\}$ & $\{ 5 \}$ \\
\hline
\end{tabular}

For the next change value 6, since there are two types of coin, 1 cent
and 5 cent, are less than this value, we need examine both of them.
\begin{itemize}
\item If we choose the 1 cent coin, we need next make changes for 5; Since
we've already known that the best solution to change 5 is $\{ 5 \}$,
which only needs a coin of 5 cents, by looking up the solution table,
we have one candidate solution to change 6 as $\{5, 1\}$;
\item The other option is to choose the 5 cent coin, we need next make
changes for 1; By looking up the solution table we've filled so far,
the sub optimal solution to change 1 is $\{ 1 \}$. Thus we get another
candidate solution to change 6 as $\{1, 5\}$;
\end{itemize}

It happens that, both options yield a solution of two coins, we can
select either of them as the best solution. Generally speaking, the
candidate with fewest number of coins is selected as the solution,
and filled into the table.

At any iteration, when we are trying to change the $i < X$ value of money,
we examine all the types of coin. For any coin $c$ not greater than $i$,
we look up the solution table to fetch the sub solution $T[i-c]$. The
number of coins in this sub solution plus the one coin of $c$ are the
total coins needed in this candidate solution. The fewest candidate
is then selected and updated to the solution table.

The following algorithm realizes this bottom-up idea.

\begin{algorithmic}[1]
\Function{Change}{$X$}
  \State $T \gets \{ \Phi, \Phi, ... \}$
  \For{$i \gets 1$ to $X$}
    \For{$c \in C, c \leq i$}
      \If{$T[i] = \Phi \lor 1 + |T[i - c]| < |T[i]|$}
        \State $T[i] \gets \{ c \} \cup T[i-c]$
      \EndIf
    \EndFor
  \EndFor
  \State \Return $T[X]$
\EndFunction
\end{algorithmic}

This algorithm can be directly translated to imperative programs,
like Python for example.

\lstset{language=Python}
\begin{lstlisting}
def changemk(x, cs):
    s = [[] for _ in range(x+1)]
    for i in range(1, x+1):
        for c in cs:
            if c <= i and (s[i] == [] or 1 + len(s[i-c]) < len(s[i])):
                s[i] = [c] + s[i-c]
    return s[x]
\end{lstlisting}

Observe the solution table, it's easy to find that, there are many duplicated
contents being stored.

\begin{tabular}{|c|c|c|c|c|c|}
\hline
6 & 7 & 8 & 9 & 10 & ... \\
\hline
$\{ 1, 5 \}$ & $\{1, 1, 5\}$ & $\{1, 1, 1, 5\}$ & $\{1, 1, 1, 1, 5\}$ & $\{ 5, 5 \}$ & ... \\
\hline
\end{tabular}

This is because the optimal sub solutions are
completely copied and saved in parent solution.
In order to use less space, we can only record the `delta' part
from the sub optimal solution. In change-making problem, it means that
we only need to record the coin being selected for value $i$.

\begin{algorithmic}[1]
\Function{Change'}{$X$}
  \State $T \gets \{ 0, \infty, \infty, ... \}$
  \State $S \gets \{ NIL, NIL, ... \}$
  \For{$i \gets 1$ to $X$}
    \For{$c \in C, c \leq i$}
      \If{$1 + T[i - c] < T[i]$}
        \State $T[i] \gets 1 + T[i-c]$
        \State $S[i] \gets c$
      \EndIf
    \EndFor
  \EndFor
  \While{$X > 0$}
    \State \Call{Print}{$S[X]$}
    \State $X \gets X - S[X]$
  \EndWhile
\EndFunction
\end{algorithmic}

Instead of recording the complete solution list of coins, this new algorithm
uses two tables $T$ and $S$. $T$ holds the minimum number of coins needed for
changing value 0, 1, 2, ...; while $S$ holds the first coin being selected
for the optimal solution. For the complete coin list to change
money $X$, the first coin is thus $S[X]$, the sub optimal solution is
to change money $X' = X-S[X]$. We can look up table $S[X']$ for the
next coin. The coins for sub optimal solutions are repeatedly looked
up like this till the beginning of the table. Below Python example program
implements this algorithm.

\lstset{language=Python}
\begin{lstlisting}
def chgmk(x, cs):
    cnt = [0] + [x+1] * x
    s = [0]
    for i in range(1, x+1):
        coin = 0
        for c in cs:
            if c <= i and 1 + cnt[i-c] < cnt[i]:
                cnt[i] = 1 + cnt[i-c]
                coin = c
        s.append(coin)
    r = []
    while x > 0:
        r.append(s[x])
        x = x - s[x]
    return r
\end{lstlisting}

This change-making solution loops $n$ times for given money $n$. It examines
at most the full coin system in each iteration. The time is bound to $\Theta(nk)$
where $k$ is the number of coins for a certain coin system. The last algorithm
adds $O(n)$ spaces to record sub optimal solutions with table $T$ and $S$.

In purely functional settings, There is no means to mutate the solution table
and look up in constant time. One alternative is to use finger tree as we mentioned
in previous chapter \footnote{Some purely functional programming environments,
Haskell for instance, provide built-in array; while other almost pure ones,
such as ML, provide mutable array}. We can store the minimum number of coins,
and the coin leads to the sub optimal solution in pairs.

The solution table, which is a finger tree, is initialized as $T = \{(0, 0)\}$.
It means change 0 money need no coin. We can fold on list $\{1, 2, ..., X\}$,
start from this table, with a binary function $change(T, i)$. The folding
will build the solution table, and we can construct the coin list from this
table by function $make(X, T)$.

\be
makeChange(X) = make(X, fold(change, \{(0, 0)\}, \{1, 2, ..., X\}))
\ee

In function $change(T, i)$, all the coins not greater than $i$ are examined
to select the one lead to the best result. The fewest number of coins, and
the coin being selected are formed to a pair. This pair is inserted to the
finger tree, so that a new solution table is returned.

\be
change(T, i) = insert(T, fold(sel, (\infty, 0), \{c | c \in C, c \leq i\}))
\ee

Again, folding is used to select the candidate with the minimum
number of coins. This folding starts with initial value $(\infty, 0)$, on
all valid coins. function $sel((n, c), c')$ accepts two arguments, one is
a pair of length and coin, which is the best solution so far;
the other is a candidate coin, it examines if this candidate can make
better solution.

\be
sel((n, c), c') = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (1 + n', c') & 1 + n' < n, (n', c') = T[i-c'] \\
  (n, c) & otherwise
  \end{array}
\right.
\ee

After the solution table is built, the coins needed can be generated from it.

\be
make(X, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & X = 0 \\
  \{c\} \cup make(X-c, T) & otherwise, (n, c) = T[X]
  \end{array}
\right.
\ee

The following example Haskell
program uses \texttt{Data.Sequence}, which is the library of finger tree,
to implement change making solution.

\lstset{language=Haskell}
\begin{lstlisting}
import Data.Sequence (Seq, singleton, index, (|>))

changemk x cs = makeChange x $ foldl change (singleton (0, 0)) [1..x] where
  change tab i = let sel c = min (1 + fst (index tab (i - c)), c)
                 in tab |> (foldr sel ((x + 1), 0) $ filter (<=i) cs)
  makeChange 0 _ = []
  makeChange x tab = let c = snd $ index tab x in c : makeChange (x - c) tab
\end{lstlisting} %$

It's necessary to memorize the optimal solution to sub problems no matter using
the top-down or the bottom-up approach. This is because a sub problem is used many
times when computing the overall optimal solution. Such properties are called
overlapping sub problems.

\paragraph{Properties of dynamic programming}
Dynamic programming was originally named by Richard Bellman in 1940s. It is a
powerful tool to search for optimal solution for problems with two properties.

\begin{itemize}
\item Optimal sub structure. The problem can be broken down into smaller problems, and
the optimal solution can be constructed efficiently from solutions of these sub problems;
\item Overlapping sub problems. The problem can be broken down into sub problems which
are reused several times in finding the overall solution.
\end{itemize}

The change-making problem, as we've explained, has both optimal sub structures, and
overlapping sub problems.

\paragraph{Longest common subsequence problem}
\index{LCS} \index{Longest common subsequence problem}

The longest common subsequence problem, is different with the longest common substring
problem. We've show how to solve the later in the chapter of suffix tree. The
longest common subsequence needn't be consecutive part of the original sequence.

For example, The longest common substring for text ``Mississippi'', and ``Missunderstanding''
is ``Miss'', while the longest common subsequence for them are ``Misssi''.
This is shown in figure \ref{fig:lcs}.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/lcs.eps}
 \caption{The longest common subsequence}
 \label{fig:lcs}
\end{figure}

If we rotate the figure vertically, and consider the two texts as two pieces of
source code, it turns to be a `diff' result between them.
Most modern version control tools need calculate the difference content among the
versions. The longest common subsequence problem plays a very important role.

If either one of the two strings $X$ and $Y$ is empty, the longest
common subsequence $LCS(X, Y)$ is definitely empty;
Otherwise, denote $X = \{x_1, x_2, ..., x_n\}$,
$Y = \{y_1, y_2, ..., y_m \}$, if the first elements $x_1$ and $y_1$ are same, we can
recursively find the longest subsequence of $X' = \{x_2, x_3, ..., x_n \}$ and
$Y' = \{y_2, y_3, ..., y_m \}$. And the final result $LCS(X, Y)$ can be constructed
by concatenating $x_1$ with $LCS(X', Y')$; Otherwise if $x_1 \neq y_1$, we need
recursively find the longest common subsequences of $LCS(X, Y')$ and $LCS(X', Y)$,
and pick the longer one as the final result. Summarize these cases gives
the below definition.

\be
LCS(X, Y) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & X = \Phi \lor Y = \Phi \\
  \{x_1\} \cup LCS(X', Y') & x_1 = y_1 \\
  longer(LCS(X, Y'), LCS(X', Y)) & otherwise
  \end{array}
\right.
\ee

Note that this algorithm shows clearly the optimal substructure, that the
longest common subsequence problem can be broken to smaller problems. The
sub problem is ensured to be at least one element shorter than the original
one.

It's also clear that, there are overlapping sub-problems. The longest
common subsequences to the sub strings are used multiple times in finding
the overall optimal solution.

The existence of these two properties, the optimal substructure and the
overlapping sub-problem, indicates the dynamic programming can be used
to solve this problem.

A 2-dimension table can be used to record the solutions to the sub-problems.
The rows and columns represent the substrings of $X$ and $Y$ respectively.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 & & a & n & t & e & n & n & a \\
\hline
 & & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
\hline
b & 1 & & & & & & & \\
\hline
a & 2 & & & & & & & \\
\hline
n & 3 & & & & & & & \\
\hline
a & 4 & & & & & & & \\
\hline
n & 5 & & & & & & & \\
\hline
a & 6 & & & & & & & \\
\hline
\end{tabular}

This table shows an example of finding the longest common subsequence for
strings ``antenna'' and ``banana''. Their lengths are 7, and 6. The right bottom
corner of this table is looked up first, Since it's empty we need compare
the 7th element in ``antenna'' and the 6th in ``banana'', they are both
`a', Thus we need next recursively look up the cell at row 5, column 6;
It's still empty, and we repeated this till either get a trivial case
that one substring becomes empty, or some cell we are looking up has
been filled before. Similar to the change-making problem, whenever the
optimal solution for a sub-problem is found, it is recorded in the cell
for further reusing. Note that this process is in the reversed order
comparing to the recursive equation given above, that we start from the
right most element of each string.

Considering that the longest common subsequence for any empty string is
still empty, we can extended the solution table so that the first row
and column hold the empty strings.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 & & a & n & t & e & n & n & a \\
\hline
 & & $\Phi$ & $\Phi$ & $\Phi$ & $\Phi$ & $\Phi$ & $\Phi$ & $\Phi$ \\
\hline
b & $\Phi$ & & & & & & & \\
\hline
a & $\Phi$ & & & & & & & \\
\hline
n & $\Phi$ & & & & & & & \\
\hline
a & $\Phi$ & & & & & & & \\
\hline
n & $\Phi$ & & & & & & & \\
\hline
a & $\Phi$ & & & & & & & \\
\hline
\end{tabular}

Below algorithm realizes the top-down recursive dynamic programming
solution with such a table.

\begin{algorithmic}[1]
\State $T \gets$ NIL
\Function{LCS}{$X, Y$}
  \State $m \gets |X|, n \gets |Y|$
  \State $m' \gets m+1, n' \gets n+1$
  \If{$T =$ NIL}
    \State $T \gets \{\{\Phi, \Phi, ..., \Phi\}, \{\Phi, NIL, NIL, ...\}, ...\}$ \Comment{$m' \times n'$}
  \EndIf
  \If{$X \neq \Phi \land Y \neq \Phi \land T[m'][n'] =$ NIL}
    \If{$X[m] = Y[n]$}
      \State $T[m'][n'] \gets$ \textproc{Append}(\Call{LCS}{$X[1..m-1], Y[1..n-1]$}, $X[m]$)
    \Else
      \State $T[m'][n'] \gets$ \textproc{Longer}(\Call{LCS}{$X, Y[1..n-1]$}, \Call{LCS}{$X[1.. m-1], Y$})
    \EndIf
  \EndIf
  \State \Return $T[m'][n']$
\EndFunction
\end{algorithmic}

The table is firstly initialized with the first row and column filled with empty strings; the rest
are all NIL values. Unless either string is empty, or the cell content isn't NIL, the last two
elements of the strings are compared, and recursively computes the longest common subsequence
with substrings. The following Python example program implements this algorithm.

\lstset{language=Python}
\begin{lstlisting}
def lcs(xs, ys):
    m = len(xs)
    n = len(ys)
    global tab
    if tab is None:
        tab = [[""]*(n+1)] + [[""] + [None]*n for _ in xrange(m)]
    if m != 0 and n !=0 and tab[m][n] is None:
        if xs[-1] == ys[-1]:
            tab[m][n] = lcs(xs[:-1], ys[:-1]) + xs[-1]
        else:
            (a, b) = (lcs(xs, ys[:-1]), lcs(xs[:-1], ys))
            tab[m][n] = a if len(b) < len(a) else b
    return tab[m][n]
\end{lstlisting}

The longest common subsequence can also be found in a bottom-up manner as what we've done
with the change-making problem. Besides that, instead of recording the whole sequences
in the table, we can just store the lengths of the longest subsequences, and later construct
the subsubsequence with this table and the two strings. This time, the table is
initialized with all values set as 0.

\begin{algorithmic}[1]
\Function{LCS}{$X, Y$}
  \State $m \gets |X|, n \gets |Y|$
  \State $T \gets \{\{0, 0, ...\}, \{0, 0, ...\}, ...\}$ \Comment{$(m+1) \times (n+1)$}
  \For{$i \gets 1$ to $m$}
    \For{$j \gets 1$ to $n$}
      \If{$X[i] = Y[j]$}
        \State $T[i+1][j+1] \gets T[i][j] + 1$
      \Else
        \State $T[i+1][j+1] \gets$ \Call{Max}{$T[i][j+1], T[i+1][j]$}
      \EndIf
    \EndFor
  \EndFor
  \State \Return \Call{Get}{$T, X, Y, m, n$}
\EndFunction
\Statex
\Function{Get}{$T, X, Y, i, j$}
  \If{$i = 0 \lor j = 0$}
    \State \Return $\Phi$
  \ElsIf{$X[i] = Y[j]$}
    \State \Return \textproc{Append}(\Call{Get}{$T, X, Y, i-1, j-1$}, $X[i]$)
  \ElsIf{$T[i-1][j] > T[i][j-1]$}
    \State \Return \Call{Get}{$T, X, Y, i-1, j$}
  \Else
    \State \Return \Call{Get}{$T, X, Y, i, j-1$}
  \EndIf
\EndFunction
\end{algorithmic}

In the bottom-up approach, we start from the cell at the second row and the second column.
The cell is corresponding to the first element in both $X$, and $Y$. If they are same,
the length of the longest common subsequence so far is 1. This can be yielded by increasing the
length of empty sequence, which is stored in the top-left cell, by one; Otherwise,
we pick the maximum value from the upper cell and left cell. The table is repeatedly
filled in this manner.

After that, a back-track is performed to construct the longest common subsequence. This
time we start from the bottom-right corner of the table. If the last elements in $X$
and $Y$ are same, we put this element as the last one of the result, and
go on looking up the cell along the diagonal line; Otherwise, we compare the values in
the left cell and the right cell, and go on looking up the cell with the bigger value.

The following example Python program implements this algorithm.

\lstset{language=Python}
\begin{lstlisting}
def lcs(xs, ys):
    m = len(xs)
    n = len(ys)
    c = [[0]*(n+1) for _ in xrange(m+1)]
    for i in xrange(1, m+1):
        for j in xrange(1, n+1):
            if xs[i-1] == ys[j-1]:
                c[i][j] = c[i-1][j-1] + 1
            else:
                c[i][j] = max(c[i-1][j], c[i][j-1])

    return get(c, xs, ys, m, n)

def get(c, xs, ys, i, j):
    if i==0 or j==0:
        return []
    elif xs[i-1] == ys[j-1]:
        return get(c, xs, ys, i-1, j-1) + [xs[i-1]]
    elif c[i-1][j] > c[i][j-1]:
        return get(c, xs, ys, i-1, j)
    else:
        return get(c, xs, ys, i, j-1)
\end{lstlisting}

The bottom-up dynamic programming solution can also be defined in purely functional way.
The finger tree can be used as a table. The first row is filled with $n+1$ zero values.
This table can be built by folding on sequence $X$. Then the longest common subsequence
is constructed from the table.

\be
LCS(X, Y) = construct(fold(f, \{\{0, 0, ..., 0\}\}, zip(\{1, 2, ...\}, X)))
\ee

Note that, since the table need be looked up by index, $X$ is zipped with natural numbers.
Function $f$ creates a new row of this table by folding on sequence $Y$, and records
the lengths of the longest common sequence for all possible cases so far.

\be
f(T, (i, x)) = insert(T, fold(longest, \{0\}, zip(\{1, 2, ...\}, Y)))
\ee

Function $longest$ takes the intermediate filled row result, and a
pair of index and element in $Y$, it compares if this element is
the same as the one in $X$. Then fills the new cell with the length
of the longest one.

\be
longest(R, (j, y)) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  insert(R, 1 + T[i-1][j-1]  ) & x = y \\
  insert(R, max(T[i-1][j], T[i][j-1])) & otherwise
  \end{array}
\right.
\ee

After the table is built. The longest common sub sequence can be constructed recursively
by looking up this table. We can pass the reversed sequences $\overleftarrow{X}$, and
$\overleftarrow{Y}$ together with their lengths $m$ and $n$ for efficient building.

\be
construct(T) = get((\overleftarrow{X}, m), (\overleftarrow{Y}, n))
\ee

If the sequences are not empty, denote the first elements as $x$ and $y$. The rest elements
are hold in $\overleftarrow{X}'$ and $\overleftarrow{Y}'$ respectively. The function
$get$ can be defined as the following.

\be
get((\overleftarrow{X}, i), (\overleftarrow{Y}, j)) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & \overleftarrow{X} = \Phi \land \overleftarrow{Y} = \Phi \\
  get((\overleftarrow{X}', i-1), (\overleftarrow{Y}', j-1)) \cup \{x\} & x = y \\
  get((\overleftarrow{X}', i-1), (\overleftarrow{Y}, j)) & T[i-1][j] > T[i][j-1] \\
  get((\overleftarrow{X}, i), (\overleftarrow{Y}', j-1)) & otherwise
  \end{array}
\right.
\ee

Below Haskell example program implements this solution.

\lstset{language=Haskell}
\begin{lstlisting}
lcs' xs ys = construct $ foldl f (singleton $ fromList $ replicate (n+1) 0)
                               (zip [1..] xs) where
  (m, n) = (length xs, length ys)
  f tab (i, x) = tab |> (foldl longer (singleton 0) (zip [1..] ys)) where
    longer r (j, y) = r |> if x == y
                    then 1 + (tab `index` (i-1) `index` (j-1))
                    else max (tab `index` (i-1) `index` j) (r `index` (j-1))
  construct tab = get (reverse xs, m) (reverse ys, n) where
    get ([], 0) ([], 0) = []
    get ((x:xs), i) ((y:ys), j)
      | x == y = get (xs, i-1) (ys, j-1) ++ [x]
      | (tab `index` (i-1) `index` j) > (tab `index` i `index` (j-1)) =
                 get (xs, i-1) ((y:ys), j)
      | otherwise = get ((x:xs), i) (ys, j-1)
\end{lstlisting}

\paragraph{Subset sum problem}
\index{Subset sum problem}
Dynamic programming does not limit to solve the optimization problem, but
can also solve some more general searching problems. Subset sum
problem is such an example. Given a set of integers, is there a non-empty
subset sums to zero? for example, there are
two subsets of
$\{11, 64, -82, -68, 86, 55, -88, -21, 51\}$ both sum to zero. One
is $\{64, -82, 55, -88, 51\}$, the other is $\{64, -82, -68, 86\}$.

Of course summing to zero is a special
case, because sometimes, people want to find a subset, whose sum is a
given value $s$. Here we are going to develop a method to find all the
candidate subsets.

There is obvious a brute-force exhausting search solution. For every
element, we can either pick it or not. So there are total $2^n$ options
for set with $n$ elements. Because for every selection, we need
check if it sums to $s$. This is a linear operation. The overall
complexity is bound to $O(n2^n)$. This is the exponential algorithm, which
takes very huge time if the set is big.

There is a recursive solution to subset sum problem. If the set is empty,
there is no solution definitely; Otherwise, let the set is $X = \{x_1, x_2, ...\}$.
If $x_1 = s$, then subset $\{x_1\}$ is a solution, we need
next search for subsets $X' = \{x_2, x_3, ...\}$ for those sum to $s$;
Otherwise if $x_1 \neq s$, there are two different kinds of possibilities.
We need search $X'$ for both sum $s$, and sum $s-x_1$. For any subset
sum to $s-x_1$, we can add $x_1$ to it to form a new set as a solution.
The following equation defines this algorithm.

\be
solve(X, s) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & X = \Phi \\
  \{\{x_1\}\} \cup solve(X', s) & x_1 = s \\
  solve(X', s) \cup \{ \{x_1\} \cup S | S \in solve(X', s-x_1)\} & otherwise
  \end{array}
\right.
\ee

There are clear substructures in this definition, although they are
not in a sense of optimal. And there are also overlapping sub-problems. This
indicates the problem can be solved with dynamic programming with a table
to memorize the solutions to sub-problems.

Instead of developing a solution to output all the
subsets directly, let's consider how to give the existence
answer firstly. That output 'yes' if there exists some subset sum to $s$, and
'no' otherwise.

One fact is that, the upper and lower limit for all possible answer can
be calculated in one scan. If the given sum $s$ doesn't belong to this
range, there is no solution obviously.

\be
\left \{
  \begin{array}{l}
  s_l = \sum \{x \in X, x < 0\} \\
  s_u = \sum \{x \in X, x > 0\}
  \end{array}
\right.
\ee

Otherwise, if $s_l \leq s \leq s_u$, since the values are all integers, we
can use a table, with $s_u - s_l + 1$ columns, each column represents a
possible value in this range, from $s_l$ to $s_u$.
The value of the cell is either true or false to represents if there exists
subset sum to this value. All cells are initialized as false.
Starts from the first element $x_1$ in $X$, definitely, set $\{x_1\}$
can sum to $x_1$, so that the cell represents this value in the first row
can be filled as true.

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & $s_l$ & $s_l+1$ & ... & $x_1$ & ... & $s_u$ \\
\hline
$x_1$ & F & F & ... & T & ... & F \\
\hline
\end{tabular}

With the next element $x_2$, There are three possible sums.
Similar as the first row, $\{x_2\}$ sums to $x_2$; For all possible sums
in previous row, they can also been achieved without $x_2$. So the cell
below to $x_1$ should also be filled as true; By adding $x_2$ to all
possible sums so far, we can also get some new values. That the cell
represents $x_1 + x_2$ should be true.

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 & $s_l$ & $s_l+1$ & ... & $x_1$ & ... & $x_2$ & ... & $x_1 + x_2$ & ... & $s_u$ \\
\hline
$x_1$ & F & F & ... & T & ... & F & ... & F & ... & F \\
\hline
$x_2$ & F & F & ... & T & ... & T & ... & T & ... & F \\
\hline
\end{tabular}

Generally speaking, when fill the $i$-th row, all the possible sums
constructed with $\{x_1, x_2, ..., x_{i-1}$ so far
can also be achieved with $x_i$. So the cells previously are true
should also be true in this new row. The cell represents value $x_i$
should also be true since the singleton set $\{x_i\}$ sums to it.
And we can also adds $x_i$ to all previously constructed sums to
get the new results. Cells represent these new sums should also
be filled as true.

When all the elements are processed like this, a table with $|X|$
rows is built. Looking up the cell represents $s$ in the last row
tells if there exists subset can sum to this value. As mentioned
above, there
is no solution if $s < s_l$ or $s_u < s$. We skip handling this
case for the sake of brevity.

\begin{algorithmic}[1]
\Function{Subset-Sum}{$X, s$}
  \State $s_l \gets \sum \{x \in X, x < 0\}$
  \State $s_u \gets \sum \{x \in X, x > 0\}$
  \State $n \gets |X|$
  \State $T \gets \{\{False, False, ...\}, \{False, False, ...\}, ...\}$ \Comment{$n \times (s_u - s_l + 1)$}
  \For{$i \gets 1$ to $n$}
    \For{$j \gets s_l$ to $s_u$}
      \If{$X[i] = j$}
        \State $T[i][j] \gets True$
      \EndIf
      \If{$i > 1$}
        \State $T[i][j] \gets T[i][j] \lor T[i-1][j]$
        \State $j' \gets j - X[i]$
        \If{$s_l \leq j' \leq s_u$}
          \State $T[i][j] \gets T[i][j] \lor T[i-1][j']$
        \EndIf
      \EndIf
    \EndFor
  \EndFor
  \State \Return $T[n][s]$
\EndFunction
\end{algorithmic}

Note that the index to the columns of the table, doesn't range from 1 to $s_u-s_l + 1$,
but maps directly from $s_l$ to $s_u$. Because most programming environments don't support
negative index, this can be dealt with $T[i][j-s_l]$. The following example
Python program utilizes the property of negative indexing.

\lstset{language=Python}
\begin{lstlisting}
def solve(xs, s):
    low = sum([x for x in xs if x < 0])
    up  = sum([x for x in xs if x > 0])
    tab = [[False]*(up-low+1) for _ in xs]
    for i in xrange(0, len(xs)):
        for j in xrange(low, up+1):
            tab[i][j] = (xs[i] == j)
            j1 = j - xs[i];
            tab[i][j] = tab[i][j] or tab[i-1][j] or
                        (low <= j1 and j1 <= up and tab[i-1][j1])
    return tab[-1][s]
\end{lstlisting}

Note that this program doesn't use different branches for $i = 0$ and $i = 1, 2, ..., n-1$. This
is because when $i = 0$, the row index to $i - 1 = -1$ refers to the last row in the table, which are
all false. This simplifies the logic one more step.

With this table built, it's easy to construct all subsets sum to $s$. The method is to look up
the last row for cell represents $s$. If the last element $x_n = s$, then $\{x_n\}$ definitely
is a candidate. We next look up the previous row for $s$, and recursively construct all the
possible subsets sum to $s$ with $\{x_1, x_2, x_3, ..., x_{n-1}\}$. Finally, we look up
the second last row for cell represents $s - x_n$. And for every subset sums to this value,
we add element $x_n$ to construct a new subset, which sums to $s$.

\begin{algorithmic}[1]
\Function{Get}{$X, s, T, n$}
  \State $S \gets \Phi$
  \If{$X[n] = s$}
    \State $S \gets S \cup \{X[n]\}$
  \EndIf
  \If{$n > 1$}
    \If{$T[n-1][s]$}
      \State $S \gets S \cup $ \Call{Get}{$X, s, T, n-1$}
    \EndIf
    \If{$T[n-1][s-X[n]]$}
      \State $S \gets S \cup \{\{X[n]\} \cup S' | S' \in $ \Call{Get}{$X, s-X[n], T, n-1$} $\}$
    \EndIf
  \EndIf
  \State \Return $S$
\EndFunction
\end{algorithmic}

The following Python example program translates this algorithm.

\lstset{language=Python}
\begin{lstlisting}
def get(xs, s, tab, n):
    r = []
    if xs[n] == s:
        r.append([xs[n]])
    if n > 0:
        if tab[n-1][s]:
            r = r + get(xs, s, tab, n-1)
        if tab[n-1][s - xs[n]]:
            r = r + [[xs[n]] + ys for ys in get(xs, s - xs[n], tab, n-1)]
    return r
\end{lstlisting}

This dynamic programming solution to subset sum problem loops $O(n(s_u - s_l + 1))$ times
to build the table, and recursively uses $O(n)$ time to construct the final solution from this table.
The space it used is also bound to $O(n(s_u - s_l + 1))$.

Instead of using table with $n$ rows, a vector can be used alternatively. For every cell
represents a possible sum, the list of subsets are stored. This vector is initialized to contain
all empty sets. For every element in $X$, we update the vector, so that it records all the possible
sums which can be built so far. When all the elements are considered, the cell corresponding
to $s$ contains the final result.

\begin{algorithmic}[1]
\Function{Subset-Sum}{$X, s$}
  \State $s_l \gets \sum \{x \in X, x < 0\}$
  \State $s_u \gets \sum \{x \in X, x > 0\}$
  \State $T \gets \{\Phi, \Phi, ...\}$ \Comment{$s_u - s_l + 1$}
  \For{$x \in X$}
    \State $T' \gets$ \Call{Duplicate}{$T$}
    \For{$j \gets s_l$ to $s_u$}
      \State $j' \gets j - x$
      \If{$x = j$}
        \State $T'[j] \gets T'[j] \cup \{x\}$
      \EndIf
      \If{$s_l \leq j' \leq s_u \land T[j'] \neq \Phi$}
        \State $T'[j] \gets T'[j] \cup \{\{x\} \cup S | S \in T[j']\}$
      \EndIf
    \EndFor
    \State $T \gets T'$
  \EndFor
  \State \Return $T[s]$
\EndFunction
\end{algorithmic}

The corresponding Python example program is given as below.

\lstset{language=Python}
\begin{lstlisting}
def subsetsum(xs, s):
    low = sum([x for x in xs if x < 0])
    up  = sum([x for x in xs if x > 0])
    tab = [[] for _ in xrange(low, up+1)]
    for x in xs:
        tab1 = tab[:]
        for j in xrange(low, up+1):
            if x == j:
                tab1[j].append([x])
            j1 = j - x
            if low <= j1 and j1 <= up and tab[j1] != []:
                tab1[j] = tab1[j] + [[x] + ys for ys in tab[j1]]
        tab = tab1
    return tab[s]
\end{lstlisting}

This imperative algorithm shows a clear structure, that the solution table is
built by looping every element. This can be realized in purely functional
way by folding. A finger tree can be used to represents the vector spans
from $s_l$ to $s_u$. It is initialized with all empty values as in the following
equation.

\be
subsetsum(X, s) = fold(build, \{\Phi, \Phi, ..., \}, X)[s]
\ee

After folding, the solution table is built, the answer is looked up at
cell $s$\footnote{Again, here we skip the error handling to the case that
$s < s_l$ or $s > s_u$. There is no solution if $s$ is out of range.}.

For every element $x \in X$,
function $build$ folds the list $\{s_l, s_l + 1, ..., s_u\}$, with every
value $j$, it checks if it equals to $x$ and appends the singleton set $\{x\}$
to the $j$-th cell. Not that here the cell is indexed from
$s_l$, but not 0. If the cell corresponding to $j - x$ is not empty,
the candidate solutions stored in that place are also duplicated and
add element $x$ is added to every solution.

\be
build(T, x) = fold(f, T, \{s_l, s_l+1, ..., s_u\})
\ee

\be
f(T, j) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  T'[j] \cup \{\{x\} \cup Y | Y \in T[j']\} & s_l \leq j' \leq s_u \land T[j'] \neq \Phi, j' = j - x \\
  T' & otherwise
  \end{array}
\right.
\label{eq:mutate1}
\ee

Here the adjustment is applied on $T'$, which is another adjustment to $T$ as shown as below.

\be
T' = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{x\} \cup T[j] & x = j \\
  T & otherwise
  \end{array}
\right.
\label{eq:mutate2}
\ee

Note that the first clause in both equation (\ref{eq:mutate1}) and (\ref{eq:mutate2}) return
a new table with certain cell being updated with the given value.

The following Haskell example program implements this algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
subsetsum xs s = foldl build (fromList [[] | _ <- [l..u]]) xs `idx` s where
  l = sum $ filter (< 0) xs
  u = sum $ filter (> 0) xs
  idx t i = index t (i - l)
  build tab x = foldl (\t j -> let j' = j - x in
                   adjustIf (l <= j' && j' <= u && tab `idx` j' /= [])
                            (++ [(x:ys) | ys <- tab `idx` j']) j
                            (adjustIf (x == j) ([x]:) j t)) tab [l..u]
  adjustIf pred f i seq = if pred then adjust f (i - l) seq else seq
\end{lstlisting}

Some materials like \cite{algorithms-fp} provide common structures to abstract dynamic programming.
So that problems can be solved with a generic solution by customizing the precondition, the
comparison of candidate solutions for better choice, and the merge method for sub solutions.
However, the variety of problems makes things complex in practice. It's important to study
the properties of the problem carefully.

\begin{Exercise}
\begin{itemize}
\item Realize a maze solver by using the stack approach, which can find all the possible paths.
\item There are 92 distinct solutions for the 8 queens puzzle. For any one solution, rotating it
$90^{\circ}, 180^{\circ}, 270^{\circ}$ gives solutions too. Also flipping it vertically and horizontally
also generate solutions. Some solutions are symmetric, so that rotation or flip gives the same one.
There are 12 unique solutions in this sense. Modify the program to find the 12 unique solutions.
Improve the program, so that the 92 distinct solutions can be found with fewer search.
\item Make the 8 queens puzzle solution generic so that it can solve $n$ queens puzzle.
\item Make the functional solution to the leap frogs puzzle generic, so that it can solve $n$
frogs case.
\item Modify the wolf, goat, and cabbage puzzle algorithm, so that it can find all possible
solutions.
\item Give the complete algorithm definition to solve the 2 water jugs puzzle with extended
Euclid algorithm.
\item We needn't the exact linear combination information $x$ and $y$ in fact. After we know
the puzzle is solvable by testing with GCD, we can blindly execute the process that:
fill $A$, pour $A$ into $B$, whenever $B$ is full, empty it till there is expected volume
in one jug. Realize this solution. Can this one find faster solution than the original
version?
\item Compare to the extended Euclid method, the DFS approach is a kind of brute-force searching.
Improve the extended Euclid approach by finding the best linear combination which minimize
$|x| + |y|$.
\item Conway sliding puzzle, refer to the English version...
\item Realize the imperative Huffman code table generating algorithm.
\item One option to realize the bottom-up solution for the longest common subsequence problem
is to record the direction in the table. Thus, instead of storing the length information,
three values like 'N', for north, 'W' for west, and 'NW' for northwest are used to indicate
how to construct the final result. We start from the bottom-right corner of the table, if
the cell value is 'NW', we go along the diagonal by moving to the cell in the upper-left;
if it's 'N', we move vertically to the upper row; and move horizontally if it's 'W'.
Implement this approach in your favorite programming language.
\end{itemize}
\end{Exercise}

\section{Short summary}
This chapter introduces the elementary methods about searching. Some of them instruct
the computer to scan for interesting information among the data. They often have
some structure, that can be updated during the scan. This can be considered
as a special case for the information reusing approach. The other commonly used
strategy is divide and conquer, that the scale of the search domain is
kept decreasing till some obvious result. This chapter also explains methods
to search for solutions among domains. The solutions typically are not
the elements being searched. They can be a series of decisions or some
operation arrangement. If there are multiple solutions, sometimes, people want
to find the optimized one. For some spacial cases, there exist simplified
approach such as the greedy methods. And dynamic programming can be used
for more wide range of problems when they shows optimal substructures.

\begin{thebibliography}{99}

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)''. Addison-Wesley Professional; 2 edition (May 4, 1998) ISBN-10: 0201896850 ISBN-13: 978-0201896855

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein.
``Introduction to Algorithms, Second Edition''. ISBN:0262032937. The MIT Press. 2001

\bibitem{median-of-median}
M. Blum, R.W. Floyd, V. Pratt, R. Rivest and R. Tarjan, "Time bounds for selection," J. Comput. System Sci. 7 (1973) 448-461.

\bibitem{programming-pearls}
Jon Bentley. ``Programming pearls, Second Edition''. Addison-Wesley Professional; 1999. ISBN-13: 978-0201657883

\bibitem{fp-pearls}
Richard Bird. ``Pearls of functional algorithm design''. Chapter 3. Cambridge University Press. 2010. ISBN, 1139490605, 9781139490603

\bibitem{saddle-back}
Edsger W. Dijkstra. ``The saddleback search''. EWD-934. 1985. http://www.cs.utexas.edu/users/EWD/index09xx.html.

\bibitem{boyer-moore-majority}
Robert Boyer, and Strother Moore. ``MJRTY - A Fast Majority Vote Algorithm''. Automated Reasoning: Essays in Honor of Woody Bledsoe, Automated Reasoning Series, Kluwer Academic Publishers, Dordrecht, The Netherlands, 1991, pp. 105-117.

\bibitem{count-min-sketch}
Cormode, Graham; S. Muthukrishnan (2004). ``An Improved Data Stream Summary: The Count-Min Sketch and its Applications''. J. Algorithms 55: 29-38.

\bibitem{kmp}
Knuth Donald, Morris James H., jr, Pratt Vaughan. ``Fast pattern matching in strings''. SIAM Journal on Computing 6 (2): 323-350. 1977.

\bibitem{boyer-moore}
Robert Boyer, Strother Moore. ``A Fast String Searching Algorithm''. Comm. ACM (New York, NY, USA: Association for Computing Machinery) 20 (10): 762-772. 1977

\bibitem{boyer-moore-horspool}
R. N. Horspool. ``Practical fast searching in strings''. Software - Practice \& Experience 10 (6): 501-506. 1980.

\bibitem{wiki-boyer-moore}
Wikipedia. ``Boyer-Moore string search algorithm''. http://en.wikipedia.org/wiki/Boyer-Moore\_string\_search\_algorithm

\bibitem{wiki-8-queens}
Wikipedia. ``Eight queens puzzle''. http://en.wikipedia.org/wiki/Eight\_queens\_puzzle

\bibitem{how-to-solve-it}
George P\'{o}lya. ``How to solve it: A new aspect of mathematical method''. Princeton University Press(April 25, 2004). ISBN-13: 978-0691119663

\bibitem{Huffman}
Wikipedia. ``David A. Huffman''. http://en.wikipedia.org/wiki/David\_A.\_Huffman

\bibitem{algorithms-fp}
Fethi Rabhi, Guy Lapalme ``Algorithms: a functional programming approach''. Second edition. Addison-Wesley.

\end{thebibliography}

\ifx\wholebook\relax\else
\end{document}
\fi
