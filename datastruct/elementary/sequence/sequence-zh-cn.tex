\ifx\wholebook\relax \else
% ------------------------

\documentclass[UTF8]{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
\input{../../../common-zh-cn.tex}

\setcounter{page}{1}

\begin{document}

%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{序列，最后一块砖}

\author{刘新宇
\thanks{{\bfseries 刘新宇 } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\maketitle
\fi

\markboth{序列}{初等算法}

\ifx\wholebook\relax
\chapter{序列，最后一块砖}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{简介}
\label{introduction}
本书的第一章把二叉搜索树作为“hello world”的数据结构加以介绍。我们指出，同时给出队列和序列的命令式和函数式实现并不简单。前一章中，我们给出了函数式队列，可以达到和命令式的队列相同的性能。本章中，我们将仔细探讨类似数组的数据结构。

本书中介绍的大部分函数式数据结构往往简洁直观。但是仍然有某些领域，人们尚未发现和命令式实现相当的解法。例如在线性时间内构造后缀树的Ukkonen算法、Hash表、以及数组。

数组属于命令式环境中的最基本数据结构，它支持通过索引在常数时间$O(1)$内随机访问元素。但是在函数式环境中，只能使用列表作为基本数据结构，我们无法直接获得这样的随机访问性能。

本章中，我们将数组的概念抽象到序列（sequence）。它需要支持下面的特性：

\begin{itemize}
\item 可以在序列的头部用常数时间$O(1)$快速插入、删除元素；
\item 可以在序列的尾部用常数时间$O(1)$快速插入、删除元素；
\item 可以快速（优于线性时间）连接两个序列；
\item 可以快速随机访问、更改任何元素；
\item 可以快速在任何位置将序列断开；
\end{itemize}

我们称上述特性为抽象序列性质。显然，即使是命令式环境中的数组（普通数组）也无法同时满足这些要求。

本章中，我们将给出三种解法。首先我们介绍一种基于二叉树森林和numeric representation的解法；接着将介绍可连接列表（concatenate-able list）的解法；最后，我们给出finger tree的解法。

大部分的结果来自于Chris Okasaki的工作\cite{okasaki-book}。

% ================================================================
%                 Binary random access list
% ================================================================
\section{二叉随机访问列表}
\index{序列!Binary random access list}

\subsection{普通数组和列表}

下表列出了普通数组和单向链表的性能对比，我们可以看到它们在不同情况下的表现。

\begin{tabular}{l | c | r}
  \hline
  操作 & 数组 & 链表 \\
  \hline
  在头部操作 & $O(n)$ & $O(1)$ \\
  在尾部操作 & $O(1)$ & $O(n)$ \\
  随机访问 & $O(1)$ & 平均$O(n)$ \\
  在给定位置删除 & 平均$O(n)$ & $O(1)$ \\
  连接 & $O(n_2)$ & $O(n_1)$ \\
  \hline
\end{tabular}

由于持有表头，所以在链表的头部进行插入和删除只需要常数时间；但是我们需要遍历到链表的末尾来进行尾部的删除和追加；给定位置$i$，需要前进$i$步来访问第$i$个元素。当到达第$i$个位置，只需要做指针的改动就可以在常数时间内完成元素的删除。为了连接两个链表，我们需要遍历到第一个链表的末尾，然链接到第二个链表的表头。因此所用时间和第一个链表的长度成比例。

而对于数组，我们必须将第一个cell空出以在头部插入一个新元素；删除第一个元素后，我们需要将第一个cell释放。这两个操作都需要对后续元素进行逐一移动，因此花费线性时间。反之，在数组的尾部进行操作则很简单，只需要常数时间。数组支持常数时间访问第$i$个元素；但是将第$i$个元素删除则需要将后面的元素逐一向前移动。为了连接两个数组，我们需要将第二个数组的元素全部复制到第一个数组的后面（忽略内存重新分配的细节），因此所用时间和第二个数组的长度成比例。

在二项式堆的章节中，我们给出了森林的概念，森林是若干树的列表。它的好处是，给定任何非负整数$n$，将其表达为二进制，我们就知道需要多少棵二项式树来存储$n$个元素。每个值为1的二进制位代表一棵二项式树，树的rank为对应的第几个二进制位。我们可以得到进一步的结论：对于含有$n$个节点的二项式堆，给定任何索引$1 < i < n$，我们都可以快速地在堆中定位到保存第$i$个节点的二项式树。

\subsection{使用森林表示序列}
\index{Binary Random Access List!定义}

可以使用完全二叉树的森林来实现随机访问序列。图\ref{fig:bi-tree-sequence}展示了如何将使用若干完全二叉树来管理序列。

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.0]{img/bi-tree-sequence.ps}
  \caption{使用森林来表示含有6个元素的序列。} \label{fig:bi-tree-sequence}
\end{figure}

图中使用了两棵树 $t_1$和$t_2$来表示序列$\{x_1, x_2, x_3, x_4, x_5, x_6\}$。二叉树$t_1$的大小为2。前两个元素$\{x_1, x_2\}$存储在$t_1$的叶子中；二叉树$t_2$的大小为4，接下来的4个元素$\{x_3, x_4, x_5, x_6\}$保存在$t_2$中。

对于完全二叉树，我们定义只含有一个叶子的树深度为0。将深度为$i+1$的树记为$t_i$。显然，$t_i$含有$2^i$个叶子。

对于含有任意$n$个元素的序列，我们可以将其转换为一组这样的完全二叉树森林。首先我们将$n$表示为如下的二进制数。

\be
n = 2^0 e_0 + 2^1 e_1 + ... + 2^m e_m
\ee

其中$e_i$的值为1或0，即$n=(e_m e_{m-1} ... e_1 e_0)_2$。如果$e_i \neq 0$，就需要一棵大小为$2^i$的完全二叉树。例如图\ref{fig:bi-tree-sequence}中，序列的长度为6，写成二进制为$(110)_2$。最低位是0，因此我们不需要大小为1的树；第2位为1，因此需要一棵大小为2的树；最高位也是1，因此需要一棵深度为3，大小为4的树。

这一方法将序列$\{x_1, x_2, ..., x_n\}$表示为一个树的列表$\{t_0, t_1, ..., t_m\}$，其中若$e_i = 0$, 则$t_i$为空，否则，若$e_i = 1$, 则$t_i$为一棵完全二叉树。我们称之为\underline{二叉随机访问列表（Binary Random Access List）}\cite{okasaki-book}。

我们可以复用二叉树的定义。下面的Haskell例子程序定义了树和二叉随机存取列表。

\lstset{language=Haskell}
\begin{lstlisting}
data Tree a = Leaf a
            | Node Int (Tree a) (Tree a)  -- size, left, right

type BRAList a = [Tree a]
\end{lstlisting}

和标准二叉树定义相比，我们增加了size的信息。这样可以避免每次都递归计算size，可以用常数时间获取树的大小。

\begin{lstlisting}
size (Leaf _) = 1
size (Node sz _ _) = sz
\end{lstlisting}

\subsection{在序列的头部插入}
\index{Binary Random Access List!插入}
使用森林表示序列可以高效实现许多操作。例如，将新元素$y$插入到序列的前面可以实现如下：

\begin{enumerate}
\item 创建一个只含有一个叶子节点$y$的树$t'$；
\item 检查森林中的第一棵树，比较它和$t'$的size大小，如果它的size大于$t'$的，就将$t'$作为森林中的第一棵树，由于森林本质是一个树的链表，在表头插入$t'$只需要常数时间$O(1)$；
\item 否则，若森林中的第一棵树的size和$t'$相等，记森林中的这棵树为$t_i$，可以通过将$t_i$和$t'$链接起来，形成一棵新树$t'_{i+1}$，$t_i$和$t'$分别为这棵新树的左右子树。然后我们递归地将$t'_{i+1}$插入到森林中。
\end{enumerate}

图\ref{fig:bralist-1}和\ref{fig:bralist-2}描述了将元素$x_1, x_2, ..., x_6$依次插入到空列表的情况。

\begin{figure}[htbp]
  \centering
  \subfloat[包含一个元素$x_1$的叶子。]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-1.ps}\hspace{0.2\textwidth}}
  \subfloat[插入$x_2$。进行连接，产生一棵高度为1的树。]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-2.ps}\hspace{0.2\textwidth}} \\
  \subfloat[插入$x_3$。结果包含两棵树$t_1$和$t_2$。]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-3.ps}\hspace{0.2\textwidth}}
  \subfloat[插入$x_4$。首先将两个叶子链接成一棵二叉树，然后再次链接，产生一棵高度为2的树。]{\includegraphics[scale=0.5]{img/bralst-4.ps}}
  \caption{将元素插入到空列表的步骤，1。} \label{fig:bralist-1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \subfloat[插入$x_5$。 森林中包含两棵树，一棵仅含有一个叶子节点（$t_0$），另一棵为$t_2$。]{\includegraphics[scale=0.5]{img/bralst-5.ps}}
  \subfloat[插入$x_6$。将两个叶子链接成$t_1$。]{\includegraphics[scale=0.5]{img/bralst-6.ps}} \\
  \caption{将元素插入到空列表的步骤，2。} \label{fig:bralist-2}
\end{figure}

由于森林中最多包含$m$棵树，$m$的大小为$O(\lg n)$，因此在头部插入的算法最坏情况下的性能为$O(\lg n)$。稍后我们会证明分摊性能为$O(1)$。

接下来我们将上述算法形式化。定义将元素插入到序列头部的函数为$insert(S, x)$。

\be
insert(S, x) = insertTree(S, leaf(x))
\ee

这一函数从元素$x$构造一棵只有一个叶子节点的树，然后调用$insertTree$将树插入到森林中。若森林不为空，记$F=\{ t_1, t_2, ...\}$，$F' = \{ t_2, t_3, ...\}$表示森林中除第一棵树意外的剩余部分。

\be
insertTree(F, t) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ t \} & F = \Phi \\
  \{ t \} \cup F & size(t) < size(t_1) \\
  insertTree(F', link(t, t_1)) & otherwise
  \end{array}
\right .
\ee

其中函数$link(t_1, t_2)$从两棵size相同的较小的树构造一棵新树。定义函数$tree(s, t_1, t_2)$构造一棵树，size为$s$，左右子树分别为$t_1$和$t_2$。树的链接可以实现如下：

\be
link(t_1, t_2) = tree(size(t_1) + size(t_2), t_1, t_2)
\ee

下面的Haskell例子程序实现了在表头插入元素的算法。

\begin{lstlisting}
cons :: a -> BRAList a -> BRAList a
cons x ts = insertTree ts (Leaf x)

insertTree :: BRAList a -> Tree a -> BRAList a
insertTree [] t = [t]
insertTree (t':ts) t = if size t < size t' then  t:t':ts
                       else insertTree ts (link t t')

-- Precondition: rank t1 = rank t2
link :: Tree a -> Tree a -> Tree a
link t1 t2 = Node (size t1 + size t2) t1 t2
\end{lstlisting}

这段代码中，我们使用了Lisp中的命名传统，把向列表头部插入元素称为“cons”。

\subsubsection{从序列头部删除元素}
\index{Binary Random Access List!从头部删除}

类似地，我们可以实现“cons”的逆运算，从序列的头部删除元素。

\begin{itemize}
\item 如果森林中的第一棵树只含有一个叶子节点，将这棵大小为1的树删除；
\item 否则，将第一棵树的两个子树拆分，然后递归地将第一棵子树继续拆分直到获得一棵只有一个叶子节点的树。
\end{itemize}

图\ref{fig:bralist-pop}给出了从序列头部删除元素的步骤。

\begin{figure}[htbp]
  \centering
  \subfloat[含有5个元素的序列]{\includegraphics[scale=0.5]{img/bralst-5.ps}}
  \subfloat[删除$x_5$，大小为1，只有一个叶子节点的树被删除。]{\includegraphics[scale=0.5]{img/bralst-4.ps}} \\
  \subfloat[删除$x_4$，由于不是仅包含一个叶子节点的树，它首先被拆分成大小为2的两棵子树。然后第一棵子树继续被拆分成两个叶子。此后含有$x_4$的第一个叶子被删除。森林中剩下两棵树，一棵是含有$x_3$的子树，另外一棵大小为2，包含元素$x_2, x_1$。]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-3.ps}\hspace{0.2\textwidth}}
  \caption{从头部删除元素的步骤。} \label{fig:bralist-pop}
\end{figure}

简单起见，假设序列不为空，我们可以忽略从空序列中删除元素的错误处理。上述算法可以表示为下面的表达式。记森林为$F = \{t_1, t_2, ... \}$，出去第一棵树后的剩余部分为$F' = \{ t_2, t_3, ...\}$。

\be
extractTree(F) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (t_1, F') & t_1 {\quad} \mbox{is leaf} \\
  extractTree(\{t_l, t_r\} \cup F') & otherwise
  \end{array}
\right .
\ee

其中$\{ t_l, t_r \} = unlink(t_1)$，为$t_1$的两棵子树。

下面的Haskell程序实现了这一算法。

\begin{lstlisting}
extractTree (t@(Leaf x):ts) = (t, ts)
extractTree (t@(Node _ t1 t2):ts) = extractTree (t1:t2:ts)
\end{lstlisting}

使用这一定义，可以很容易地给出$head()$和$tail()$函数，前者返回序列中的第一个元素，后者返回剩余元素。

\be
head(S) = key(first(extractTree(S)))
\ee

\be
tail(S) = second(extractTree(S))
\ee

其中函数$first()$返回一对元素（亦称为tuple）中的前一个元素；$second()$返回后一个元素。函数$key()$用以访问叶子节点中存储的元素。下面的Haskell例子程序实现这两个函数。

\begin{lstlisting}
head' ts = x where (Leaf x, _) = extractTree ts
tail' = snd . extractTree
\end{lstlisting}

为了和Haskell的标准库中定义的\texttt{head}和\texttt{tail}区别，我们加上了“'”号（另外一种方法是通过隐藏import不导入标准库中的定义，我们忽略这些和语言相关的特定细节）。

\subsubsection{随机访问元素}
\index{Binary Random Access List!随机访问}

森林中的树实际上将元素划分为大小不同的block进行管理，给定任意索引，可以很容易地定位到保存此元素的树。然后在树种进行一次查找就可以得到结果。因为所有的树都是二叉树（确切地说是完全二叉树），所以树中的查找实际上是二分查找，性能和树的大小为对数比例。进行随机访问时，相比在链表中进行线性查找要快得多。

给定索引$i$和序列$S$，序列使用由树组成的森林来表示，随机访问算法可以描述如下\footnote{按照传统，在进行算法描述时，索引$i$从1开始；在大多数编程语言中，索引从0开始。}：

\begin{enumerate}
\item 比较$i$和森林中第一棵树$T_1$的size的大小，若$i$小于等于size，则元素在$T_1$中，接下来在$T_1$中查找；
\item 否则，从$i$中减去$T_1$的size，然后在森林中剩余的树中重复前面的步骤。
\end{enumerate}

这一算法可以形式化为下面的定义。

\be
get(S, i) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  lookupTree(T_1, i) & i \leq |T_1| \\
  get(S', i- |T_1|) & otherwise
  \end{array}
\right .
\ee

其中$|T| = size(T)$，而$S' = \{ T_2, T_3, ... \}$为森林中除第一棵树以外的剩余部分。这里我们没有进行越界的检查和错误处理，这些可以留给读者作为练习。

函数$lookupTree()$是一个二分查找算法，如果$i$等于1，我们返回树的根节点，否则，我们将树对半拆分，如果$i$小于拆分后树的size，就递归地在左子树中查找，否则就递归地在右子树中查找。

\be
lookupTree(T, i) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  root(T) & i = 1 \\
  lookupTree(left(T)) & i \leq \lfloor \frac{|T|}{2} \rfloor \\
  lookupTree(right(T)) & otherwise
  \end{array}
\right .
\ee

其中函数$left()$返回$T$的左子树$T_l$，而$right()$返回右子树$T_r$。

下面的Haskell例子程序实现了相应的算法。

\begin{lstlisting}
getAt (t:ts) i = if i < size t then lookupTree t i
                 else getAt ts (i - size t)

lookupTree (Leaf x) 0 = x
lookupTree (Node sz t1 t2) i = if i < sz `div` 2 then lookupTree t1 i
                               else lookupTree t2 (i - sz `div` 2)
\end{lstlisting}

图\ref{fig:get-at-example}描述了在一个长度为6的序列中查找第4个元素的步骤。首先检查第一棵树，由于大小为2，小于4，所以继续检查第二棵树，同时将索引更新为$i'=4-2$。即查找森林中剩余部分的第2个元素。由于接下来的树大小为4，大于2，所以待查找的元素就在这棵树中。因为新索引为2，它不大于对半拆分的子树尺寸4/2=2，所以接下来需要检查左子树，然后检查右侧的孙子分支，最终得到要访问的元素。

\begin{figure}[htbp]
  \centering
  \subfloat[$getAt(S, 4))$, $4 > size(t_1) = 2$]{\includegraphics[scale=0.5]{img/bralst-6.ps}}
  \subfloat[$getAt(S', 4-2) \Rightarrow lookupTree(t_2, 2)$]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-4.ps}\hspace{0.2\textwidth}} \\
  \subfloat[$ 2 \leq \lfloor size(t_2)/2 \rfloor \Rightarrow lookupTree(left(t_2), 2)$]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-4l.ps}\hspace{0.2\textwidth}}
  \subfloat[$lookupTree(right(left(t_2)), 1)$, $x_3$被返回]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-4lr.ps}\hspace{0.2\textwidth}}
  \caption{在序列中访问第4个元素的步骤。} \label{fig:get-at-example}
\end{figure}

使用类似的思路，我们可以修改任意位置$i$的元素。首先比较森林中第一棵树$T_1$的size和$i$的大小，若小于$i$，待修改的元素不在第一棵树种。我们递归地检查森林中的下一棵树，将它的size和$i - |T_1|$比较，其中$|T_1|$是第一棵树的size。否则，若这一size大于等于$i$，待修改的元素在树中，我们递归地拆分树，直到获得叶子节点，此时我们将节点中的元素替换为新元素。

\be
set(S, i, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ updateTree(T_1, i, x) \} \cup S' & i < |T_1| \\
  \{T_1\} \cup set(S', i - |T_1|, x) & otherwise
  \end{array}
\right .
\ee

其中$S' = \{ T_2, T_3, ...\}$是森林中除第一棵树外的剩余部分。

函数$setTree(T, i, x)$执行树搜索，并将第$i$个元素替换为$x$。

\be
setTree(T, i, x) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & i = 0 \land |T| = 1 \\
  tree(|T|, setTree(T_l, i, x), T_r) & i < \lfloor \frac{|T|}{2} \rfloor \\
  tree(|T|, T_l, setTree(T_r, i - \lfloor \frac{|T|}{2} \rfloor, x)) & otherwise
  \end{array}
\right .
\ee

其中$T_l$和$T_r$分别为$T$的左右子树。下面的Haskell例子程序实现了这一算法。

\begin{lstlisting}
setAt :: BRAList a -> Int -> a -> BRAList a
setAt (t:ts) i x = if i < size t then (updateTree t i x):ts
                   else t:setAt ts (i-size t) x

updateTree :: Tree a -> Int -> a -> Tree a
updateTree (Leaf _) 0 x = Leaf x
updateTree (Node sz t1 t2) i x =
    if i < sz `div` 2 then Node sz (updateTree t1 i x) t2
    else Node sz t1 (updateTree t2 (i - sz `div` 2) x)
\end{lstlisting}

根据完全二叉树的性质，对于含有$n$个元素，用二叉随机访问列表表示的序列，森林中树木的棵树为$O(\lg n)$。因此任意给定的索引$i$，最多需要$O(\lg n)$时间来定位到树。接下来的搜索和树的高度成正比，最多也是$O(\lg n)$。因此随机访问的总体性能为$O(\lg n)$。

\begin{Exercise}
\begin{enumerate}
\item 本节给出的随机访问算法没有处理索引越界的错误情况。选择一门编程语言，修改算法，实现错误处理。

\item 也可以用命令式方式实现二叉随机访问列表，提供在序列头部的快速操作。随机访问可以通过两个步骤实现：首先定位到树，然后使用数组的常数时间随机存取能力。选择一门命令式语言实现这一思路。
\end{enumerate}
\end{Exercise}

\section{二叉随机访问列表的数字表示（Numeric representation）}
\index{Sequence!numeric representation for binary access list}

在前一节，我们提到对于任意长度为$n$的序列，可以将$n$表示为二进制形式$n = 2^0e_0 + 2^1e_1 + ... + 2^me_m$，其中$e_i$为第$i$位，值为1或者0。若$e_i \neq 0$，则存在一棵大小为$2^i$的完全二叉树。

这一事实反映了$n$的二进制形式和森林之间存在明确的关系。向序列的头部插入元素，类似于将二进制数增加1；而从序列的头部删除元素类似于将二进制数减少1。我们称这种关系为\underline{numeric representation}\cite{okasaki-book}。

为了将二叉随机访问列表用二进制数字表示，可以为每一个二进制位定义两个状态。状态$Zero$表示不存在对应此二进制位大小的树，而$One$表示森林中存在一棵对应于此二进制位大小的树。如果状态为$One$，我们可以将对应的树附加到状态上。

下面的Haskell例子程序定义了这样状态。

\begin{lstlisting}
data Digit a = Zero
             | One (Tree a)

type RAList a = [Digit a]
\end{lstlisting}

我们重用了完全二叉树的定义，并把它附加到状态$One$上。我们同时将树的大小信息也加以缓存。

定义了digit后，一个森林就可以按照包含若干digit的列表来处理。我们首先看如何将新元素的插入操作实现为二进制数的增加。设函数$one(t)$创建一个$One$的状态，并将树$t$附加到这个状态上。函数$getTree(s)$从状态$s$中获取树。序列$S$是一个列表，包含若干表示digit的状态，记为$S = \{ s_1, s_2, ... \}$。$S'$为除第一个状态外的剩余部分。

\be
insertTree(S, t) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ one(t) \} & S = \Phi \\
  \{ one(t) \} \cup S' & s_1 = Zero \\
  \{ Zero \} \cup insertTree(S', link(t, getTree(s_1))) & otherwise
  \end{array}
\right .
\ee

将一棵新树$t$插入到一个用二进制数字序列表示的森林$S$时，若森林为空，我们创建一个状态$One$，将待插入的树附加到状态上。这个状态是二进制数中的唯一一位。相当于二进制加法$0 + 1 = 1$。

否则，如果森林不为空，我们需要检查二进制数的第一位，如果第一个digit是$Zero$，我们创建一个状态$One$，附加上待插入的树，然后用这个新创建的$One$状态替换掉$Zero$状态。这相当于二进制加法$(...digits...0)_2 + 1 = (...digits...1)_2$。例如$6 + 1 = (110)_2 + 1 = (111)_2 = 7$。

最后一种情况是二进制数的第一位digit是$One$，这里我们假设待插入的树$t$和状态$One$中附加的树具有相同的size。这一点可以通过插入过程得到保证，我们总是从一个叶子开始插入，然后待插入的树的大小逐渐增长，呈一个序列$1, 2, 4, ..., 2^i, ...$。此时，我们将两棵树链接成一棵更大的树，然后递归地将链接结果插入到剩余的digit中。而之前的$One$状态，被替换为一个$Zero$状态。这相当于二进制加法$(...digits...1)_2 + 1 = (...digits'...0)_2$，其中$(...digits'...)_2 = (...digits...)_2+1$。例如$7 + 1 = (111)_2 + 1 = (1000)_2 = 8$。

下面的Haskell例子程序实现了这一算法。

\begin{lstlisting}
insertTree :: RAList a -> Tree a -> RAList a
insertTree [] t = [One t]
insertTree (Zero:ts) t = One t : ts
insertTree (One t' :ts) t = Zero : insertTree ts (link t t')
\end{lstlisting}

其他函数，包括$link()$、$cons()$等和此前的定义一样。

接下来我们解释如何用二进制数的减法来表示从序列的头部删除元素。如果序列只含有唯一的状态$One$，且状态上附加的树只有一个叶子。删除后序列变为空。这相当于二进制减法$1 - 1 = 0$。

否则，我们检查序列中的第一个数字，如果是$One$，它将被替换为$Zero$，表示森林中的这棵树将被删除。这相当于二进制减法 $(...digits...1)_2 - 1 = (...digits...0)_2$。例如$7 - 1 = (111)_2 - 1 = (110)_2 = 6$;

如果序列中的第一个数字是$Zero$，我们需要向后继的数字借位来进行删除。我们递归地从剩余的digit中抽取树，将其分拆成两棵子树。$Zero$状态将被替换成$One$状态，并将此前分拆出的右子树附加到状态上，而删除掉左子树。这相当于二进制减法$(...digits...0)_2 - 1 = (...digits'...1)_2$，其中$(...digits'...)_2 = (...digits)_2 - 1$。例如$4 - 1 = (100)_2 - 1 = (11)_2 = 3$。下面的定义给出了删除算法。

\be
extractTree(S) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (t, \Phi) & S = \{ one(t) \} \\
  (t, S') & s_1 = one(t) \\
  (t_l, \{ one(t_r) \} \cup S'' & otherwise
  \end{array}
\right .
\ee

其中$(t', S'') = extractTree(S')$，$t_l$和$t_r$分别是$t'$的左右子树。其他函数，包括$head()$和$tail()$的定义和此前一样。

使用数字表示二叉随机访问列表并没有改变复杂度，Okasaki在\cite{okasaki-ralist}中给出了详细地解释。作为例子，我们使用aggregation法，来分析在头部插入的平均（或称分摊）复杂度。

考虑依次向一个空的二叉随机访问列表插入$n = 2^m$个元素的过程。森林的二进制表示可以列成下表：

\begin{tabular}{l | r}
  \hline
  i & forest (MSB ... LSB) \\
  \hline
  0 & 0, 0, ..., 0, 0 \\
  1 & 0, 0, ..., 0, 1 \\
  2 & 0, 0, ..., 1, 0 \\
  3 & 0, 0, ..., 1, 1 \\
  ... & ... \\
  $2^m-1$ & 1, 1, ..., 1, 1 \\
  $2^m$ & 1, 0, 0, ..., 0, 0 \\
  \hline
  bits changed & 1, 1, 2, ... $2^{m-1}$. $2^m$ \\
  \hline
\end{tabular}

森林对应的LSB（最低位）每次在插入新元素时都变化，它总共需要$2^m$单位次计算；接下来的一位没隔一次变化，执行一次树的链接操作。总共需要$2^{m-1}$单位次计算；森林中对应MSB（最高位）的前一位总共只变化一次，它将此前所有的树链接成一棵更大的、森林中唯一的树。这发生在插入过程中的正中间。当最后一个元素插入后，MSB变化成1。

将所有的这些计算次数相加，我们得到$T = 1 + 1 + 2 + 3 + ... + 2^{m_1} + 2^m = 2^{m+1}$。因此平均下来每次插入操作的计算耗时：

\be
O(T/N) = O(\frac{2^{m+1}}{2^m}) = O(1)
\ee

这证明了插入算法的分摊复杂度为$O(1)$常数时间。删除算法复杂度的证明留给读者作为练习。

\subsection{命令式二叉随机访问列表}
\index{序列!Imperative binary random access list}

使用二叉树实现命令式二叉随机访问列表非常简单，递归可以通过在循环中修改当前的树进行消除。我们将此作为练习留给读者。本节中，我们给出一些不同的命令式实现，但基本思路仍然是使用numeric representation。

回顾一下二叉堆一章中的内容。二叉堆可以通过隐式的数组来实现。我们可以借鉴类似的方法，用只含有一个元素的数组代表叶子；用含有2个元素的数组代表高度为1的二叉树；用含有$2^m$个元素的数组代表高度为$m$的完全二叉树。

这样做的好处是，我们可以通过索引快速访问任何元素，而无需进行分而治之的树查找。代价是树的链接操作被替换成了耗时的数组复制。

下面的ANSI C例子代码定义了二叉树森林。

\lstset{language=C}
\begin{lstlisting}
#define M sizeof(int) * 8
typedef int Key;

struct List {
  int n;
  Key* tree[M];
};
\end{lstlisting}

其中$n$是森林中存储的元素个数。当然，我们也可以通过使用动态数组来避免限制树的棵树。例如下面的ISO C++例子程序。

\lstset{language=C++}
\begin{lstlisting}
template<typename Key>
struct List {
  int n;
  vector<vector<key> > tree;
};
\end{lstlisting}

简单起见，我们使用ANSI C作为例子，读者可以参考和本书一起发布的完整ISO C++例子程序。

我们首先回顾一下插入过程，若第一棵树为空（一个状态为$Zero$的digit），只需要将第一棵树变成一棵含有一个叶子节点的树，将待插入元素放入其中；否则，插入将引发树的链接，这一过程是递归的，直到某一个位置（digit），这个位置上对应的树为空。numeric representation告诉我们，如果第一棵、第二棵、……、第$i-1$棵树都存在，而第$i$棵树为空，结果会构造一棵大小为$2^i$的树，代插入的元素，和所有此前的元素都存储在这棵树中。而位置$i$之后的所有树都保持不变。

怎样能高效地定位到位置$i$呢？如果使用二进制数来代表含有$n$个元素的森林，当插入一个新元素后，$n$增长到$n+1$。比较$n$和$n+1$的二进制形式可以发现，所有$i$之前的位都从1变到0，而第$i$位从0变成1，所有$i$之后的位都保持不变。我们可以用位运算异或（$\oplus$）来检测到这一位，算法如下：

\begin{algorithmic}
\Function{Number-Of-Bits}{$n$}
  \State $i \gets 0$
  \While{$\lfloor \frac{n}{2} \rfloor \neq 0$}
    \State $ n \gets \lfloor \frac{n}{2} \rfloor$
    \State $ i \gets i + 1$
  \EndWhile
  \State \Return $i$
\EndFunction
\Statex
\State $i \gets $ \Call{Number-Of-Bits}{$n \oplus (n + 1)$}
\end{algorithmic}

这一算法可以通过移位运算实现，如下面的ANSI C例子程序。

\begin{lstlisting}
int nbits(int n) {
  int i=0;
  while(n >>= 1)
    ++i;
  return i;
}
\end{lstlisting}

因此，命令式插入算法可以这样实现：首先定位到从0翻转成1的位$i$，然后创建一个大小为$2^i$的数组，用以代表相应的完全二叉树，然后将待插入元素和此位之前所有树中保存的内容移动到这一数组中。

\begin{algorithmic}
\Function{Insert}{$L, x$}
  \State $i \gets $ \Call{Number-Of-Bits}{$n \oplus (n + 1)$}
  \State \Call{Tree}{$L$}[$i+1$] $\gets $ \Call{Create-Array}{$2^i$}
  \State $l \gets 1$
  \State  \Call{Tree}{$L$}[$i+1$][$l$]  $\gets x$
  \For{$j \in [1, i]$}
    \For{$k \in [1, 2^j]$}
      \State $l \gets l + 1$
      \State \Call{Tree}{$L$}[$i+1$][$l$]  $\gets$ \Call{Tree}{$L$}[$j$][$k$]
    \EndFor
    \State \Call{Tree}{$L$}[$j$] $\gets$ NIL
  \EndFor
  \State \Call{Size}{$L$} $\gets$ \Call{Size}{$L$} + 1
  \State \Return $L$
\EndFunction
\end{algorithmic}

对应的ANSI C例子程序如下。

\begin{lstlisting}
struct List insert(struct List a, Key x) {
  int i, j, sz;
  Key* xs;
  i = nbits( (a.n+1) ^ a.n );
  xs = a.tree[i] = (Key*)malloc(sizeof(Key)*(1<<i));
  for(j=0, *xs++ = x, sz = 1; j<i; ++j, sz <<= 1) {
    memcpy((void*)xs, (void*)a.tree[j], sizeof(Key)*(sz));
    xs += sz;
    free(a.tree[j]);
    a.tree[j] = NULL;
  }
  ++a.n;
  return a;
}
\end{lstlisting}

However, the performance in theory isn't as good as before. This is because the
linking operation downgrade from $O(1)$ constant time to linear array copying.

We can again calculate the average (amortized) performance by using aggregation
analysis. When insert $n = 2^m$ elements to an empty list which is represented
by implicit binary trees in arrays, the numeric presentation of the forest of
arrays are as same as before except for the cost of bit flipping.

\begin{tabular}{l | r}
  \hline
  i & forest (MSB ... LSB) \\
  \hline
  0 & 0, 0, ..., 0, 0 \\
  1 & 0, 0, ..., 0, 1 \\
  2 & 0, 0, ..., 1, 0 \\
  3 & 0, 0, ..., 1, 1 \\
  ... & ... \\
  $2^m-1$ & 1, 1, ..., 1, 1 \\
  $2^m$ & 1, 0, 0, ..., 0, 0 \\
  \hline
  bit change cost & $1 \times 2^m$, $1 \times 2^{m-1}$, $2 \times 2^{m-2}$, ... $2^{m-2} \times 2$, $2^{m-1} \times 1$ \\
  \hline
\end{tabular}

The LSB of the forest changed every time when there is a new element inserted, however,
it creates leaf tree and performs copying only it changes from 0 to 1, so the
cost is half of $n$ unit, which is $2^{m-1}$;
The next bit flips as half as the LSB. Each time the bit
gets flipped to 1, it copies the first tree as well as the new element to the second tree.
the the cost of flipping a bit to 1 in this bit is 2 units, but not 1; For the MSB, it only
flips to 1 at the last time, but the cost of flipping this bit, is copying all the previous
trees to fill the array of size $2^m$.

Summing all to cost and distributing them to the $n$ times of insertion yields the
amortized performance as below.

\be
\begin{array}{rcl}
O(T/N) & = & O(\frac{1 \times 2^m + 1 \times 2^{m-1} + 2 \times 2^{m-2} + ... + 2^{m-1} \times 1}{2^m}) \\
       & = & O(1 + \frac{m}{2}) \\
       & = & O(m)
\end{array}
\ee

As $m = O(\lg n)$, so the amortized performance downgrade from constant time to logarithm,
although it is still faster than the normal array insertion which is $O(n)$ in average.

The random accessing gets a bit faster because we can use array indexing instead of tree
search.

\begin{algorithmic}
\Function{Get}{$L, i$}
  \For{each $t \in $ \Call{Trees}{$L$}}
    \If{$t \neq$ NIL}
      \If{$i \leq $ \Call{Size}{$t$}}
        \State \Return $t$[i]
      \Else
        \State $i \gets i -$ \Call{Size}{$t$}
      \EndIf
    \EndIf
  \EndFor
\EndFunction
\end{algorithmic}

Here we skip the error handling such as out of bound indexing etc. The ANSI C program
of this algorithm is like the following.

\begin{lstlisting}
Key get(struct List a, int i) {
  int j, sz;
  for(j = 0, sz = 1; j < M; ++j, sz <<= 1)
    if(a.tree[j]) {
      if(i < sz)
	break;
      i -= sz;
    }
  return a.tree[j][i];
}
\end{lstlisting}

The imperative removal and random mutating algorithms are left as exercises to the reader.

\begin{Exercise}
\begin{enumerate}
\item Please implement the random access algorithms, including looking up and updating,
for binary random access list with numeric representation in your favorite programming
language.

\item Prove that the amortized performance of deletion is $O(1)$ constant time by
using aggregation analysis.

\item Design and implement the binary random access list by implicit array in your
favorite imperative programming language.
\end{enumerate}
\end{Exercise}

\section{Imperative paired-array list}
\index{Sequence!Paired-array list}

\subsection{Definition}
\index{Paired-array list!Definition}
In previous chapter about queue, a symmetric solution of paired-array is presented.
It is capable to operate on both ends of the list. Because the nature that array supports
fast random access. It can be also used to realize a fast random access sequence
in imperative setting.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.0]{img/palist.ps}
  \caption{A paired-array list, which is consist of 2 arrays linking in head-head manner.} \label{fig:palist}
\end{figure}

Figure \ref{fig:palist} shows the design of paired-array list. Tow arrays are linked in head-head manner.
To insert a new element on the head of the sequence, the element is appended at the end of front list;
To append a new element on the tail of the sequence, the element is appended at the end of rear list;

Here is a ISO C++ code snippet to define the this data structure.

\lstset{language=C++}
\begin{lstlisting}
template<typename Key>
struct List {
  int n, m;
  vector<Key> front;
  vector<Key> rear;

  List() : n(0), m(0) {}
  int size() { return n + m; }
};
\end{lstlisting}

Here we use vector provides in standard library to cover the dynamic memory management issues, so
that we can concentrate on the algorithm design.

\subsection{Insertion and appending}
\index{Paired-array list!Insertion and appending}
Suppose function \textproc{Front}($L$) returns the front array, while \textproc{Rear}($L$) returns the
rear array. For illustration purpose, we assume the arrays are dynamic allocated. inserting and appending
can be realized as the following.

\begin{algorithmic}
\Function{Insert}{$L, x$}
  \State $F \gets $ \Call{Front}{$L$}
  \State \Call{Size}{$F$} $\gets $ \Call{Size}{$F$} + 1
  \State $F$[\Call{Size}{$F$}] $\gets x$
\EndFunction
\Statex
\Function{Append}{$L, x$}
  \State $R \gets $ \Call{Rear}{$L$}
  \State \Call{Size}{$R$} $\gets $ \Call{Size}{$R$} + 1
  \State $R$[\Call{Size}{$R$}] $\gets x$
\EndFunction
\end{algorithmic}

As all the above operations manipulate the front and rear array on tail, they are all constant $O(1)$ time. And the following are the corresponding ISO C++ programs.

\begin{lstlisting}
template<typename Key>
void insert(List<Key>& xs, Key x) {
  ++xs.n;
  xs.front.push_back(x);
}

template<typename Key>
void append(List<Key>& xs, Key x) {
  ++xs.m;
  xs.rear.push_back(x);
}
\end{lstlisting}

\subsection{random access}
\index{Paired-array list!Random access}
As the inner data structure is array (dynamic array as vector), which supports random access
by nature, it's trivial to implement constant time indexing algorithm.

\begin{algorithmic}
\Function{Get}{$L, i$}
  \State $F \gets $ \Call{Front}{$L$}
  \State $n \gets $ \Call{Size}{$F$}
  \If{$i \leq n $}
    \State \Return $F$[$n-i+1$]
  \Else
    \State \Call{Rear}{$L$}[$i-n$]
  \EndIf
\EndFunction
\end{algorithmic}

Here the index $i \in [1, |L|]$ starts from 1. If it is not greater than the size of front
array, the element is stored in front. However, as front and rear arrays are connect head-to-head,
so the elements in front array are in reverse order. We need locate the element by subtracting
the size of front array by $i$; If the index $i$ is greater than the size of front array,
the element is stored in rear array. Since elements are stored in normal order in rear,
we just need subtract the index $i$ by an offset which is the size of front array.

Here is the ISO C++ program implements this algorithm.

\begin{lstlisting}
template<typename Key>
Key get(List<Key>& xs, int i) {
  if( i < xs.n )
    return xs.front[xs.n-i-1];
  else
    return xs.rear[i-xs.n];
}
\end{lstlisting}

The random mutating algorithm is left as an exercise to the reader.

\subsection{removing and balancing}
\index{Paired-array list!Removing and balancing}
Removing isn't as simple as insertion and appending. This is because we must handle the
condition that one array (either front or rear) becomes empty due to removal, while the
other still contains elements. In extreme case, the list turns to be quite unbalanced.
So we must fix it to resume the balance.

One idea is to trigger this fixing when either front or rear array becomes empty. We just
cut the other array in half, and reverse the first half to form the new pair. The algorithm
is described as the following.

\begin{algorithmic}
\Function{Balance}{$L$}
  \State $F \gets$ \Call{Front}{$L$}, $R \gets$ \Call{Rear}{$L$}
  \State $n \gets$ \Call{Size}{$F$}, $m \gets$ \Call{Size}{$R$}
  \If{ $F = \Phi$}
    \State $F \gets$ \Call{Reverse}{$R$[1 ... $\lfloor \frac{m}{2} \rfloor$]}
    \State $R \gets R[\lfloor \frac{m}{2} \rfloor + 1 ... m]$
  \ElsIf{ $R = \Phi$ }
    \State $R \gets$ \Call{Reverse}{$F$[1 ... $\lfloor \frac{n}{2} \rfloor$]}
    \State $F \gets F[\lfloor \frac{n}{2} \rfloor + 1 ... n]$
  \EndIf
\EndFunction
\end{algorithmic}

Actually, the operations are symmetric for the case that front is empty and the case that
rear is empty. Another approach is to swap the front and rear for one symmetric case
and recursive resumes the balance, then swap the front and rear back. For example
below ISO C++ program uses this method.

\begin{lstlisting}
template<typename Key>
void balance(List<Key>& xs) {
  if(xs.n == 0) {
    back_insert_iterator<vector<Key> > i(xs.front);
    reverse_copy(xs.rear.begin(), xs.rear.begin() + xs.m/2, i);
    xs.rear.erase(xs.rear.begin(), xs.rear.begin() +xs.m/2);
    xs.n = xs.m/2;
    xs.m -= xs.n;
  }
  else if(xs.m == 0) {
    swap(xs.front, xs.rear);
    swap(xs.n, xs.m);
    balance(xs);
    swap(xs.front, xs.rear);
    swap(xs.n, xs.m);
  }
}
\end{lstlisting}

With \textproc{Balance} algorithm defined, it's trivial to implement remove algorithm
both on head and on tail.

\begin{algorithmic}
\Function{Remove-Head}{$L$}
  \State \Call{Balance}{$L$}
  \State $F \gets $ \Call{Front}{$L$}
  \If{$F = \Phi$}
    \State \Call{Remove-Tail}{$L$}
  \Else
    \State \Call{Size}{$F$} $\gets $ \Call{Size}{$F$} - 1
  \EndIf
\EndFunction
\Statex
\Function{Remove-Tail}{$L$}
  \State \Call{Balance}{$L$}
  \State $R \gets $ \Call{Rear}{$L$}
  \If{$R = \Phi$}
    \State \Call{Remove-Head}{$L$}
  \Else
    \State \Call{Size}{$R$} $\gets $ \Call{Size}{$R$} - 1
  \EndIf
\EndFunction
\end{algorithmic}

There is an edge case for each, that is even after balancing, the array targeted to
perform removal is still empty. This happens that there is only one element stored
in the paired-array list. The solution is just remove this singleton left element,
and the overall list results empty. Below is the ISO C++ program implements this
algorithm.

\begin{lstlisting}
template<typename Key>
void remove_head(List<Key>& xs) {
  balance(xs);
  if(xs.front.empty())
    remove_tail(xs); //remove the singleton elem in rear
  else {
    xs.front.pop_back();
    --xs.n;
  }
}

template<typename Key>
void remove_tail(List<Key>& xs) {
  balance(xs);
  if(xs.rear.empty())
    remove_head(xs); //remove the singleton elem in front
  else {
    xs.rear.pop_back();
    --xs.m;
  }
}
\end{lstlisting}

It's obvious that the worst case performance is $O(n)$ where $n$ is the number of elements
stored in paired-array list. This happens when balancing is triggered, and both reverse
and shifting are linear operation. However, the amortized performance of removal is still
$O(1)$, the proof is left as exercise to the reader.

\begin{Exercise}
\begin{enumerate}
\item Implement the random mutating algorithm in your favorite imperative programming language.
\item We utilized vector provided in standard library to manage memory dynamically, try to realize
a version using plain array and manage the memory allocation manually. Compare this version and
consider how does this affect the performance?
\item Prove that the amortized performance of removal is $O(1)$ for paired-array list.
\end{enumerate}
\end{Exercise}

\section{Concatenate-able list}
\index{Sequence!Concatenate-able list}
By using binary random access list, we realized sequence data structure which
supports $O(\lg n)$ time insertion and removal on head, as well as random accessing element
with a given index.

However, it's not so easy to concatenate two lists. As both lists are forests of
complete binary trees, we can't merely merge them (Since forests are essentially
list of trees, and for any size, there is at most one tree of that size. Even
concatenate forests directly is not fast). One solution is to push the element
from the first sequence one by one to a stack and then pop those elements and insert
them to the head of the second one by using `cons' function. Of course the
stack can be implicitly used in recursion manner, for instance:

\be
concat(s_1, s_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  s_2 & s_1 = \Phi \\
  cons(head(s_1), concat(tail(s_1), s_2)) & otherwise
  \end{array}
\right .
\ee

Where function $cons()$, $head()$ and $tail()$ are defined in previous section.

If the length of the two sequence is $n$, and $m$, this method takes $O(n \lg n)$
time repeatedly push all elements from the first sequence to stacks, and then
takes $\Omega(N \lg (N + M))$ to insert the elements in front of the second sequence.
Note that $\Omega$ means the upper limit, There is detailed definition for it in
\cite{CLRS}.

We have already implemented the real-time queue in previous chapter. It supports
$O(1)$ time pop and push. If we can turn the sequence concatenation to a kind
of pushing operation to queue, the performance will be improved to $O(1)$ as well.
Okasaki gave such realization in \cite{okasaki-book}, which can concatenate
lists in constant time.

To represent a concatenate-able list, the data structure designed by Okasaki is
essentially a K-ary tree. The root of the tree stores the first element in the
list. So that we can access it in constant $O(1)$ time. The sub-trees or children
are all small concatenate-able lists, which are managed by real-time queues.
Concatenating another list to the end is just adding it as the last child, which
is in turn a queue pushing operation. Appending a new element can be realized
as that, first wrapping the element to a singleton tree, which is a leaf with
no children. Then, concatenate this singleton to finalize appending.

Figure \ref{fig:clist} illustrates this data structure.

\begin{figure}[htbp]
  \centering
  \subfloat[The data structure for list $\{ x_1, x_2, ..., x_n\}$]{\includegraphics[scale=0.5]{img/clist.ps}} \\
  \subfloat[The result after concatenated with list $\{y_1, y_2, ..., y_m\}$]{\includegraphics[scale=0.4]{img/clist1.ps}}
  \caption{Data structure for concatenate-able list} \label{fig:clist}
\end{figure}

Such recursively designed data structure can be defined in the following
Haskell code.

\lstset{language=Haskell}
\begin{lstlisting}
data CList a = Empty | CList a (Queue (CList a)) deriving (Show, Eq)
\end{lstlisting}

It means that a concatenate-able list is either empty or a K-ary tree, which
again consists of a queue of concatenate-able sub-lists and a root element.
Here we reuse the realization of real-time queue mentioned in previous
chapter.

Suppose function $clist(x, Q)$ constructs a concatenate-able list from
an element $x$, and a queue of sub-lists $Q$. While function $root(s)$
returns the root element of such K-ary tree implemented list. and
function $queue(s)$ returns the queue of sub-lists respectively.
We can implement the algorithm
to concatenate two lists like this.

\be
concat(s_1, s_2) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  s_1 & s_2 = \Phi \\
  s_2 & s_1 = \Phi \\
  clist(x, push(Q, s_2)) & otherwise
  \end{array}
\right .
\ee

Where $x = root(s_1)$ and $Q = queue(s_1)$. The idea of concatenation is that
if either one of the list to be concatenated is empty, the result is
just the other list; otherwise, we push the second list as the last
child to the queue of the first list.

Since the push operation is $O(1)$ constant time for a well realized
real-time queue, the performance of concatenation is bound to $O(1)$.

The $concat()$ function can be translated to the below Haskell program.

\begin{lstlisting}
concat x Empty = x
concat Empty y = y
concat (CList x q) y = CList x (push q y)
\end{lstlisting}

Besides the good performance of concatenation, this design also brings
satisfied features for adding element both on head and tail.

\be
cons(x, s) = concat(clist(x, \Phi), s)
\ee

\be
append(s, x) = concat(s, clist(x, \Phi))
\ee

It's a bit complex to realize the algorithm that removes the first
element from a concatenate-able list. This is because after the root,
which is the first element in the sequence got removed, we have to
re-construct the rest things, a queue of sub-lists, to a K-ary
tree.

Before diving into the re-construction, let's solve the trivial
part first. Getting the first element is just returning the root
of the K-ary tree.

\be
head(s) = root(s)
\ee

As we mentioned above, after root being removed, there left
all children of the K-ary tree. Note that all of them are also
concatenate-able list, so that one natural solution is to
concatenate them all together to a big list.

\be
concatAll(Q) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & Q = \Phi \\
  concat(front(Q), concatAll(pop(Q))) & otherwise
  \end{array}
\right .
\ee

Where function $front()$ just returns the first element from a
queue without removing it, while $pop()$ does the removing work.

If the queue is empty, it means that there is no children at all, so
the result is also an empty list; Otherwise, we pop the first child,
which is a concatenate-able list, from the queue, and recursively
concatenate all the rest children to a list; finally, we concatenate this list
behind the already popped first children.

With $concatAll()$ defined, we can then implement the algorithm
of removing the first element from a list as below.

\be
tail(s) = linkAll(queue(s))
\ee

The corresponding Haskell program is given like the following.

\begin{lstlisting}
head (CList x _) = x
tail (CList _ q) = linkAll q

linkAll q | isEmptyQ q = Empty
          | otherwise = link (front q) (linkAll (pop q))
\end{lstlisting}

Function \verb|isEmptyQ| is used to test a queue is empty, it is trivial and
we omit its definition. Readers can refer to the source code along with
this book.

$linkAll()$ algorithm actually traverses the queue data structure,
and reduces to a final result. This remind us of {\em folding} mentioned
in the chapter of binary search tree. readers can refer to the
appendix of this book for the detailed description of folding.
It's quite possible to define a folding algorithm for queue instead
of list\footnote{Some functional programming language, such as
Haskell, defined type class, which is a concept of monoid so that
it's easy to support folding on a customized data structure.}
\cite{learn-haskell}.

\be
foldQ(f, e, Q) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & Q = \Phi \\
  f(front(Q), foldQ(f, e, pop(Q))) & otherwise
  \end{array}
\right .
\ee

Function $foldQ()$ takes three parameters, a function $f$, which is used
for reducing, an initial value $e$, and the queue $Q$ to be traversed.

Here are some examples to illustrate folding on queue. Suppose
a queue $Q$ contains elements $\{ 1, 2, 3, 4, 5 \}$ from head to tail.

\[
\begin{array}{l}
foldQ(+, 0, Q) = 1 + (2 + (3 + (4 + (5 + 0)))) = 15 \\
foldQ(\times, 1, Q) = 1 \times (2 \times (3 \times (4 \times (5 \times 1)))) = 120 \\
foldQ(\times, 0, Q) = 1 \times (2 \times (3 \times (4 \times (5 \times 0)))) = 0 \\
\end{array}
\]

Function $linkAll$ can be changed by using $foldQ$ accordingly.

\be
linkAll(Q) = foldQ(link, \Phi, Q)
\ee

The Haskell program can be modified as well.

\begin{lstlisting}
linkAll = foldQ link Empty

foldQ :: (a -> b -> b) -> b -> Queue a -> b
foldQ f z q | isEmptyQ q = z
            | otherwise = (front q) `f` foldQ f z (pop q)
\end{lstlisting}

However, the performance of removing can't be ensured in all cases.
The worst case is that, user keeps appending $n$ elements to a empty
list, and then immediately performs removing. At this time, the K-ary
tree has the first element stored in root. There are $n-1$ children,
all are leaves. So $linkAll()$ algorithm downgrades to $O(n)$ which
is linear time.

The average case is amortized $O(1)$, if the add, append, concatenate
and removing operations are randomly performed. The proof is left as
en exercise to the reader.

\begin{Exercise}
\begin{enumerate}
\item Can you figure out a solution to append an element to the end of a binary
random access list?

\item Prove that the amortized performance of removal operation is
$O(1)$. Hint: using the banker's method.

\item Implement the concatenate-able list in your favorite imperative language.
\end{enumerate}
\end{Exercise}

% =========================================
%  Finger tree
% =========================================
\section{Finger tree}
\index{Sequence!finger tree}
We haven't been able to meet all the performance targets listed at the beginning
of this chapter.

Binary random access list enables to insert, remove element
on the head of sequence, and random access elements fast. However, it performs
poor when concatenates lists. There is no good way to append element at the
end of binary access list.

Concatenate-able list is capable to concatenates multiple lists in a fly, and
it performs well for adding new element both on head and tail. However, it doesn't
support randomly access element with a given index.

These two examples bring us some ideas:

\begin{itemize}
\item In order to support fast manipulation both on head and tail of the sequence,
there must be some way to easily access the head and tail position;
\item Tree like data structure helps to turn the random access into divide and
conquer search, if the tree is well balance, the search can be ensured to
be logarithm time.
\end{itemize}

\subsection{Definition}
\index{Finger tree!Definition}
Finger tree\cite{finger-tree-1977}, which was first invented in 1977, can help to
realize efficient sequence. And it is also well implemented in purely functional
settings\cite{finger-tree-2006}.

As we mentioned that the balance of the tree is critical to ensure the performance
for search. One option is to use balanced tree as the under ground data structure
for finger tree. For example the 2-3 tree, which is a special B-tree. (readers
can refer to the chapter of B-tree of this book).

A 2-3 tree either contains 2 children or 3. It can be defined as below in Haskell.

\lstset{language=Haskell}
\begin{lstlisting}
data Node a = Br2 a a | Br3 a a a
\end{lstlisting}

In imperative settings, node can be defined with a list of sub nodes, which contains
at most 3 children. For instance the following ANSI C code defines node.

\lstset{language=C}
\begin{lstlisting}
union Node {
  Key* keys;
  union Node* children;
};
\end{lstlisting}

Note in this definition, a node can either contain $2 \sim 3$ keys, or $2 \sim 3$ sub nodes.
Where key is the type of elements stored in leaf node.

We mark the left-most none-leaf node as the front finger and the right-most
none-leaf node as the rear finger. Since both fingers are essentially
2-3 trees with all leafs as children, they can be directly represented as
list of 2 or 3 leafs. Of course a finger tree can be empty or contain
only one element as leaf.

So the definition of a finger tree is specified like this.

\begin{itemize}
\item A finger tree is either empty;
\item or a singleton leaf;
\item or contains three parts: a left finger which is a list contains at most
3 elements; a sub finger tree; and a right finger which is also a list contains
at most 3 elements.
\end{itemize}

Note that this definition is recursive, so it's quite possible to be translated
to functional settings. The following Haskell definition summaries these cases
for example.

\lstset{language=Haskell}
\begin{lstlisting}
data Tree a = Empty
            | Lf a
            | Tr [a] (Tree (Node a)) [a]
\end{lstlisting}

In imperative settings, we can define the finger tree in a similar manner. What's more,
we can add a parent field, so that it's possible to back-track to root from any tree node.
Below ANSI C code defines finger tree accordingly.

\lstset{language=C}
\begin{lstlisting}
struct Tree {
  union Node* front;
  union Node* rear;
  Tree* mid;
  Tree* parent;
};
\end{lstlisting}

We can use NIL pointer to represent an empty tree; and a leaf tree contains only one element
in its front finger, both its rear finger and middle part are empty;

Figure \ref{fig:ftr-example-1} and \ref{fig:ftr-example-2} show some examples
of figure tree.

\begin{figure}[htbp]
  \centering
  \subfloat[An empty tree]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/ftr-empty.ps}\hspace{0.2\textwidth}}
  \subfloat[A singleton leaf]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/ftr-leaf.ps}\hspace{0.2\textwidth}} \\
  \subfloat[Front finger and rear finger contain one element for each, the middle part is empty]{\includegraphics[scale=0.5]{img/ftr-ab.ps}}
  \caption{Examples of finger tree, 1} \label{fig:ftr-example-1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \subfloat[After inserting extra 3 elements to front finger, it exceeds the 2-3 tree constraint, which isn't balanced any more]{\includegraphics[scale=0.5]{img/ftr-abcde.ps}}
  \hspace{0.2\textwidth}
  \subfloat[The tree resumes balancing. There are 2 elements in front finger; The middle part is a leaf, which contains a 3-branches 2-3 tree.]{\includegraphics[scale=0.5]{img/ftr-abcdef.ps}}
  \caption{Examples of finger tree, 2} \label{fig:ftr-example-2}
\end{figure}

The first example is an empty finger tree; the second one shows the result after
inserting one element to empty, it becomes a leaf of one node; the third example
shows a finger tree contains 2 elements, one is in front finger, the other is
in rear;

If we continuously insert new elements, to the tree, those elements will be
put in the front finger one by one, until it exceeds the limit of 2-3 tree.
The 4-th example shows such condition, that there are 4 elements in front
finger, which isn't balanced any more.

The last example shows that the finger tree gets fixed so that it resumes
balancing. There are two elements in the front finger. Note that the middle
part is not empty any longer. It's a leaf of a 2-3 tree. The content of the
leaf is a tree with 3 branches, each contains an element.

We can express these 5 examples as the following Haskell expression.

\lstset{language=Haskell}
\begin{lstlisting}
Empty
Lf a
[b] Empty [a]
[e, d, c, b] Empty [a]
[f, e] Lf (Br3 d c b) [a]
\end{lstlisting}

As we mentioned that the definition of finger tree is recursive.
The middle part besides the front and rear finger is a deeper finger tree,
which is defined as $Tree(Node(a))$. Every time we go deeper, the
$Node()$ is embedded one more level. if the element type of the first level tree
is $a$, the element type for the second level tree is $Node(a)$,
the third level is $Node(Node(a))$, ..., the n-th level is
$Node(Node(Node(...(a))...)) = Node^n(a)$, where $^n$ indicates the $Node()$
is applied $n$ times.

\subsection{Insert element to the head of sequence}
\index{Finger tree!Insert to head}

The examples list above actually reveal the typical process that the
elements are inserted one by one to a finger tree. It's possible to summarize
these examples to some cases for insertion on head algorithm.

When we insert an element $x$ to a finger tree $T$,
\begin{itemize}
\item If the tree is empty, the result is a leaf which contains the singleton element $x$;
\item If the tree is a singleton leaf of element $y$, the result is a new finger tree. The front finger contains the new element $x$, the rear finger contains the previous element $y$; the middle part is a empty finger tree;
\item If the number of elements stored in front finger isn't bigger than the upper limit of 2-3 tree, which is 3, the new element is just inserted to the head
of front finger;
\item otherwise, it means that the number of elements stored in front finger exceeds the upper limit of 2-3 tree. the last 3 elements in front finger is wrapped in a 2-3 tree and recursively inserted to the middle part. the new element $x$ is inserted in front of the rest elements in front finger.
\end{itemize}

Suppose that function $leaf(x)$ creates a leaf of element $x$, function
$tree(F, T', R)$ creates a finger tree from three part: $F$ is the front
finger, which is a list contains several elements. Similarity, $R$ is the
rear finger, which is also a list. $T'$ is the middle part which is a
deeper finger tree. Function $tr3(a, b, c)$ creates a 2-3 tree from 3
elements $a, b, c$; while $tr2(a, b)$ creates a 2-3 tree from 2 elements
$a$ and $b$.

\be
insertT(x, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree(\{x\}, \Phi, \{y\}) & T = leaf(y) \\
  tree(\{x, x_1\}, insertT(tr3(x_2, x_3, x_4), T'), R) & T = tree(\{x_1, x_2, x_3, x_4\}, T', R) \\
  tree(\{x\} \cup F, T', R) & otherwise
  \end{array}
\right .
\ee

The performance of this algorithm is dominated by the recursive case. All the
other cases are constant $O(1)$ time. The recursion depth is proportion to
the height of the tree, so the algorithm is bound to $O(h)$ time, where $h$ is
the height. As we use 2-3 tree to ensure that the tree is well balanced,
$h = O(\lg n)$, where $n$ is the number of elements stored in the finger tree.

More analysis reveal that the amortized performance of $insertT()$ is $O(1)$
because we can amortize the expensive recursion case to other trivial cases.
Please refer to \cite{okasaki-book} and \cite{finger-tree-2006} for the detailed proof.

Translating the algorithm yields the below Haskell program.

\begin{lstlisting}
cons :: a -> Tree a -> Tree a
cons a Empty = Lf a
cons a (Lf b) = Tr [a] Empty [b]
cons a (Tr [b, c, d, e] m r) = Tr [a, b] (cons (Br3 c d e) m) r
cons a (Tr f m r) = Tr (a:f) m r
\end{lstlisting}

Here we use the LISP naming convention to illustrate inserting a
new element to a list.

The insertion algorithm can also be implemented in imperative approach. Suppose
function \textproc{Tree}() creates an empty tree, that all fields, including
front and rear finger, the middle part inner tree and parent are empty.
Function \textproc{Node}() creates an empty node.

\begin{algorithmic}
\Function{Prepend-Node}{$n, T$}
  \State $r \gets $ \textproc{Tree}()
  \State $p \gets r$
  \State \Call{Connect-Mid}{$p, T$}
  \While{\textproc{Full?}(\Call{Front}{$T$})}
    \State $F \gets $ \Call{Front}{$T$}  \Comment{$F = \{n_1, n_2, n_3, ...\}$}
    \State \Call{Front}{$T$} $\gets$ $\{n, F[1]\}$  \Comment{$F[1] = n_1$}
    \State $n \gets$ \textproc{Node}()
    \State \Call{Children}{$n$} $\gets F[2..]$  \Comment{$F[2..] = \{ n_2, n_3, ... \}$}
    \State $p \gets T$
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile
  \If{$T =$ NIL}
    \State $T \gets$ \textproc{Tree}()
    \State \Call{Front}{$T$}$\gets \{ n \}$
  \ElsIf{ $|$ \Call{Front}{$T$} $|$ = 1 $\land$ \Call{Rear}{$T$} = $\Phi$}
    \State \Call{Rear}{$T$} $\gets$ \Call{Front}{$T$}
    \State \Call{Front}{$T$} $\gets \{ n \}$
  \Else
    \State \Call{Front}{$T$} $\gets \{ n \} \cup $ \Call{Front}{$T$}
  \EndIf
  \State \Call{Connect-Mid}{$p, T$} $\gets T$
  \State \Return \Call{Flat}{$r$}
\EndFunction
\end{algorithmic}

Where the notation $L[i..]$ means a sub list of $L$ with the first $i-1$
elements removed, that if $L = \{a_1, a_2, ..., a_n\}$, $L[i..] = \{a_i, a_{i+1}, ..., a_n\}$.

Functions \textproc{Front}(), \textproc{Rear}(), \textproc{Mid}(), and \textproc{Parent}()
are used to access the front finger, the rear finger, the middle part inner tree and
the parent tree respectively; Function \textproc{Children}() accesses the children of a
node.

Function \textproc{Connect-Mid}($T_1, T_2$), connect $T_2$ as the inner middle part tree of
$T_1$, and set the parent of $T_2$ as $T_1$ if $T_2$ isn't empty.

In this algorithm, we performs a one pass top-down traverse along the middle part inner tree
if the front finger is full that it can't afford to store any more. The criteria for full
for a 2-3 tree is that the finger contains 3 elements already. In such case, we
extract all the elements except the first one off, wrap them to a new node (one level deeper node),
and continuously insert this new node to its middle inner tree. The first element is left
in the front finger, and the element to be inserted is put in front of it, so that this
element becomes the new first one in the front finger.

After this traversal, the algorithm either reach an empty tree, or the tree still has room
to hold more element in its front finger. We create a new leaf for the former case, and
perform a trivial list insert to the front finger for the latter.

During the traversal, we use $p$ to record the parent of the current tree we are processing.
So any new created tree are connected as the middle part inner tree to $p$.

Finally, we return the root of the tree $r$. The last trick of this algorithm is the \textproc{Flat}()
function. In order to simplify the logic, we create an empty `ground' tree and set
it as the parent of the root. We need eliminate this extra `ground' level before return the
root. This flatten algorithm is realized as the following.

\begin{algorithmic}
\Function{Flat}{$T$}
  \While{$T \neq$ NIL $\land T$ is empty}
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile
  \If{$T \neq \Phi$}
    \State \Call{Parent}{$T$} $\gets \Phi$
  \EndIf
  \State \Return $T$
\EndFunction
\end{algorithmic}

The while loop test if $T$ is trivial empty, that it's not NIL($=\Phi$), while both its front
and rear fingers are empty.

Below Python code implements the insertion algorithm for finger tree.

\lstset{language=Python}
\begin{lstlisting}
def insert(x, t):
    return prepend_node(wrap(x), t)

def prepend_node(n, t):
    root = prev = Tree()
    prev.set_mid(t)
    while frontFull(t):
        f = t.front
        t.front = [n] + f[:1]
        n = wraps(f[1:])
        prev = t
        t = t.mid
    if t is None:
        t = leaf(n)
    elif len(t.front)==1 and t.rear == []:
        t = Tree([n], None, t.front)
    else:
        t = Tree([n]+t.front, t.mid, t.rear)
    prev.set_mid(t)
    return flat(root)

def flat(t):
    while t is not None and t.empty():
        t = t.mid
    if t is not None:
        t.parent = None
    return t
\end{lstlisting}

The implementation of function '\verb|set_mid()|', '\verb|frontFull()|', '\verb|wrap()|',
'\verb|wraps()|', '\verb|empty()|', and tree
constructor are trivial enough, that we skip the detail of them here. Readers can take these as
exercises.

\subsection{Remove element from the head of sequence}
\index{Finger tree!Remove from head}

It's easy to implement the reverse operation
that remove the first element from the list by
reversing the $insertT()$ algorithm line by line.

Let's denote $F = \{f_1, f_2, ...\}$ is the front finger list,
$M$ is the middle part inner finger tree. $R = \{r_1, r_2, ...\}$
is the rear finger list of a finger tree,
and $R' = \{r_2, r_3, ... \}$ is the rest of element with the first one
removed from $R$.

\be
extractT(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (x, \Phi) & T = leaf(x) \\
  (x, leaf(y)) & T = tree(\{x\}, \Phi, \{y\}) \\
  (x, tree(\{r_1\}, \Phi, R')) & T = tree(\{x\}, \Phi, R) \\
  (x, tree(toList(F'), M', R)) & T = tree(\{x\}, M, R), (F', M') = extractT(M)\\
  (f_1, tree(\{f_2, f_3, ...\}, M, R)) & otherwise
  \end{array}
\right .
\ee

Where function $toList(T)$ converts a 2-3 tree to plain list as the
following.

\be
toList(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{x, y\} & T = tr2(x, y) \\
  \{x, y, z\} & T = tr3(x, y, z)
  \end{array}
\right .
\ee

Here we skip the error handling such as trying to remove element from
empty tree etc. If the finger tree is a leaf, the result after removal
is an empty tree; If the finger tree contains two elements, one in the
front rear, the other in rear, we return the element stored in front rear
as the first element, and the resulted tree after removal is a leaf;
If there is only one element in front finger, the middle part inner tree
is empty, and the rear finger isn't empty, we return the only element
in front finger, and borrow one element from the rear finger to
front; If there is only one element in front finger, however, the
middle part inner tree isn't empty, we can recursively remove a node
from the inner tree, and flatten it to a plain list to replace the
front finger, and remove the original only element in front finger;
The last case says that if the front finger contains more than one
element, we can just remove the first element from front finger
and keep all the other part unchanged.

\begin{figure}[htbp]
  \centering
  \subfloat[A sequence of 10 elements represented as a finger tree]{\includegraphics[scale=0.4]{img/ftr-10.ps}} \\
  \subfloat[The first element is removed. There is one element left in front finger.]{\includegraphics[scale=0.4]{img/ftr-9.ps}} \\
  \subfloat[Another element is remove from head. We borrowed one node from the middle part inner tree, change the node, which is a 2-3 tree to a list, and use it as the new front finger. the middle part inner tree becomes a leaf of one 2-3 tree node.]{\hspace{0.2\textwidth}\includegraphics[scale=0.4]{img/ftr-8.ps}\hspace{0.2\textwidth}}
  \caption{Examples of remove 2 elements from the head of a sequence} \label{fig:ftr-uncons-example}
\end{figure}

Figure \ref{fig:ftr-uncons-example} shows the steps of removing two elements from
the head of a sequence. There are 10 elements stored in the finger tree.
When the first element is removed, there is still one element left in the front finger.
However, when the next element is removed, the front finger is empty. So we `borrow'
one tree node from the middle part inner tree. This is a 2-3 tree. it is converted
to a list of 3 elements, and the list is used as the new finger. the middle part
inner tree change from three parts to a singleton leaf, which contains only one
2-3 tree node. There are three elements stored in this tree node.

Below is the corresponding Haskell program for `uncons'.

\lstset{language=Haskell}
\begin{lstlisting}
uncons :: Tree a -> (a, Tree a)
uncons (Lf a) = (a, Empty)
uncons (Tr [a] Empty [b]) = (a, Lf b)
uncons (Tr [a] Empty (r:rs)) = (a, Tr [r] Empty rs)
uncons (Tr [a] m r) = (a, Tr (nodeToList f) m' r) where (f, m') = uncons m
uncons (Tr f m r) = (head f, Tr (tail f) m r)
\end{lstlisting}

And the function $nodeToList$ is defined like this.

\begin{lstlisting}
nodeToList :: Node a -> [a]
nodeToList (Br2 a b) = [a, b]
nodeToList (Br3 a b c) = [a, b, c]
\end{lstlisting}

Similar as above, we can define $head$ and $tail$ function from
$uncons$.

\begin{lstlisting}
head = fst . uncons
tail = snd . uncons
\end{lstlisting}

\subsection{Handling the ill-formed finger tree when removing}
\index{Finger tree!Ill-formed tree}
The strategy used so far to remove element from finger tree is a kind of removing and borrowing.
If the front finger becomes empty after removing, we borrows more nodes from the middle part
inner tree. However there exists cases that the tree is ill-formed, for example, both the
front fingers of the tree and its middle part inner tree are empty. Such ill-formed tree
can result from imperatively splitting, which we'll introduce later.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.4]{img/ftr-illed-1.ps}
  \caption{Example of an ill-formed tree. The front finger of the i-th level sub tree isn't empty.} \label{fig:ftr-illed-form}
\end{figure}

Here we developed an imperative algorithm which can remove the first element from finger tree
even it is ill-formed. The idea is first perform a top-down traverse to find a sub tree which
either has a non-empty front finger or both its front finger and middle part inner tree are empty.
For the former case, we can safely extract the first element which is a node from the front finger;
For the latter case, since only the rear finger isn't empty, we can swap it with the empty front
finger, and change it to the former case.

After that, we need examine the node we extracted from the front finger is leaf node (How to do
that? this is left as an exercise to the reader). If not, we need go on extracting the first
sub node from the children of this node, and left the rest of other children as the new
front finger to the parent of the current tree. We need repeatedly go up along with the
parent field till the node we extracted is a leaf. At that time point, we arrive at the
root of the tree. Figure \ref{fig:ftr-illed-extract} illustrates this process.

\begin{figure}[htbp]
  \centering
  \subfloat[Extract the first element n{[i]}{[1]} and put its children to the front finger of upper level tree.]{\includegraphics[scale=0.4]{img/ftr-illed-2.ps}}
  \subfloat[Repeat this process $i$ times, and finally x{[1]} is extracted.]{\includegraphics[scale=0.4]{img/ftr-illed-i.ps}}
  \caption{Traverse bottom-up till a leaf is extracted.} \label{fig:ftr-illed-extract}
\end{figure}

Based on this idea, the following algorithm realizes the removal operation on head. The
algorithm assumes that the tree passed in isn't empty.

\begin{algorithmic}
\Function{Extract-Head}{$T$}
  \State $r \gets$ \textproc{Tree}()
  \State \Call{Connect-Mid}{$r, T$}
  \While{\Call{Front}{$T$} $= \Phi \land $ \Call{Mid}{$T$} $\neq $ NIL}
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile

  \If{\Call{Front}{$T$} $ = \Phi \land $ \Call{Rear}{$T$} $\neq \Phi$}
    \State \textproc{Exchange} \Call{Front}{$T$} $\leftrightarrow$ \Call{Rear}{$T$}
  \EndIf

  \State $n \gets $ \textproc{Node}()
  \State \Call{Children}{$n$} $\gets$ \Call{Front}{$T$}
  \Repeat
    \State $L \gets$ \Call{Children}{$n$} \Comment{$L = \{n_1, n_2, n_3, ...\}$}
    \State $n \gets L[1]$ \Comment{$ n \gets n_1$}
    \State \Call{Front}{$T$} $\gets L[2..]$ \Comment{$L[2..] = \{n_2, n_3, ...\}$}
    \State $T \gets $ \Call{Parent}{$T$}
    \If{\Call{Mid}{$T$} becomes empty}
      \State \Call{Mid}{$T$} $\gets$ NIL
    \EndIf
  \Until{$n$ is a leaf}
  \State \Return (\Call{Elem}{$n$}, \Call{Flat}{$r$})
\EndFunction
\end{algorithmic}

Note that function \textproc{Elem}($n$) returns the only element stored inside leaf node $n$.
Similar as imperative insertion algorithm, a stub `ground' tree is used as the parent of
the root, which can simplify the logic a bit. That's why we need flatten the tree finally.

Below Python program translates the algorithm.

\lstset{language=Python}
\begin{lstlisting}
def extract_head(t):
    root = Tree()
    root.set_mid(t)
    while t.front == [] and t.mid is not None:
        t = t.mid
    if t.front == [] and t.rear != []:
        (t.front, t.rear) = (t.rear, t.front)
    n = wraps(t.front)
    while True: # a repeat-until loop
        ns = n.children
        n = ns[0]
        t.front = ns[1:]
        t = t.parent
        if t.mid.empty():
            t.mid.parent = None
            t.mid = None
        if n.leaf:
            break
    return (elem(n), flat(root))
\end{lstlisting}

Member function \verb|Tree.empty()| returns true if all the three parts - the front finger,
the rear finger and the middle part inner tree - are empty. We put a flag \verb|Node.leaf|
to mark if a node is a leaf or compound node. The exercise of this section asks the reader
to consider some alternatives.

As the ill-formed tree is allowed, the algorithms to access the first and last element of the
finger tree must be modified, so that they don't blindly return the first or last child
of the finger as the finger can be empty if the tree is ill-formed.

The idea is quite similar to the \textproc{Extract-Head}, that in case the finger is empty
while the middle part inner tree isn't, we need traverse along with the inner tree till a
point that either the finger becomes non-empty or all the nodes are stored in the
other finger. For instance, the following algorithm can return the first leaf node even
the tree is ill-formed.

\begin{algorithmic}
\Function{First-Lf}{$T$}
  \While{\Call{Front}{$T$} $ = \Phi \land $ \Call{Mid}{$T$} $\neq$ NIL}
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile
  \If{\Call{Front}{$T$} $ = \Phi \land$ \Call{Rear}{$T$} $\neq \Phi$}
    \State $n \gets$ \Call{Rear}{$T$}[1]
  \Else
    \State $n \gets$ \Call{Front}{$T$}[1]
  \EndIf
  \While{$n$ is NOT leaf}
    \State $n \gets$ \Call{Children}{$n$}[1]
  \EndWhile
  \State \Return $n$
\EndFunction
\end{algorithmic}

Note the second loop in this algorithm that it keeps traversing on the first sub-node
if current node isn't a leaf. So we always get a leaf node and it's trivial to get
the element inside it.

\begin{algorithmic}
\Function{First}{$T$}
  \State \Return \textproc{Elem}(\Call{First-Lf}{$T$})
\EndFunction
\end{algorithmic}

The following Python code translates the algorithm to real program.

\lstset{language=Python}
\begin{lstlisting}
def first(t):
    return elem(first_leaf(t))

def first_leaf(t):
    while t.front == [] and t.mid is not None:
        t = t.mid
    if t.front == [] and t.rear != []:
        n = t.rear[0]
    else:
        n = t.front[0]
    while not n.leaf:
        n = n.children[0]
    return n
\end{lstlisting}

To access the last element is quite similar, and we left it as an exercise to the reader.

\subsection{append element to the tail of the sequence}
\index{Finger tree!Append to tail}

Because finger tree is symmetric, we can give the realization of appending element on tail
by referencing to $insertT()$ algorithm.

\be
appendT(T, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree(\{y\}, \Phi, \{x\}) & T = leaf(y) \\
  tree(F, appendT(M, tr3(x_1, x_2, x_3)), \{x_4, x\}) & T = tree(F, M, \{x_1, x_2, x_3, x_4\}) \\
  tree(F, M, R \cup \{x\}) & otherwise
  \end{array}
\right .
\ee

Generally speaking, if the rear finger is still valid 2-3 tree, that the number of elements
is not greater than 4, the new elements is directly appended to rear finger.
Otherwise, we break the rear finger, take the first 3 elements in rear finger to create a
new 2-3 tree, and recursively append it to the middle part inner tree.
If the finger tree is empty or a singleton leaf, it will be handled in the first two cases.

Translating the equation to Haskell yields the below program.

\lstset{language=Python}
\begin{lstlisting}
snoc :: Tree a -> a -> Tree a
snoc Empty a = Lf a
snoc (Lf a) b = Tr [a] Empty [b]
snoc (Tr f m [a, b, c, d]) e = Tr f (snoc m (Br3 a b c)) [d, e]
snoc (Tr f m r) a = Tr f m (r++[a])
\end{lstlisting}

Function name `\verb|snoc|' is mirror of '\verb|cons|', which indicates the symmetric relationship.

Appending new element to the end imperatively is quite similar. The following algorithm
realizes appending.

\begin{algorithmic}
\Function{Append-Node}{$T, n$}
  \State $r \gets $ \textproc{Tree}()
  \State $p \gets r$
  \State \Call{Connect-Mid}{$p, T$}
  \While{\textproc{Full?}(\Call{Rear}{$T$})}
    \State $R \gets $ \Call{Rear}{$T$} \Comment{$R = \{n_1, n_2, ..., , n_{m-1}, n_m \}$}
    \State \Call{Rear}{$T$} $\gets$ $\{n, $ \Call{Last}{$R$} $\}$ \Comment{last element $n_m$}
    \State $n \gets$ \textproc{Node}()
    \State \Call{Children}{$n$} $\gets R[1...m-1]$ \Comment{ $\{n1, n2, ..., n_{m-1}\}$}
    \State $p \gets T$
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile
  \If{$T =$ NIL}
    \State $T \gets$ \textproc{Tree}()
    \State \Call{Front}{$T$} $\gets \{ n \}$
  \ElsIf{ $|$ \Call{Rear}{$T$} $|$ = 1 $\land$ \Call{Front}{$T$} = $\Phi$}
    \State \Call{Front}{$T$} $\gets$ \Call{Rear}{$T$}
    \State \Call{Rear}{$T$} $\gets \{ n \}$
  \Else
    \State \Call{Rear}{$T$} $\gets$ \Call{Rear}{$T$} $\cup \{ n \} $
  \EndIf
  \State \Call{Connect-Mid}{$p, T$} $\gets T$
  \State \Return \Call{Flat}{$r$}
\EndFunction
\end{algorithmic}

And the corresponding Python programs is given as below.

\lstset{language=Python}
\begin{lstlisting}
def append_node(t, n):
    root = prev = Tree()
    prev.set_mid(t)
    while rearFull(t):
        r = t.rear
        t.rear = r[-1:] + [n]
        n = wraps(r[:-1])
        prev = t
        t = t.mid
    if t is None:
        t = leaf(n)
    elif len(t.rear) == 1 and t.front == []:
        t = Tree(t.rear, None, [n])
    else:
        t = Tree(t.front, t.mid, t.rear + [n])
    prev.set_mid(t)
    return flat(root)
\end{lstlisting}

\subsection{remove element from the tail of the sequence}
\index{Finger tree!Remove from tail}

Similar to $appendT()$, we can realize the algorithm which remove the last element from
finger tree in symmetric manner of $extractT()$.

We denote the non-empty, non-leaf finger tree as $tree(F, M, R)$, where $F$ is the
front finger, $M$ is the middle part inner tree, and $R$ is the rear finger.

\be
removeT(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x) & T = leaf(x) \\
  (leaf(y), x) & T = tree(\{y\}, \Phi, \{x\}) \\
  (tree(init(F), \Phi, last(F)), x) & T = tree(F, \Phi, \{x\}) \land F \neq \Phi \\
  (tree(F, M', toList(R')), x) & T = tree(F, M, \{x\}), (M', R') = removeT(M) \\
  (tree(F, M, init(R)), last(R)) & otherwise
  \end{array}
\right .
\ee

Function $toList(T)$ is used to flatten a 2-3 tree to plain list, which is defined
previously. Function $init(L)$ returns all elements except for the last one in list $L$,
that if $L = \{a_1, a_2, ..., a_{n-1}, a_n\}$, $init(L) = \{a_1, a_2, ..., a_{n-1}\}$.
And Function $last(L)$ returns the last element, so that $last(L) = a_n$. Please
refer to the appendix of this book for their implementation.

Algorithm $removeT()$ can be translated to the following Haskell program, we name
it as `\verb|unsnoc|' to indicate it's the reverse function of `\verb|snoc|'.

\lstset{language=Haskell}
\begin{lstlisting}
unsnoc :: Tree a -> (Tree a, a)
unsnoc (Lf a) = (Empty, a)
unsnoc (Tr [a] Empty [b]) = (Lf a, b)
unsnoc (Tr f@(_:_) Empty [a]) = (Tr (init f) Empty [last f], a)
unsnoc (Tr f m [a]) = (Tr f m' (nodeToList r), a) where (m', r) = unsnoc m
unsnoc (Tr f m r) = (Tr f m (init r), last r)
\end{lstlisting}

And we can define a special function `\verb|last|' and '\verb|init|' for finger tree which is similar
to their counterpart for list.

\begin{lstlisting}
last = snd . unsnoc
init = fst . unsnoc
\end{lstlisting}

Imperatively removing the element from the end is almost as same as removing on the head.
Although there seems to be a special case, that as we always store the only element (or sub node) in the front
finger while the rear finger and middle part inner tree are empty (e.g. $Tree(\{n\}, NIL, \Phi)$),
it might get nothing if always try to fetch the last element from rear finger.

This can be solved by swapping the front and the rear finger if the rear is empty as in the
following algorithm.

\begin{algorithmic}
\Function{Extract-Tail}{$T$}
  \State $r \gets$ \textproc{Tree}()
  \State \Call{Connect-Mid}{$r, T$}
  \While{\Call{Rear}{$T$} $= \Phi \land $ \Call{Mid}{$T$} $\neq $ NIL}
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile

  \If{\Call{Rear}{$T$} $ = \Phi \land $ \Call{Front}{$T$} $\neq \Phi$}
    \State \textproc{Exchange} \Call{Front}{$T$} $\leftrightarrow$ \Call{Rear}{$T$}
  \EndIf

  \State $n \gets $ \textproc{Node}()
  \State \Call{Children}{$n$} $\gets$ \Call{Rear}{$T$}
  \Repeat
    \State $L \gets$ \Call{Children}{$n$} \Comment{$L = \{n_1, n_2, ..., n_{m-1}, n_m\}$}
    \State $n \gets$ \Call{Last}{$L$} \Comment{$ n \gets n_m$}
    \State \Call{Rear}{$T$} $\gets L[1...m-1]$ \Comment{$\{n_1, n_2, ..., n_{m-1}\}$}
    \State $T \gets $ \Call{Parent}{$T$}
    \If{\Call{Mid}{$T$} becomes empty}
      \State \Call{Mid}{$T$} $\gets$ NIL
    \EndIf
  \Until{$n$ is a leaf}
  \State \Return (\Call{Elem}{$n$}, \Call{Flat}{$r$})
\EndFunction
\end{algorithmic}

How to access the last element as well as implement this algorithm to working program are
left as exercises.

\subsection{concatenate}
\index{Finger tree!Concatenate}

Consider the none-trivial case that concatenate two finger trees $T_1 = tree(F_1, M_1, R_1)$ and
$T_2 = tree(F_2, M_2, R_2)$. One natural idea is to use $F_1$ as the new front finger for the
concatenated result, and keep $R_2$ being the new rear finger. The rest of work is to merge
$M_1$, $R_1$, $F_2$ and $M_2$ to a new middle part inner tree.

Note that both $R_1$ and $F_2$ are plain lists of node, so the sub-problem is to realize a
algorithm like this.

\[
merge(M_1, R_1 \cup F_2, M_2) = ?
\]

More observation reveals that both $M_1$ and $M_2$ are also finger trees, except that they
are one level deeper than $T_1$ and $T_2$ in terms of $Node(a)$, where $a$ is the type of
element stored in the tree. We can recursively use the strategy that keep the front finger
of $M_1$ and the rear finger of $M_2$, then merge the middle part inner tree of $M_1$, $M_2$,
as well as the rear finger of $M_1$ and front finger of $M_2$.

If we denote function $front(T)$ returns the front finger, $rear(T)$ returns the rear finger,
$mid(T)$ returns the middle part inner tree. the above $merge()$ algorithm can be
expressed for non-trivial case as the following.

\be
\begin{array}{l}
merge(M_1, R_1 \cup F_2, M_2) = tree(front(M_1), S, rear(M_2)) \\
S = merge(mid(M_1), rear(M_1) \cup R_1 \cup F_2 \cup front(M_2), mid(M_2))
\end{array}
\label{eq:merge-recursion}
\ee

If we look back to the original concatenate solution, it can be expressed as below.

\be
concat(T_1, T_2) = tree(F_1, merge(M_1, R_1 \cup R_2, M_2), R_2)
\ee

And compare it with equation \ref{eq:merge-recursion}, it's easy to note the fact that
concatenating is essentially merging. So we have the final algorithm like this.

\be
concat(T_1, T_2) = merge(T_1, \Phi, T_2)
\ee

By adding edge cases, the $merge()$ algorithm can be completed as below.

\be
merge(T_1, S, T_2) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  foldR(insertT, T_2, S) & T_1 = \Phi \\
  foldL(appendT, T_1, S) & T_2 = \Phi \\
  merge(\Phi, \{x\} \cup S, T_2) & T_1 = leaf(x) \\
  merge(T_1, S \cup \{x\}, \Phi) & T_2 = leaf(x) \\
  tree(F_1, merge(M_1, nodes(R_1 \cup S \cup F_2), M2), R_2) & otherwise
  \end{array}
\right .
\ee

Most of these cases are straightforward. If any one of $T_1$ or $T_2$ is empty, the algorithm
repeatedly insert/append all elements in $S$ to the other tree; Function $foldL$ and
$foldR$ are kinds of for-each process in imperative settings. The difference is that
$foldL$ processes the list $S$ from left to right while $foldR$ processes from right to left.

Here are their definition. Suppose list $L=\{ a_1, a_2, ..., a_{n-1}, a_n\}$,
$L' = \{ a_2, a_3, ..., a_{n-1}, a_n\}$ is the rest of elements except for the first one.

\be
foldL(f, e, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & L = \Phi \\
  foldL(f, f(e, a_1), L') & otherwise
  \end{array}
\right .
\ee

\be
foldR(f, e, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & L = \Phi \\
  f(a_1, foldR(f, e, L')) & otherwise
  \end{array}
\right .
\ee

They are detailed explained in the appendix of this book.

If either one of the tree is a leaf, we can insert or append the element of this leaf to
$S$, so that it becomes the trivial case of concatenating one empty tree with another.

Function $nodes()$ is used to wrap a list of elements to a list of 2-3 trees.
This is because the contents of middle part inner tree, compare to the
contents of finger, are one level deeper in terms of $Node()$. Consider the
time point that transforms from recursive case to edge case. Let's suppose
$M_1$ is empty at that time, we then need repeatedly insert all elements from
$R_1 \cup S \cup F_2$ to $M_2$. However, we can't directly do the insertion.
If the element type is $a$, we can only insert $Node(a)$ which is 2-3 tree
to $M_2$. This is just like what we did in the $insertT()$ algorithm,
take out the last 3 elements, wrap them in a 2-3 tree, and recursive
perform $insertT()$. Here is the definition of $nodes()$.

\be
nodes(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{tr2(x_1, x_2)\} & L = \{x_1, x_2\} \\
  \{tr3(x_1, x_2, x_3)\} & L = \{x_1, x_2, x_3\} \\
  \{tr2(x_1, x_2), tr2(x_3, x_4)\} & L = \{x_1, x_2, x_3, x_4\} \\
  \{tr3(x_1, x_2, x_3)\} \cup nodes(\{x_4, x_5, ...\}) & otherwise
  \end{array}
\right .
\ee

Function $nodes()$ follows the constraint of 2-3 tree, that if there are
only 2 or 3 elements in the list, it just wrap them in singleton list
contains a 2-3 tree; If there are 4 elements in the lists, it split them
into two trees each is consist of 2 branches; Otherwise, if there are
more elements than 4, it wraps the first three in to one tree with 3 branches,
and recursively call $nodes()$ to process the rest.

The performance of concatenation is determined by merging. Analyze the
recursive case of merging reveals that the depth of recursion is
proportion to the smaller height of the two trees. As the tree is
ensured to be balanced by using 2-3 tree. it's height is bound to
$O(\lg n')$ where $n'$ is the number of elements. The edge
case of merging performs as same as insertion, (It calls $insertT()$
at most 8 times) which is amortized $O(1)$ time, and $O(\lg m)$
at worst case, where $m$ is the difference in height of the two trees.
So the overall performance is bound to $O(\lg n)$, where $n$ is
the total number of elements contains in two finger trees.

The following Haskell program implements the concatenation algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
concat :: Tree a -> Tree a -> Tree a
concat t1 t2 = merge t1 [] t2
\end{lstlisting}

Note that there is `\verb|concat|' function defined in prelude standard library,
so we need distinct them either by hiding import or take a different name.

\begin{lstlisting}
merge :: Tree a -> [a] -> Tree a -> Tree a
merge Empty ts t2 = foldr cons t2 ts
merge t1 ts Empty = foldl snoc t1 ts
merge (Lf a) ts t2 = merge Empty (a:ts) t2
merge t1 ts (Lf a) = merge t1 (ts++[a]) Empty
merge (Tr f1 m1 r1) ts (Tr f2 m2 r2) = Tr f1 (merge m1 (nodes (r1 ++ ts ++ f2)) m2) r2
\end{lstlisting}

And the implementation of $nodes()$ is as below.

\begin{lstlisting}
nodes :: [a] -> [Node a]
nodes [a, b] = [Br2 a b]
nodes [a, b, c] = [Br3 a b c]
nodes [a, b, c, d] = [Br2 a b, Br2 c d]
nodes (a:b:c:xs) = Br3 a b c:nodes xs
\end{lstlisting}

To concatenate two finger trees $T_1$ and $T_2$ in imperative approach, we can traverse the two trees along
with the middle part inner tree till either tree turns to be empty. In every iteration,
we create a new tree $T$, choose the front finger of $T_1$ as the front finger of $T$; and choose the
rear finger of $T_2$ as the rear finger of $T$. The other two fingers (rear finger of $T_1$ and front finger
of $T_2$) are put together as a list, and this list is then balanced grouped to several 2-3 tree nodes as $n$.
Note that $n$ grows along with traversing not only in terms of length, the depth of its elements
increases by one in each iteration. We attach this new tree as the middle part inner tree of the
upper level result tree to end this iteration.

Once either tree becomes empty, we stop traversing, and repeatedly insert the 2-3 tree nodes in $n$ to
the other non-empty tree, and set it as the new middle part inner tree of the upper level result.

Below algorithm describes this process in detail.

\begin{algorithmic}
\Function{Concat}{$T_1, T_2$}
  \State \Return \Call{Merge}{$T_1, \Phi, T_2$}
\EndFunction
\Statex
\Function{Merge}{$T_1, N, T_2$}
  \State $r \gets$ \textproc{Tree}()
  \State $p \gets r$

  \While{$T_1 \neq$ NIL $\land T_2 \neq$ NIL}
    \State $T \gets$ \textproc{Tree}()
    \State \Call{Front}{$T$} $\gets$ \Call{Front}{$T_1$}
    \State \Call{Rear}{$T$} $\gets$ \Call{Rear}{$T_2$}
    \State \Call{Connect-Mid}{$p, T$}
    \State $p \gets T$
    \State $N \gets$ \textproc{Nodes}(\Call{Rear}{$T_1$} $\cup N \cup$ \Call{Front}{$T_2$})
    \State $T_1 \gets$ \Call{Mid}{$T_1$}
    \State $T_2 \gets$ \Call{Mid}{$T_2$}
  \EndWhile

  \If{$T_1 =$ NIL}
    \State $T \gets T_2$
    \For{each $n \in $ \Call{Reverse}{$N$}}
      \State $T \gets$ \Call{Prepend-Node}{$n, T$}
    \EndFor
  \ElsIf{$T_2 =$ NIL}
    \State $T \gets T_1$
    \For{each $n \in N$}
      \State $T \gets$ \Call{Append-Node}{$T, n$}
    \EndFor
  \EndIf
  \State \Call{Connect-Mid}{$p, T$}

  \State \Return \Call{Flat}{$r$}
\EndFunction
\end{algorithmic}

Note that the for-each loops in the algorithm can also be replaced by folding from left
and right respectively. Translating this algorithm to Python program yields the below code.

\lstset{language=Python}
\begin{lstlisting}
def concat(t1, t2):
    return merge(t1, [], t2)

def merge(t1, ns, t2):
    root = prev = Tree() #sentinel dummy tree
    while t1 is not None and t2 is not None:
        t = Tree(t1.size + t2.size + sizeNs(ns), t1.front, None, t2.rear)
        prev.set_mid(t)
        prev = t
        ns = nodes(t1.rear + ns + t2.front)
        t1 = t1.mid
        t2 = t2.mid
    if t1 is None:
        prev.set_mid(foldR(prepend_node, ns, t2))
    elif t2 is None:
        prev.set_mid(reduce(append_node, ns, t1))
    return flat(root)
\end{lstlisting}

Because Python only provides folding function from left as \verb|reduce()|, a folding function
from right is given like what we shown in pseudo code, that it repeatedly applies function
in reverse order of the list.

\begin{lstlisting}
def foldR(f, xs, z):
    for x in reversed(xs):
        z = f(x, z)
    return z
\end{lstlisting}

The only function in question is how to balanced-group nodes to bigger 2-3 trees. As a 2-3 tree
can hold at most 3 sub trees, we can firstly take 3 nodes and wrap them to a ternary tree if there
are more than 4 nodes in the list and continuously deal with the rest.
If there are just 4 nodes, they can be wrapped to two
binary trees. For other cases (there are 3 trees, 2 trees, 1 tree), we simply wrap them
all to a tree.

Denote node list $L=\{ n_1, n_2, ... \}$, The following algorithm realizes this process.

\begin{algorithmic}
\Function{Nodes}{$L$}
  \State $N = \Phi$
  \While{$|L| > 4$}
    \State $n \gets$ \textproc{Node}()
    \State \Call{Children}{$n$} $\gets L[1..3]$  \Comment{ $\{n_1, n_2, n_3 \}$ }
    \State $N \gets N \cup \{ n \}$
    \State $L \gets L[4...]$ \Comment{ $\{ n_4, n_5, ... \}$ }
  \EndWhile

  \If{$|L| = 4$}
    \State $x \gets$ \textproc{Node}()
    \State \Call{Children}{$x$} $\gets \{L[1], L[2]\}$
    \State $y \gets$ \textproc{Node}()
    \State \Call{Children}{$y$} $\gets \{L[3], L[4]\}$
    \State $N \gets N \cup \{ x, y \}$
  \ElsIf{$L \neq \Phi$}
    \State $n \gets$ \textproc{Node}()
    \State \Call{Children}{$n$} $\gets L$
    \State $N \gets N \cup \{ n \}$
  \EndIf

  \State \Return $N$
\EndFunction
\end{algorithmic}

It's straight forward to translate the algorithm to below Python program. Where function \verb|wraps()|
helps to create an empty node, then set a list as the children of this node.

\begin{lstlisting}
def nodes(xs):
    res = []
    while len(xs) > 4:
        res.append(wraps(xs[:3]))
        xs = xs[3:]
    if len(xs) == 4:
        res.append(wraps(xs[:2]))
        res.append(wraps(xs[2:]))
    elif xs != []:
        res.append(wraps(xs))
    return res
\end{lstlisting}

\begin{Exercise}
\begin{enumerate}
\item Implement the complete finger tree insertion program in your favorite imperative
programming language. Don't check the example programs along with this chapter before
having a try.

\item How to determine a node is a leaf? Does it contain only a raw element inside or a compound
node, which contains sub nodes as children? Note that we can't distinguish it by testing
the size, as there is case that node contains a singleton leaf, such as $node(1, \{node(1, \{x\}\})$.
Try to solve this problem in both dynamic typed language (e.g. Python, lisp etc) and
in strong static typed language (e.g. C++).

\item Implement the \textproc{Extract-Tail} algorithm in your favorite imperative programming
language.

\item Realize algorithm to return the last element of a finger tree in both functional and
imperative approach. The later one should be able to handle ill-formed tree.

\item Try to implement concatenation algorithm without using folding. You can either use
recursive methods, or use imperative for-each method.
\end{enumerate}
\end{Exercise}

\subsection{Random access of finger tree}
\index{Finger tree!Random access}

\subsubsection{size augmentation}
\index{Finger tree!Size augmentation}
The strategy to provide fast random access, is to turn the looking up into tree-search.
In order to avoid calculating the size of tree many times, we augment an extra field
to tree and node. The definition should be modified accordingly, for example the
following Haskell definition adds size field in its constructor.

\lstset{language=Haskell}
\begin{lstlisting}
data Tree a = Empty
            | Lf a
            | Tr Int [a] (Tree (Node a)) [a]
\end{lstlisting}

And the previous ANSI C structure is augmented with size as well.

\lstset{language=C}
\begin{lstlisting}
struct Tree {
  union Node* front;
  union Node* rear;
  Tree* mid;
  Tree* parent;
  int size;
};
\end{lstlisting}

Suppose the function $tree(s, F, M, R)$ creates a finger tree from size $s$, front
finger $F$, rear finger $R$, and middle part inner tree $M$.
When the size of the tree is needed, we can call a $size(T)$ function. It will be
something like this.

\[
size(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & T = \Phi \\
  ? & T = leaf(x) \\
  s & T = tree(s, F, M, R)
  \end{array}
\right .
\]

If the tree is empty, the size is definitely zero; and if it can be expressed as $tree(s, F, M, R)$,
the size is $s$; however, what if the tree is a singleton leaf? is it 1? No, it
can be 1 only if $T = leaf(a)$ and $a$ isn't a tree node, but a raw element stored in finger tree.
In most cases, the size is not 1, because $a$ can be again a tree node. That's why we
put a `?' in above equation.

The correct way is to call some size function on the tree node as the following.

\be
size(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & T = \Phi \\
  size'(x) & T = leaf(x) \\
  s & T = tree(s, F, M, R)
  \end{array}
\right .
\ee

Note that this isn't a recursive definition since $size \neq size'$, the argument to $size'$
is either a tree node, which is a 2-3 tree, or a plain element stored in the finger tree.
To uniform these two cases, we can anyway wrap the single plain element to a tree node
of only one element. So that we can express all the situation as a tree node augmented
with a size field. The following Haskell program modifies the definition of tree node.

\lstset{language=Haskell}
\begin{lstlisting}
data Node a = Br Int [a]
\end{lstlisting}

The ANSI C node definition is modified accordingly.

\lstset{language=C}
\begin{lstlisting}
struct Node {
  Key key;
  struct Node* children;
  int size;
};
\end{lstlisting}

We change it from union to structure. Although there is a overhead field `key' if the node isn't a
leaf.

Suppose function $tr(s, L)$, creates such a node (either one element being wrapped or a 2-3 tree)
from a size information $s$, and a list $L$. Here are some example.

\[
\begin{array}{ll}
tr(1, \{x\}) & \text{a tree contains only one element} \\
tr(2, \{x, y\}) & \text{a 2-3 tree contains two elements} \\
tr(3, \{x, y, z\}) & \text{a 2-3 tree contains three elements}
\end{array}
\]

So the function $size'$ can be implemented as returning the size information of a tree node.
We have $size'(tr(s, L)) = s$.

Wrapping an element $x$ is just calling $tr(1, \{x\})$. We can define auxiliary functions
$wrap$ and $unwrap$, for instance.

\be
\begin{array}{l}
wrap(x) = tr(1, \{x\}) \\
unwrap(n) = x \quad:\quad n = tr(1, \{x\})
\end{array}
\ee

As both front finger and rear finger are lists of tree nodes, in order to calculate the
total size of finger, we can provide a $size''(L)$ function, which sums up size of all
nodes stored in the list. Denote $L = \{ a_1, a_2, ... \}$ and $L' = \{ a_2, a_3, ... \}$.

\be
size''(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & L = \Phi \\
  size'(a_1) + size''(L') & otherwise
  \end{array}
\right .
\ee

It's quite OK to define $size''(L)$ by using some high order functions. For example.

\be
size''(L) = sum(map(size', L))
\ee

And we can turn a list of tree nodes into one deeper 2-3 tree and vice-versa.

\be
\begin{array}{l}
wraps(L) = tr(size''(L), L) \\
unwraps(n) = L \quad:\quad n = tr(s, L) \\
\end{array}
\ee

These helper functions are translated to the following Haskell code.

\lstset{language=Haskell}
\begin{lstlisting}
size (Br s _) = s

sizeL = sum .(map size)

sizeT Empty = 0
sizeT (Lf a) = size a
sizeT (Tr s _ _ _) = s
\end{lstlisting}

Here are the wrap and unwrap auxiliary functions.

\begin{lstlisting}
wrap x = Br 1 [x]
unwrap (Br 1 [x]) = x
wraps xs = Br (sizeL xs) xs
unwraps (Br _ xs) = xs
\end{lstlisting}

We omitted their type definitions for illustration purpose.

In imperative settings, the size information for node and tree can be accessed
through the size field. And the size of a list of nodes can be summed up
for this field as the below algorithm.

\begin{algorithmic}
\Function{Size-Nodes}{$L$}
  \State $s \gets 0$
  \For{$\forall n \in L$}
    \State $s \gets s + $ \Call{Size}{$n$}
  \EndFor
  \State \Return $s$
\EndFunction
\end{algorithmic}

The following Python code, for example, translates this algorithm by using
standard \verb|sum()| and \verb|map()| functions provided in library.

\lstset{language=Python}
\begin{lstlisting}
def sizeNs(xs):
    return sum(map(lambda x: x.size, xs))
\end{lstlisting}

As NIL is typically used to represent empty tree in imperative settings,
it's convenient to provide a auxiliary size function to uniformed calculate
the size of tree no matter it is NIL.

\begin{algorithmic}
\Function{Size-Tr}{$T$}
  \If{$T = $ NIL}
    \State \Return 0
  \Else
    \State \Return \Call{Size}{$T$}
  \EndIf
\EndFunction
\end{algorithmic}

The algorithm is trivial and we skip its implementation example program.

\subsubsection{Modification due to the augmented size}

The algorithms have been presented so far need to be modified to accomplish with the
augmented size. For example the $insertT()$ function now inserts a tree node instead
of a plain element.

\be
insertT(x, T) = insertT'(wrap(x), T)
\ee

The corresponding Haskell program is changed as below.

\lstset{language=Haskell}
\begin{lstlisting}
cons a t = cons' (wrap a) t
\end{lstlisting}

After being wrapped, $x$ is augmented with size information of 1. In the implementation
of previous insertion algorithm, function $tree(F, M, R)$ is used to create a finger tree
from a front finger, a middle part inner tree and a rear finger. This function should
also be modified to add size information of these three arguments.

\be
tree'(F, M, R) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  fromL(F) & M = \Phi \land R = \Phi \\
  fromL(R) & M = \Phi \land F = \Phi \\
  tree'(unwraps(F'), M', R) & F = \Phi, (F', M') = extractT'(M) \\
  tree'(F, M', unwraps(R')) & R = \Phi, (M', R') = removeT'(M) \\
  tree(size''(F) + size(M) + size''(R), F, M, R) & otherwise
  \end{array}
\right .
\ee

Where $fromL()$ helps to turn a list of nodes to a finger tree by repeatedly
inserting all the element one by one to an empty tree.

\[
fromL(L) = foldR(insertT', \Phi, L)
\]

Of course it can be implemented in pure recursive manner without using folding as well.

The last case is the most straightforward one. If none of $F$, $M$, and $R$ is empty,
it adds the size of these three part and construct the tree along with this size information
by calling $tree(s, F, M, R)$ function.
If both the middle part inner tree and one of the finger is empty, the algorithm
repeatedly insert all elements stored in the other finger to an empty tree, so that
the result is constructed from a list of tree nodes.
If the middle part inner tree isn't empty, and one of the finger is empty, the
algorithm `borrows' one tree node from the middle part, either by extracting from
head if front finger is empty or removing from tail if rear finger is empty.
Then the algorithm unwraps the `borrowed' tree node to a list, and recursively
call $tree'()$ function to construct the result.

This algorithm can be translated to the following Haskell code for example.

\begin{lstlisting}
tree f Empty [] = foldr cons' Empty f
tree [] Empty r = foldr cons' Empty r
tree [] m r = let (f, m') = uncons' m in tree (unwraps f) m' r
tree f m [] = let (m', r) = unsnoc' m in tree f m' (unwraps r)
tree f m r = Tr (sizeL f + sizeT m + sizeL r) f m r
\end{lstlisting}

Function $tree'()$ helps to minimize the modification. $insertT'()$ can be
realized by using it like the following.

\be
insertT'(x, T) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree'(\{x\}, \Phi, \{y\}) & T = leaf(x) \\
  tree'(\{x, x_1\}, insertT'(wraps(\{x_2, x_3, x_4\}), M), R) & T = tree(s, \{x_1, x_2, x_3, x_4\}, M, R) \\
  tree'(\{x\} \cup F, M, R) & otherwise
  \end{array}
\right .
\ee

And it's corresponding Haskell code is a line by line translation.

\begin{lstlisting}
cons' a Empty = Lf a
cons' a (Lf b) = tree [a] Empty [b]
cons' a (Tr _ [b, c, d, e] m r) = tree [a, b] (cons' (wraps [c, d, e]) m) r
cons' a (Tr _ f m r) = tree (a:f) m r
\end{lstlisting}

The similar modification for augment size should also be tuned for imperative
algorithms, for example, when a new node is prepend to the head of the finger
tree, we should update size when traverse the tree.

\begin{algorithmic}
\Function{Prepend-Node}{$n, T$}
  \State $r \gets $ \textproc{Tree}()
  \State $p \gets r$
  \State \Call{Connect-Mid}{$p, T$}
  \While{\textproc{Full?}(\Call{Front}{$T$})}
    \State $F \gets $ \Call{Front}{$T$}
    \State \Call{Front}{$T$} $\gets$ $\{n, F[1]\}$
    \State \Call{Size}{$T$} $\gets$ \Call{Size}{$T$} + \Call{Size}{$n$} \Comment{update size}
    \State $n \gets$ \textproc{Node}()
    \State \Call{Children}{$n$} $\gets F[2..]$
    \State $p \gets T$
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile
  \If{$T =$ NIL}
    \State $T \gets$ \textproc{Tree}()
    \State \Call{Front}{$T$}$\gets \{ n \}$
  \ElsIf{ $|$ \Call{Front}{$T$} $|$ = 1 $\land$ \Call{Rear}{$T$} = $\Phi$}
    \State \Call{Rear}{$T$} $\gets$ \Call{Front}{$T$}
    \State \Call{Front}{$T$} $\gets \{ n \}$
  \Else
    \State \Call{Front}{$T$} $\gets \{ n \} \cup $ \Call{Front}{$T$}
  \EndIf
  \State \Call{Size}{$T$} $\gets$ \Call{Size}{$T$} + \Call{Size}{$n$} \Comment{update size}
  \State \Call{Connect-Mid}{$p, T$} $\gets T$
  \State \Return \Call{Flat}{$r$}
\EndFunction
\end{algorithmic}

The corresponding Python code are modified accordingly as below.

\lstset{language=Python}
\begin{lstlisting}
def prepend_node(n, t):
    root = prev = Tree()
    prev.set_mid(t)
    while frontFull(t):
        f = t.front
        t.front = [n] + f[:1]
        t.size = t.size + n.size
        n = wraps(f[1:])
        prev = t
        t = t.mid
    if t is None:
        t = leaf(n)
    elif len(t.front)==1 and t.rear == []:
        t = Tree(n.size + t.size, [n], None, t.front)
    else:
        t = Tree(n.size + t.size, [n]+t.front, t.mid, t.rear)
    prev.set_mid(t)
    return flat(root)
\end{lstlisting}

Note that the tree constructor is also modified to take a size argument
as the first parameter. And the \verb|leaf()| helper function does not
only construct the tree from a node, but also set the size of the tree
with the same size of the node inside it.

For simplification purpose, we skip the detailed description of what are modified in
$extractT()'$, $appendT()$, $removeT()$, and $concat()$ algorithms. They are left as exercises to the
reader.

\subsubsection{Split a finger tree at a given position}
\index{Finger tree!splitting}

With size information augmented, it's easy to locate a node at given position by performing
a tree search. What's more, as the finger tree is constructed from three part $F$, $M$, and
$R$; and it's nature of recursive, it's also possible to split it into three sub parts with
a given position $i$: the left, the node at $i$, and the right part.

The idea is straight forward. Since we have the size information for $F$, $M$, and $R$. Denote
these three sizes as $S_f$, $S_m$, and $S_r$. if the given position $i \leq S_f$, the node must
be stored in $F$, we can go on seeking the node inside $F$; if $S_f < i \leq S_f + S_m $, the
node must be stored in $M$, we need recursively perform search in $M$; otherwise, the node
should be in $R$, we need search inside $R$.

If we skip the error handling of trying to split an empty tree, there is only one edge case
as below.

\[
splitAt(i, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x, \Phi) & T = leaf(x) \\
  ... & otherwise
  \end{array}
\right .
\]

Splitting a leaf results both the left and right parts empty, the node stored in leaf
is the resulting node.

The recursive case handles the three sub cases by comparing $i$ with the sizes.
Suppose function $splitAtL(i, L)$ splits a list of nodes at given position $i$
into three parts: $(A, x, B) = splitAtL(i, L)$, where $x$ is the $i$-th node
in $L$, $A$ is a sub list contains all nodes before position $i$, and $B$ is
a sub list contains all rest nodes after $i$.

\be
splitAt(i, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x, \Phi) & T = leaf(x) \\
  (fromL(A), x, tree'(B, M, R) & i \leq S_f, (A, x, B) = splitAtL(i, F) \\
  (tree'(F, M_l, A), x, tree'(B, M_r, R) & S_f < i \leq S_f + S_m \\
  (tree'(F, M, A), x, fromL(B)) & otherwise, (A, x, B) = splitAtL(i-S_f-S_m, R)
  \end{array}
\right .
\ee

Where $M_l, x, M_r, A, B$ in the thrid case are calculated as the following.

\[
\begin{array}{l}
(M_l, t, M_r) = splitAt(i-S_f, M) \\
(A, x, B) = splitAtL(i-S_f-size(M_l), unwraps(t))
\end{array}
\]

And the function $splitAtL()$ is just a linear traverse, since the length of list
is limited not to exceed the constraint of 2-3 tree, the performance is still
ensured to be constant $O(1)$ time. Denote $L = \{x_1, x_2, ... \}$ and
$L' = \{ x_2, x_3, ...\}$.

\be
splitAtL(i, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x_1, \Phi) & i = 0 \land L = \{x_1\} \\
  (\Phi, x_1, L') & i < size'(x_1) \\
  (\{x_1\} \cup A, x, B) & otherwise, (A, x, B) = splitAtL(i-size'(x_1), L')
  \end{array}
\right .
\ee

The solution of splitting is a typical divide and conquer strategy. The performance
of this algorithm is determined by the recursive case of searching in middle part
inner tree. Other cases are all constant time as we've analyzed. The depth of
recursion is proportion to the height of the tree $h$, so the algorithm is bound
to $O(h)$. Because the tree is well balanced (by using 2-3 tree, and all the
insertion/removal algorithms keep the tree balanced), so $h = O(\lg n)$ where
$n$ is the number of elements stored in finger tree. The overall performance
of splitting is $O(\lg n)$.

Let's first give the Haskell program for $splitAtL()$ function

\lstset{language=Haskell}
\begin{lstlisting}
splitNodesAt 0 [x] = ([], x, [])
splitNodesAt i (x:xs) | i < size x = ([], x, xs)
                      | otherwise = let (xs', y, ys) = splitNodesAt (i-size x) xs
                                    in (x:xs', y, ys)
\end{lstlisting}

Then the program for $splitAt()$, as there is already function defined in standard
library with this name, we slightly change the name by adding a apostrophe.

\begin{lstlisting}
splitAt' _ (Lf x) = (Empty, x, Empty)
splitAt' i (Tr _ f m r)
    | i < szf = let (xs, y, ys) = splitNodesAt i f
                in ((foldr cons' Empty xs), y, tree ys m r)
    | i < szf + szm = let (m1, t, m2) = splitAt' (i-szf) m
                          (xs, y, ys) = splitNodesAt (i-szf - sizeT m1) (unwraps t)
                      in (tree f m1 xs, y, tree ys m2 r)
    | otherwise = let (xs, y, ys) = splitNodesAt (i-szf -szm) r
                  in (tree f m xs, y, foldr cons' Empty ys)
    where
      szf = sizeL f
      szm = sizeT m
\end{lstlisting}

\subsubsection{Random access}
\index{Finger tree!Random access}

With the help of splitting at any arbitrary position, it's trivial to realize random
access in $O(\lg n)$ time. Denote function $mid(x)$ returns the 2-nd element of a tuple,
$left(x)$, and $right(x)$ return the first element and the 3-rd element of the tuple
respectively.

\be
getAt(S, i) = unwrap(mid(splitAt(i, S)))
\ee

It first splits the sequence at position $i$, then unwraps the node to get the element
stored inside it. When mutate the $i$-th element of sequence $S$ represented by finger tree,
we first split it at $i$, then we replace the middle to what
we want to mutate, and re-construct them to one finger tree by using concatenation.

\be
setAt(S, i, x) = concat(L, insertT(x, R))
\ee

where
\[
(L, y, R) = splitAt(i, S)
\]

What's more, we can also realize a $removeAt(S, i)$ function, which can remove the
$i$-th element from sequence $S$. The idea is first to split at $i$, unwrap and
return the element of the $i$-th node; then concatenate the left and right to a
new finger tree.

\be
removeAt(S, i) = (unwrap(y), concat(L, R))
\ee

These handy algorithms can be translated to the following Haskell program.

\lstset{language=Haskell}
\begin{lstlisting}
getAt t i = unwrap x where (_, x, _) = splitAt' i t
setAt t i x = let (l, _, r) = splitAt' i t in concat' l (cons x r)
removeAt t i = let (l, x, r) = splitAt' i t in (unwrap x, concat' l r)
\end{lstlisting}

\subsubsection{Imperative random access}
\index{Finger tree!Imperative random access}
As we can directly mutate the tree in imperative settings, it's possible to
realize \textproc{Get-At}($T, i$) and \textproc{Set-At}($T, i, x$) without
using splitting. The idea is firstly implement a algorithm which can
apply some operation to a given position. The following algorithm takes
three arguments, a finger tree $T$, a position index at $i$ which ranges from
zero to the number of elements stored in the tree, and a function $f$,
which will be applied to the element at $i$.

\begin{algorithmic}
\Function{Apply-At}{$T, i, f$}
  \While{\Call{Size}{$T$} $> 1$}
    \State $S_f \gets $ \textproc{Size-Nodes}(\Call{Front}{$T$})
    \State $S_m \gets $ \textproc{Size-Tr}(\Call{Mid}{$T$})
    \If{$i < S_f$}
      \State \Return \textproc{Lookup-Nodes}(\Call{Front}{$T$}, $i$, $f$)
    \ElsIf{$i < S_f + S_m$}
      \State $T \gets$ \Call{Mid}{$T$}
      \State $i \gets i - S_f$
    \Else
      \State \Return \textproc{Lookup-Nodes}(\Call{Rear}{$T$}, $i - S_f - S_m$, $f$)
    \EndIf
  \EndWhile
  \State $n \gets$ \Call{First-Lf}{$T$}
  \State $x \gets$ \Call{Elem}{$n$}
  \State \Call{Elem}{$n$} $\gets f(x)$
  \State \Return $x$
\EndFunction
\end{algorithmic}

This algorithm is essentially a divide and conquer tree search. It repeatedly
examine the current tree till reach a tree with size of 1 (can it be determined
as a leaf? please consider the ill-formed case and refer to the exercise later).
Every time, it checks the position to be located with the size information of
front finger and middle part inner tree.

If the index $i$ is less than the size of front finger, the location is at some
node in it. The algorithm call a sub procedure to look-up among front finger;
If the index is between the size of front finger and the total size till middle
part inner tree, it means that the location is at some node inside the middle,
the algorithm goes on traverse along the middle part inner tree with an updated
index reduced by the size of front finger; Otherwise
it means the location is at some node in rear finger, the similar looking up
procedure is called accordingly.

After this loop, we've got a node, (can be a compound node)
with what we are looking for at the first leaf inside this node. We can extract
the element out, and apply the function $f$ on it and store the new value back.

The algorithm returns the previous element before applying $f$ as the final result.

What hasn't been factored is the algorithm \textproc{Lookup-Nodes}($L$, $i$, $f$).
It takes a list of nodes, a position index, and a function to be applied. This
algorithm can be implemented by checking every node in the list. If the node
is a leaf, and the index is zero, we are at the right position to be looked up.
The function can be applied on the element stored in this leaf, and the previous
value is returned; Otherwise, we need compare the size of this node and
the index to determine if the position is inside this node and search inside the
children of the node if necessary.

\begin{algorithmic}
\Function{Lookup-Nodes}{$L, i, f$}
  \Loop
    \For{$\forall n \in L$}
      \If{$n$ is leaf $\land i = 0$}
        \State $x \gets $ \Call{Elem}{$n$}
        \State \Call{Elem}{$n$} $\gets f(x)$
        \State \Return $x$
      \EndIf
      \If{$i < $ \Call{Size}{$n$}}
        \State $L \gets $ \Call{Children}{$n$}
        \State break
      \EndIf
      \State $i \gets i - $ \Call{Size}{$n$}
    \EndFor
  \EndLoop
\EndFunction
\end{algorithmic}

The following are the corresponding Python code implements the algorithms.

\lstset{language=Python}
\begin{lstlisting}
def applyAt(t, i, f):
    while t.size > 1:
        szf = sizeNs(t.front)
        szm = sizeT(t.mid)
        if i < szf:
            return lookupNs(t.front, i, f)
        elif i < szf + szm:
            t = t.mid
            i = i - szf
        else:
            return lookupNs(t.rear, i - szf - szm, f)
    n = first_leaf(t)
    x = elem(n)
    n.children[0] = f(x)
    return x

def lookupNs(ns, i, f):
    while True:
        for n in ns:
            if n.leaf and i == 0:
                x = elem(n)
                n.children[0] = f(x)
                return x
            if i < n.size:
                ns = n.children
                break
            i = i - n.size
\end{lstlisting}

With auxiliary algorithm that can apply function at a given position, it's trivial to implement
the \textproc{Get-At}() and \textproc{Set-At}() by passing special function for applying.

\begin{algorithmic}
\Function{Get-At}{$T, i$}
  \State \Return \Call{Apply-At}{$T, i, \lambda_x . x$}
\EndFunction
\Statex
\Function{Set-At}{$T, i, x$}
  \State \Return \Call{Apply-At}{$T, i, \lambda_y . x$}
\EndFunction
\end{algorithmic}

That is we pass \verb|id| function to implement getting element at a position, which doesn't
change anything at all; and pass constant function to implement setting, which set the element
to new value by ignoring its previous value.

\subsubsection{Imperative splitting}
\index{Finger Tree!Imperative splitting}

It's not enough to just realizing \textproc{Apply-At} algorithm in imperative settings, this
is because removing element at arbitrary position is also a typical case.

Almost all the imperative finger tree algorithms so far are kind of one-pass top-down manner.
Although we sometimes need to book keeping the root. It means that we can even realize all
of them without using the parent field.

Splitting operation, however, can be easily implemented by using parent field. We can first
perform a top-down traverse along with the middle part inner tree as long as the splitting position
doesn't located in front or rear finger. After that, we need a bottom-up traverse along
with the parent field of the two split trees to fill out the necessary fields.

\begin{algorithmic}
\Function{Split-At}{$T, i$}
  \State $T_1 \gets$ \textproc{Tree}()
  \State $T_2 \gets$ \textproc{Tree}()
  \While{$S_f \leq i < S_f + S_m$} \Comment{Top-down pass}
    \State $T'_1 \gets$ \textproc{Tree}()
    \State $T'_2 \gets$ \textproc{Tree}()
    \State \Call{Front}{$T'_1$} $\gets$ \Call{Front}{$T$}
    \State \Call{Rear}{$T'_2$} $\gets$ \Call{Rear}{$T$}
    \State \Call{Connect-Mid}{$T_1, T'_1$}
    \State \Call{Connect-Mid}{$T_2, T'_2$}
    \State $T_1 \gets T'_1$
    \State $T_2 \gets T'_2$
    \State $i \gets i - S_f$
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile

  \If{$i < S_f$}
    \State $(X, n, Y) \gets$ \textproc{Split-Nodes}(\Call{Front}{$T$}, $i$)
    \State $T'_1 \gets$ \Call{From-Nodes}{$X$}
    \State $T'_2 \gets T$
    \State \Call{Size}{$T'_2$} $\gets$ \Call{Size}{$T$} - \Call{Size-Nodes}{$X$} - \Call{Size}{$n$}
    \State \Call{Front}{$T'_2$} $\gets Y$
  \ElsIf{$S_f + S_m \leq i$}
    \State $(X, n, Y) \gets$ \textproc{Split-Nodes}(\Call{Rear}{$T$}, $i - S_f - S_m$)
    \State $T'_2 \gets$ \Call{From-Nodes}{$Y$}
    \State $T'_1 \gets T$
    \State \Call{Size}{$T'_1$} $\gets$ \Call{Size}{$T$} - \Call{Size-Nodes}{$Y$} - \Call{Size}{$n$}
    \State \Call{Rear}{$T'_1$} $\gets X$
  \EndIf
  \State \Call{Connect-Mid}{$T_1, T'_1$}
  \State \Call{Connect-Mid}{$T_2, T'_2$}

  \State $i \gets i -$ \Call{Size-Tr}{$T'_1$}
  \While{$n$ is NOT leaf} \Comment{Bottom-up pass}
    \State $(X, n, Y) \gets$ \textproc{Split-Nodes}(\Call{Children}{$n$}, $i$)
    \State $i \gets i -$ \Call{Size-Nodes}{$X$}
    \State \Call{Rear}{$T_1$} $\gets X$
    \State \Call{Front}{$T_2$} $\gets Y$
    \State \Call{Size}{$T_1$} $\gets$ \Call{Sum-Sizes}{$T_1$}
    \State \Call{Size}{$T_2$} $\gets$ \Call{Sum-Sizes}{$T_2$}
    \State $T_1 \gets$ \Call{Parent}{$T_1$}
    \State $T_2 \gets$ \Call{Parent}{$T_2$}
  \EndWhile

  \State \Return (\Call{Flat}{$T_1$}, \Call{Elem}{$n$}, \Call{Flat}{$T_2$})
\EndFunction
\end{algorithmic}

The algorithm first creates two trees $T_1$ and $T_2$ to hold the split results. Note that
they are created as 'ground' trees which are parents of the roots. The first pass is
a top-down pass. Suppose $S_f$, and $S_m$ retrieve the size of the front finger and the size of middle part
inner tree respectively. If the position at which the tree to be split is located at middle part inner tree,
we reuse the front finger of $T$ for new created $T'_1$, and reuse rear finger of $T$ for $T'_2$.
At this time point, we can't fill the other fields for $T'_1$ and $T'_2$, they are left empty, and
we'll finish filling them in the future. After that, we connect $T_1$ and $T'_1$ so the latter becomes
the middle part inner tree of the former. The similar connection is done for $T_2$ and $T'_2$ as well.
Finally, we update the position by deducing it by the size of front finger, and go on traversing along
with the middle part inner tree.

When the first pass finishes, we are at a position that either the splitting should be performed
in front finger, or in rear finger. Splitting the nodes in finger results a tuple, that
the first part and the third part are lists before and after the splitting point, while the second
part is a node contains the element at the original position to be split.
As both fingers hold at most 3 nodes because they are actually 2-3 trees, the nodes splitting
algorithm can be performed by a linear search.

\begin{algorithmic}
\Function{Split-Nodes}{$L, i$}
  \For{$j \in [1, $ \Call{Length}{$L$} $]$}
    \If{$i <$ \Call{Size}{$L[j]$}}
      \State \Return ($L[1...j-1]$, $L[j]$, $L[j+1...$ \Call{Length}{$L$} $]$)
    \EndIf
    \State $i \gets i -$ \Call{Size}{$L[j]$}
  \EndFor
\EndFunction
\end{algorithmic}

We next create two new result trees $T'_1$ and $T'_2$ from this tuple, and connected them as the
final middle part inner tree of $T_1$ and $T_2$.

Next we need perform a bottom-up traverse along with the result trees to fill out all the empty
information we skipped in the first pass.

We loop on the second part of the tuple, the node, till it becomes a leaf. In each iteration, we
repeatedly splitting the children of the node with an updated position $i$. The first list of
nodes returned from splitting is used to fill the rear finger of $T_1$; and the other list of
nodes is used to fill the front finger of $T_2$. After that, since all the three parts of a
finger tree -- the front and rear finger, and the middle part inner tree -- are filled, we
can then calculate the size of the tree by summing these three parts up.

\begin{algorithmic}
\Function{Sum-Sizes}{$T$}
  \State \Return \textproc{Size-Nodes}(\Call{Front}{$T$}) + \textproc{Size-Tr}(\Call{Mid}{$T$}) + \textproc{Size-Nodes}(\Call{Rear}{$T$})
\EndFunction
\end{algorithmic}

Next, the iteration goes on along with the parent fields of $T_1$ and $T_2$. The last 'black-box'
algorithm is \textproc{From-Nodes}($L$), which can create a finger tree from a list of nodes.
It can be easily realized by repeatedly perform insertion on an empty tree. The implementation
is left as an exercise to the reader.

The example Python code for splitting is given as below.

\lstset{language=Python}
\begin{lstlisting}
def splitAt(t, i):
    (t1, t2) = (Tree(), Tree())
    while szf(t) <= i and i < szf(t) + szm(t):
        fst = Tree(0, t.front, None, [])
        snd = Tree(0, [], None, t.rear)
        t1.set_mid(fst)
        t2.set_mid(snd)
        (t1, t2) = (fst, snd)
        i = i - szf(t)
        t = t.mid

    if i < szf(t):
        (xs, n, ys) = splitNs(t.front, i)
        sz = t.size - sizeNs(xs) - n.size
        (fst, snd) = (fromNodes(xs), Tree(sz, ys, t.mid, t.rear))
    elif szf(t) + szm(t) <= i:
        (xs, n, ys) = splitNs(t.rear, i - szf(t) - szm(t))
        sz = t.size - sizeNs(ys) - n.size
        (fst, snd) = (Tree(sz, t.front, t.mid, xs), fromNodes(ys))
    t1.set_mid(fst)
    t2.set_mid(snd)

    i = i - sizeT(fst)
    while not n.leaf:
        (xs, n, ys) = splitNs(n.children, i)
        i = i - sizeNs(xs)
        (t1.rear, t2.front) = (xs, ys)
        t1.size = sizeNs(t1.front) + sizeT(t1.mid) + sizeNs(t1.rear)
        t2.size = sizeNs(t2.front) + sizeT(t2.mid) + sizeNs(t2.rear)
        (t1, t2) = (t1.parent, t2.parent)

    return (flat(t1), elem(n), flat(t2))
\end{lstlisting}

The program to split a list of nodes at a given position is listed like this.

\begin{lstlisting}
def splitNs(ns, i):
    for j in range(len(ns)):
        if i < ns[j].size:
            return (ns[:j], ns[j], ns[j+1:])
        i = i - ns[j].size
\end{lstlisting}

With splitting defined, removing an element at arbitrary position can be realized trivially
by first performing a splitting, then concatenating the two result tree to one big tree and return
the element at that position.

\begin{algorithmic}
\Function{Remove-At}{$T, i$}
  \State $(T_1, x, T_2) \gets$ \Call{Split-At}{$T, i$}
  \State \Return $(x, $ \Call{Concat}{$T_1, T_2$} $)$
\EndFunction
\end{algorithmic}

\begin{Exercise}
\begin{enumerate}
\item Another way to realize $insertT'()$ is to force increasing the size field by one, so
that we needn't write function $tree'()$. Try to realize the algorithm by using this idea.

\item Try to handle the augment size information as well as in $insertT'()$ algorithm for
the following algorithms (both functional and imperative): $extractT()'$, $appendT()$, $removeT()$, and $concat()$. The $head$, $tail$,
$init$ and $last$ functions should be kept unchanged. Don't refer to the download-able
programs along with this book before you take a try.

\item In the imperative \textproc{Apply-At}() algorithm, it tests if the size of the
current tree is greater than one. Why don't we test if the current tree is
a leaf? Tell the difference between these two approaches.

\item Implement the \textproc{From-Nodes}($L$) in your favorite imperative programming language.
You can either use looping or create a folding-from-right sub algorithm.
\end{enumerate}
\end{Exercise}

% ================================================================
%                 Short summary
% ================================================================
\section{Notes and short summary}

Although we haven't been able to give a purely functional realization to match the
$O(1)$ constant time random access as arrays in imperative settings. The result
finger tree data structure achieves an overall well performed sequence.
It manipulates fast in amortized $O(1)$ time both on head an on tail, it can also
concatenates two sequence in logarithmic time as well as break one sequence into
two sub sequences at any position. While neither arrays in imperative settings
nor linked-list in functional settings satisfies all these goals.
Some functional programming languages adopt this sequence realization in its
standard library \cite{hackage-ftr}.

Just as the title of this chapter, we've presented the last corner stone of
elementary data structures in both functional and imperative settings.
We needn't concern about being lack of elementary data structures when
solve problems with some typical algorithms.

\index{MTF}
For example, when writing a MTF (move-to-front) encoding algorithm\cite{mtf-wiki}, with
the help of the sequence data structure explained in this chapter. We can
implement it quite straightforward.

\[
mtf(S, i) = \{x\} \cup S' \\
\]

where $(x, S') = removeAt(S, i)$.

In the next following chapters, we'll first explains some typical divide and conquer
sorting methods, including quick sort, merge sort and their variants; then
some elementary searching algorithms, and string matching algorithms will be
covered.
% This is planned in the 2nd edition
%finally,
%we'll give a real-world example of algorithms, BWT (Burrows-Wheeler transform) compressor,
%which is one of the best compression tool in the world.

% ================================================================
%                 Appendix
% ================================================================

\begin{thebibliography}{99}

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{okasaki-ralist}
Chris Okasaki. ``Purely Functional Random-Access Lists''. Functional Programming Languages and Computer Architecture, June 1995, pages 86-95.

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. ``Introduction to Algorithms, Second Edition''. The MIT Press, 2001. ISBN: 0262032937.

\bibitem{learn-haskell}
Miran Lipovaca. ``Learn You a Haskell for Great Good! A Beginner's Guide''. No Starch Press; 1 edition April 2011, 400 pp. ISBN: 978-1-59327-283-8

\bibitem{finger-tree-2006}
Ralf Hinze and Ross Paterson. ``Finger Trees: A Simple General-purpose Data Structure." in Journal of Functional Programming16:2 (2006), pages 197-217. http://www.soi.city.ac.uk/~ross/papers/FingerTree.html

\bibitem{finger-tree-1977}
Guibas, L. J., McCreight, E. M., Plass, M. F., Roberts, J. R. (1977), "A new representation for linear lists". Conference Record of the Ninth Annual ACM Symposium on Theory of Computing, pp. 49-60.

\bibitem{hackage-ftr}
Generic finger-tree structure. http://hackage.haskell.org/packages/archive/fingertree/0.0/doc/html/Data-FingerTree.html

\bibitem{mtf-wiki}
Wikipedia. Move-to-front transform. http://en.wikipedia.org/wiki/Move-to-front\_transform

\end{thebibliography}

\ifx\wholebook\relax \else
\end{document}
\fi

% LocalWords:  typedef struct typename
