\ifx\wholebook\relax \else
% ------------------------

\documentclass[UTF8]{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
\input{../../common-zh-cn.tex}

\setcounter{page}{1}

\begin{document}

%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{从吃葡萄到世界杯，选择排序的进化}

\author{刘新宇
\thanks{{\bfseries 刘新宇 } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\maketitle
\fi

\markboth{选择排序}{初等算法}

\ifx\wholebook\relax
\chapter{从吃葡萄到世界杯，选择排序的进化}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{简介}
\label{introduction} \index{选择排序}

我们此前介绍了排序中的“hello world”―插入排序算法。本章中，我们介绍另外一种直观的排序方法―选择排序。最基本的选择排序在性能上不如分而治之排序算法，如快速排序和归并排序。同插入排序那一章一样，我们将会分析为什么选择排序的速度慢，并且从不同的角度改进它，最终进化到堆排序，从而达到基于比较的排序的性能上限$O(n \lg n)$。

选择排序的思想在日常生活中很常见。考虑一个小孩要吃掉一串葡萄。我们通常会发现两种类型的小孩，一种属于“乐观型”，每次吃掉最大的一颗；另一种属于“悲观型”，每次总吃掉最小的一颗。

第一种小孩实际上按照由大到小的顺序吃葡萄；第二种按照由小到大的顺序吃葡萄。实际上，孩子把葡萄按照大小进行了\underline{排序}，并且使用了选择排序的方法。
\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.6]{img/grapes-1.eps}
  \caption{总挑出最小的葡萄。}
  \label{fig:eat-grapes}
\end{figure}

选择排序的算法可以描述如下。

为了将某一序列元素排序：

\begin{itemize}
\item 简单情况：如果序列为空，排序结果也为空；
\item 否则，我们找到最小的元素，将其附加到结果的后面。
\end{itemize}

注意上面描述的算法将元素按照升序排序；如果每次选择最大的元素，则排序结果是降序的。我们后面会介绍将比较方法作为参数传入的实现。

这一描述可以形式化为下面的公式。

\be
sort(A) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & A = \phi \\
  \{ m \} \cup sort(A') & otherwise
  \end{array}
\right.
\ee

其中$m$是序列$A$中的最小元素，$A'$是除$m$外的剩余元素。

\[
\begin{array}{l}
m = min(A) \\
A' = A - \{ m \}
\end{array}
\]

我们并不限定序列的具体数据结构。通常在命令式的环境中$A$是数组，在函数式环境中，$A$是单向链表。我们在后面将会看到，$A$甚至可以是其他数据结构。

这一算法也可以用命令式的方式给出：

\begin{algorithmic}
\Function{Sort}{$A$}
  \State $X \gets \phi$
  \While{$A \neq \phi$}
    \State $x \gets$ \Call{Min}{$A$}
    \State $A \gets$ \Call{Del}{$A, x$}
    \State $X \gets$ \Call{Append}{$X, x$}
  \EndWhile
  \State \Return $X$
\EndFunction
\end{algorithmic}

图\ref{fig:sel-sort}描述了选择排序的过程。

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.8]{img/sel-sort.ps}
  \caption{左侧部份为已排序的元素，算法不断从剩余部份选择最小元素附加的左侧的尾部。}
  \label{fig:sel-sort}
\end{figure}

到目前为止，我们将“吃葡萄”的过程转换成了算法，但是没有考虑任何时间和空间上的开销。我们将结果保存在一个列表$X$中，把剩余部份选择出的元素从$A$中删除然后添加到$X$的末尾。实际上，我们可以复用$A$中的空间实现一个“in-place”的排序。

具体方法是我们将最小的元素保存在$A$的第一个cell中（若$A$为数组，我们用cell来表示存储单位，若$A$为链表，我们用节点来表示存储单位），将第二小的元素保存在下一个cell中，接下来是第三个cell……

可以用交换的方法来实现这一排序策略。当我们找到第$i$小的元素后，我们将它和第$i$个位置的cell交换。

\begin{algorithmic}
\Function{Sort}{$A$}
  \For{$i \gets 1$ to $|A|$}
    \State $m \gets$ \Call{Min}{$A[i...]$}
    \State \textproc{Exchange} $A[i] \leftrightarrow m$
  \EndFor
\EndFunction
\end{algorithmic}

记$A = \{a_1, a_2, ..., a_N\}$，任何时候，当我们处理第$i$个元素时，所有在第$i$个之前的部份$\{a_1, a_2, ..., a_{i-1}\}$都已经排好序了。我们找到$\{a_i, a_{i+1}, ..., a_N\}$中的最小元素，然后将其和$a_i$交换，这样第$i$个位置就保存了正确的元素。重复这一过程直到最后一个元素。

图\ref{fig:in-place-sort}描述了这一思路。

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.8]{img/in-place-sort.ps}
  \caption{左侧部份为已序元素，不断从剩余元素中找到最小的存储到正确的位置。}
  \label{fig:in-place-sort}
\end{figure}

% ================================================================
% Find the minimum
% ================================================================
\section{查找最小元素}
\index{选择排序!查找最小元素}

我们尚未完全实现选择排序，如何查找最小（或最大）的元素仍然是一个黑盒子。小孩子们是如何找到最小或最大的一粒葡萄的？这对于计算机算法来说也是一个有趣的题目。

虽然不够快，但是最简单的方法是对所有的元素进行一次扫描。存在几种不同的方法来实现扫描的过程。假设要选出最大的一粒葡萄。我们任选一颗，将它和另外一颗比较，然后留下较大的；此后我们再选另外一颗，和手里留下的这颗比较，并留下最大的。重复这一“选取―比较”的过程直到比较完所有的葡萄。

实际实践一下，我们会发现，如果不对比较过的葡萄做记号，很快就会搞乱，我们会忘记哪些葡萄比较过了，哪些还没有。有两种方法可以解决这一问题，它们分别适合不同的数据结构。

\subsection{标记}
第一种方法是将葡萄标记上编号，例如：$\{1, 2, ..., n\}$，然后按照编号的顺序进行比较。先比较第一号葡萄和第二号葡萄，选择较大的留下；然后用第三号葡萄做比较……重复这一步骤直到第$n$号葡萄。标记方法很适合处理数组。

\begin{algorithmic}
\Function{Min}{$A$}
  \State $m \gets A[1]$
  \For{$i \gets 2$ to $|A|$}
    \If{$A[i] < m$}
      \State $m \gets A[i]$
    \EndIf
  \EndFor
  \State \Return $m$
\EndFunction
\end{algorithmic}

使用\textproc{Min}函数，我们可以最终完成基本的选择排序（没有任何空间和时间上的优化）。

上述查找最小元素的方法返回的是元素的值而非它的位置（葡萄的编号），如果要实现in-place排序，还需要做一些调整。某些语言，如C++支持返回引用作为结果，因此可以直接实现元素的交换。

\lstset{language=C++}
\begin{lstlisting}
template<typename T>
T& min(T* from, T* to) {
    T* m;
    for (m = from++; from != to; ++from)
        if (*from < *m)
            m = from;
    return *m;
}

template<typename T>
void ssort(T* xs, int n) {
    for (int i = 0; i < n; ++i)
        std::swap(xs[i], min(xs+i, xs+n));
}
\end{lstlisting}

在不支持引用语意的环境中，可以返回元素的位置，而不是值作为结果。

\begin{algorithmic}
\Function{Min-At}{$A$}
  \State $m \gets$ \Call{First-Index}{$A$}
  \For{$i \gets m + 1 $ to $|A|$}
    \If{$A[i] < A[m]$}
      \State $m \gets i$
    \EndIf
  \EndFor
  \State \Return $m$
\EndFunction
\end{algorithmic}

由于传入\textproc{Min-At}的数组实际是$A$的片断$A[i...]$，我们假设第一个元素$A[i]$是最小的一个，并且逐一检查元素$A[i+1], A[i+2], ...$。函数\textproc{First-Index}()用于从参数中获取索引$i$。

下面的Python例子程序，根据这一思路实现了基本的in-place选择排序算法。它直接将数组片断的信息传入，并使用最小元素的位置。

\lstset{language=Python}
\begin{lstlisting}
def ssort(xs):
    n = len(xs)
    for i in range(n):
        m = min_at(xs, i, n)
        (xs[i], xs[m]) = (xs[m], xs[i])
    return xs

def min_at(xs, i, n):
    m = i;
    for j in range(i+1, n):
        if xs[j] < xs[m]:
            m = j
    return m
\end{lstlisting}

\subsection{分组}
\index{选择排序!尾递归查找最小值}

另外一种方法是将全部葡萄分成两部份：一组包含已经检查并比较过的所有葡萄，另外一组包含剩余未比较过的。记这两组葡萄为$A$和$B$、全部的元素（葡萄）为$L$。在开始的时候，我们尚未处理任何葡萄，因此$A$为空（$\phi$），$B$包含全部葡萄。我们可以从$B$中任选两颗葡萄进行比较，然后将较小的一颗放入$A$。此后我们不断从$B$中选择葡萄，和上次比较的获胜者（较大的一颗）对比直到$B$变成空。这时，最后的获胜者就是最小的元素，而$A$中包含$L - \{min(L)\}$，可以用于下一轮的最小值查找。

这一方法的不变关系（invariant）为：在任何时候我们有：$L = A \cup \{m\} \cup B$，其中$m$为目前找到的获胜者。

这一方法无需对葡萄进行编号。它适用于任何可被遍历的数据结构，包括链表等。令$b_1$为非空序列$B$中的任一元素，$B'$为除$b_1$外的剩余元素，上述方法可以形式化为如下函数。

\be
min'(A, m, B) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (m, A) & B = \phi \\
  min'(A \cup \{m\}, b_1, B') & b_1 < m \\
  min'(A \cup \{b_1\}, m, B') & otherwise
  \end{array}
\right.
\ee

为了选出最小元素，我们调用这一函数并传入空序列$A$。选取任何元素（例如第一个）来初始化$m$：

\be
extractMin(L) = min'(\phi, l_1, L')
\ee

其中$L'$包含$L$中除$l_1$以外的剩余元素。算法$extractMin$不仅查找最小元素，还返回剩余元素以用于接下来的排序。下面的Haskell例子程序实现了这一算法。

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort xs = x : sort xs' where
  (x, xs') = extractMin xs

extractMin (x:xs) = min' [] x xs where
  min' ys m [] = (m, ys)
  min' ys m (x:xs) = if m < x then min' (x:ys) m xs else min' (m:ys) x xs
\end{lstlisting}

第一行处理边界情况，空序列的排序结果仍为空；第二行确保序列中至少含有一个元素，因此\texttt{extractMin}函数无需再做额外的模式匹配。

有读者认为函数\texttt{min'}第二个子式应该实现如下：

\begin{lstlisting}
min' ys m (x:xs) = if m < x then min' ys ++ [x] m xs
                            else min' ys ++ [m] x xs
\end{lstlisting}

否则函数会返回逆序的列表。但这里我们需要用“cons”而不是追加。因为追加操作的复杂度是线性的，和序列$A$的长度成正比，而“cons”是常数时间$O(1)$的。我们并不需要保持待排序元素的顺序，因为排序过程本身就是对顺序的一种改变。

当然，既保持元素的相对顺序\footnote{称为稳定排序(stable sort)}，又能高效地查找最小元素，而不退化成平方复杂度是可以实现的。下面的定义满足了这一限制：

\be
extractMin(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (l_1, \phi) & |L| = 1 \\
  (l_1, L') & l_1 < m, (m, L'') = extractMin(L') \\
  (m, {l_1} \cup L'') & otherwise
  \end{array}
\right.
\ee

如果$L$只含有一个元素（称为singleton），最小值就是这一唯一的元素。否则记$l_1$为$L$中的第一个元素，除$l_1$以外的剩余元素为$L'$，即$L' = \{ l_2, l_3, ...\}$。算法递归地从$L'$中查找最小元素，结果记为$(m, L'')$，其中$m$是$L'$中的最小元素，而$L''$包含除$m$外的剩余元素。比较$l_1$和$m$以决定哪个是最终的最小值。

下面的Haskell程序实现了这一版本的选择排序。

\begin{lstlisting}
sort [] = []
sort xs = x : sort xs' where
  (x, xs') = extractMin xs

extractMin [x] = (x, [])
extractMin (x:xs) = if x < m then (x, xs) else (m, x:xs') where
  (m, xs') = extractMin xs
\end{lstlisting}

这里仅仅使用了“cons”操作，而无需添加操作。因为算法实际上从右向左处理列表。但是这实际上有一些额外开销，程序通常需要使用堆栈来保存递归的环境。元素间的相对顺序通过递归来加以保证。读者可以参考附录中关于“尾递归”的内容。

\subsection{选择排序的性能}

无论是标记法，还是分组法都需要在每轮中检查所有未排好的元素以挑选出最小值；总共进行了$n$次挑选。因此处理时间为：$n + (n-1) + (n-2) + ... + 1$次比较，即$\frac{n(n+1)}{2}$。选择排序是平方时间的$O(n^2)$算法。

和此前介绍的插入排序相比，选择排序在最好、最差和平均情况下的性能是相同的，而插入排序在最好情况下性能为线性时间$O(n)$（元素存储在一个链表中，并且顺序为逆序），最差情况下性能为平方时间$O(n^2)$。

在接下来的部份，我们将分析为何选择排序的性能较差，并尝试逐步改进它。

\begin{Exercise}

\begin{itemize}
\item 选择一门编程语言实现基本的命令式选择排序（非in-place版本）。和in-place的算法进行对比，分析时间和空间上的效率。
\end{itemize}

\end{Exercise}

% ================================================================
% Improvement 1
% ================================================================

\section{细微改进}

\subsection{比较方法参数化}
\index{选择排序!比较方法参数化}

在改进性能前，我们先将前面给出选择排序算法变得更加通用，以便能处理各种排序条件。

我们可以看到存在两种完全相反的排序需要：升序排序和降序排序。对于前者，需要不断查找最小元素；而对于后者，需要不断寻找最大元素。实际应用中，远非这两种情况，还存在各种各样得排序标准，例如按照尺寸、重量、年龄……进行排序。

我们可以将具体的排序标准作为一个比较函数传入选择排序算法，如下：

\be
sort(c, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  {m} \cup sort(c, L'') & otherwise, (m, L'') = extract(c, L')
  \end{array}
\right.
\ee

其中$extract(c, L)$的定义为：

\be
extract(c, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (l_1, \phi) & |L| = 1 \\
  (l_1, L') & c(l_1, m), (m, L'') = extract(c, L') \\
  (m, \{l_1\} \cup L'') & \lnot c(l_1, m)
  \end{array}
\right.
\ee

这里$c$是一个比较函数，它接受两个元素，将它们相比并决定哪个的顺序在前面。将“小于”操作$(<)$传入，就是我们此前给出的选择排序实现。

有些环境需要传入total ordering的比较函数，也就是说比较结果为“小于”、“等于”或者“大于”中的一个。对于选择排序，并不需要这样强的限制，$c$只要检查“小于”是否满足即可。但是作为最低要求，比较函数必须满足strict weak ordering\cite{wiki-sweak-order}：

\begin{itemize}
\item 非自反性（Irreflexivity），对于任何$x$，$x < x$不成立；
\item 非对称性（Asymmetric），对任何$x$和$y$，若$x < y$成立，则$y < x$不成立；
\item 传递性（Transitivity）, 对任何$x$、$y$和$z$，若$x < y$且$y < z$，则$x < z$成立。
\end{itemize}

The following Scheme/Lisp program translates this generic selection sorting algorithm.
The reason why we choose Scheme/Lisp here is because the lexical scope can simplify
the needs to pass the `less than' comparator for every function calls.

\lstset{language=Lisp}
\begin{lstlisting}
(define (sel-sort-by ltp? lst)
  (define (ssort lst)
    (if (null? lst)
        lst
        (let ((p (extract-min lst)))
          (cons (car p) (ssort (cdr p))))))
  (define (extract-min lst)
    (if (null? (cdr lst))
        lst
        (let ((p (extract-min (cdr lst))))
          (if (ltp? (car lst) (car p))
              lst
              (cons (car p) (cons (car lst) (cdr p)))))))
  (ssort lst))
\end{lstlisting}

Note that, both \verb|ssort| and \verb|extract-min| are inner functions, so that the
`less than' comparator \verb|ltp?| is available to them. Passing `$<$' to this function
yields the normal sorting in ascending order:

\lstset{language=Lisp}
\begin{lstlisting}
(sel-sort-by < '(3 1 2 4 5 10 9))
;Value 16: (1 2 3 4 5 9 10)
\end{lstlisting}

It's possible to pass varies of comparator to imperative selection sort as well. This
is left as an exercise to the reader.

For the sake of brevity, we only consider sorting elements in ascending order in
the rest of this chapter. And we'll not pass comparator as a parameter unless it's
necessary.

\subsection{Trivial fine tune}

The basic in-place imperative selection sorting algorithm iterates all elements, and picking the
minimum by traversing as well. It can be written in a compact way, that we
inline the minimum finding part as an inner loop.

\begin{algorithmic}
\Procedure{Sort}{$A$}
  \For{ $i \gets 1$ to $|A|$}
    \State $m \gets i$
    \For{$j \gets i+1$ to $|A|$}
      \If{$A[i] < A[m]$}
        \State $m \gets i$
      \EndIf
    \EndFor
    \State \textproc{Exchange} $A[i] \leftrightarrow A[m]$
  \EndFor
\EndProcedure
\end{algorithmic}

Observe that, when we are sorting $N$ elements, after the first $N-1$ minimum ones are selected,
the left only one, is definitely the $N$-th big element, so that we need NOT find the
minimum if there is only one element in the list. This indicates that the outer loop can
iterate to $N-1$ instead of $N$.

Another place we can fine tune, is that we needn't swap the elements if the $i$-th minimum one
is just $A[i]$. The algorithm can be modified accordingly as below:

\begin{algorithmic}
\Procedure{Sort}{$A$}
  \For{ $i \gets 1$ to $|A|-1$}
    \State $m \gets i$
    \For{$j \gets i+1$ to $|A|$}
      \If{$A[i] < A[m]$}
        \State $m \gets i$
      \EndIf
    \EndFor
    \If{$m \neq i$}
      \State \textproc{Exchange} $A[i] \leftrightarrow A[m]$
    \EndIf
  \EndFor
\EndProcedure
\end{algorithmic}

Definitely, these modifications won't affects the performance in terms of big-O.

\subsection{Cock-tail sort}
\index{Cock-tail sort}
Knuth gave an alternative realization of selection sort in \cite{TAOCP}. Instead of selecting the
minimum each time, we can select the maximum element, and put it to the last position. This method
can be illustrated by the following algorithm.

\begin{algorithmic}
\Procedure{Sort'}{$A$}
  \For{ $i \gets |A|$ down-to $2$}
    \State $m \gets i$
    \For{$j \gets 1$ to $i-1$}
      \If{$A[m] < A[i]$}
        \State $m \gets i$
      \EndIf
    \EndFor
    \State \textproc{Exchange} $A[i] \leftrightarrow A[m]$
  \EndFor
\EndProcedure
\end{algorithmic}

As shown in figure \ref{fig:knuth-ssort}, at any time, the elements on right most side
are sorted. The algorithm scans all unsorted ones, and locate the maximum. Then, put
it to the tail of the unsorted range by swapping.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.8]{img/knuth-ssort.ps}
  \caption{Select the maximum every time and put it to the end.}
  \label{fig:knuth-ssort}
\end{figure}

This version reveals the fact that, selecting the maximum element can sort the element in
ascending order as well. What's more, we can find both the minimum and the maximum elements
in one pass of traversing, putting the minimum at the first location, while putting the
maximum at the last position. This approach can speed up the sorting slightly (halve the
times of the outer loop).

\begin{algorithmic}
\Procedure{Sort}{$A$}
  \For{$i \gets 1 $ to $\lfloor \frac{|A|}{2} \rfloor$}
    \State $min \gets i$
    \State $max \gets |A| + 1 - i$
    \If{$A[max] < A[min]$}
      \State \textproc{Exchange} $A[min] \leftrightarrow A[max]$
    \EndIf
    \For{$j \gets i + 1$ to $|A| - i$}
      \If{$A[j] < A[min]$}
        \State $min \gets j$
      \EndIf
      \If{$A[max] < A[j]$}
        \State $max \gets j$
      \EndIf
    \EndFor
    \State \textproc{Exchange} $A[i] \leftrightarrow A[min]$
    \State \textproc{Exchange} $A[|A|+1-i] \leftrightarrow A[max]$
  \EndFor
\EndProcedure
\end{algorithmic}

This algorithm can be illustrated as in figure \ref{fig:cock-tail-sort}, at any time, the left most and right most
parts contain sorted elements so far. That the smaller sorted ones are on the left, while the bigger sorted ones
are on the right. The algorithm scans the unsorted ranges, located both the minimum and the maximum positions,
then put them to the head and the tail position of the unsorted ranges by swapping.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.7]{img/cock-tail-sort.ps}
  \caption{Select both the minimum and maximum in one pass, and put them to the proper positions.}
  \label{fig:cock-tail-sort}
\end{figure}

Note that it's necessary to swap the left most and right most elements before the inner loop if they are
not in correct order. This is because we scan the range excluding these two elements. Another method is to
initialize the first element of the unsorted range as both the maximum and minimum before the inner loop.
However, since we need two swapping operations after the scan, it's possible that the first swapping moves
the maximum or the minimum from the position we just found, which leads the second swapping malfunctioned.
How to solve this problem is left as exercise to the reader.

The following Python example program implements this cock-tail sort algorithm.

\lstset{language=Python}
\begin{lstlisting}
def cocktail_sort(xs):
    n = len(xs)
    for i in range(n / 2):
        (mi, ma) = (i, n - 1 -i)
        if xs[ma] < xs[mi]:
            (xs[mi], xs[ma]) = (xs[ma], xs[mi])
        for j in range(i+1, n - 1 - i):
            if xs[j] < xs[mi]:
                mi = j
            if xs[ma] < xs[j]:
                ma = j
        (xs[i], xs[mi]) = (xs[mi], xs[i])
        (xs[n - 1 - i], xs[ma]) = (xs[ma], xs[n - 1 - i])
    return xs
\end{lstlisting}

It's possible to realize cock-tail sort in functional approach as well. An intuitive recursive description
can be given like this:

\begin{itemize}
  \item Trivial edge case: If the list is empty, or there is only one element in the list, the sorted result is obviously the origin list;
  \item Otherwise, we select the minimum and the maximum, put them in the head and tail positions, then recursively sort the rest elements.
\end{itemize}

This algorithm description can be formalized by the following equation.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L & |L| \leq 1 \\
  \{ l_{min} \} \cup sort(L'') \cup \{ l_{max} \} & otherwise
  \end{array}
\right.
\ee

Where the minimum and the maximum are extracted from $L$ by a function $select(L)$.

\[
(l_{min}, L'', l_{max}) = select(L)
\]

Note that, the minimum is actually linked to the front of the recursive sort result. Its semantic is a constant $O(1)$
time `cons' (refer to the appendix of this book for detail). While the maximum is appending to the tail. This
is typically a linear $O(n)$ time expensive operation. We'll optimize it later.

Function $select(L)$ scans the whole list to find both the minimum and the maximum. It can be defined as below:

\be
select(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (min(l_1, l_2), max(l_1, l_2)) & L = \{ l_1, l_2\} \\
  (l_1, \{ l_{min}\} \cup L'', l_{max}) & l_1 < l_{min} \\
  (l_{min}, \{l_{max}\} \cup L'', l_1) & l_{max} < l_1 \\
  (l_{min}, \{l_1\} \cup L'', l_{max}) & otherwise
  \end{array}
\right.
\ee

Where $(l_{min}, L'', l_{max}) = select(L')$ and $L'$ is the rest of the list except for the first element $l_1$.
If there are only two elements in the list, we pick the smaller as the minimum, and the bigger as the maximum.
After extract them, the list becomes empty. This is the trivial edge case; Otherwise, we take the first element
$l_1$ out, then recursively perform selection on the rest of the list. After that, we compare if $l_1$ is less
then the minimum or greater than the maximum candidates, so that we can finalize the result.

Note that for all the cases, there is no appending operation to form the result. However, since selection must
scan all the element to determine the minimum and the maximum, it is bound to $O(n)$ linear time.

The complete example Haskell program is given as the following.

\lstset{language=Haskell}
\begin{lstlisting}
csort [] = []
csort [x] = [x]
csort xs = mi : csort xs' ++ [ma] where
  (mi, xs', ma) = extractMinMax xs

extractMinMax [x, y] = (min x y, [], max x y)
extractMinMax (x:xs) | x < mi = (x, mi:xs', ma)
                     | ma < x = (mi, ma:xs', x)
                     | otherwise = (mi, x:xs', ma)
  where (mi, xs', ma) = extractMinMax xs
\end{lstlisting}

We mentioned that the appending operation is expensive in this intuitive version. It can be improved.
This can be achieved in two steps. The first step is to convert the cock-tail sort into tail-recursive
call. Denote the sorted small ones as $A$, and sorted big ones as $B$ in figure \ref{fig:cock-tail-sort}.
We use $A$ and $B$ as accumulators. The new cock-tail sort is defined as the following.

\be
sort'(A, L, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A \cup L \cup B & L = \phi \lor |L| = 1 \\
  sort'(A \cup \{ l_{min}\}, L'', \{ l_{max}\} \cup B) & otherwise
  \end{array}
\right.
\ee

Where $l_{min}$, $l_{max}$ and $L''$ are defined as same as before. And we start sorting by passing
empty $A$ and $B$: $sort(L) = sort'(\phi, L, \phi)$.

Besides the edge case, observing that the appending operation only happens on $A \cup \{l_{min} \}$; while
$l_{max}$ is only linked to the head of $B$. This appending occurs in every recursive call. To eliminate
it, we can store $A$ in reverse order as $\overleftarrow{A}$, so that $l_{max}$ can be `cons' to the
head instead of appending. Denote $cons(x, L) = \{x\} \cup L$ and $append(L, x) = L \cup \{x\}$,
we have the below equation.

\be
\begin{array}{rl}
append(L, x) & = reverse(cons(x, reverse(L))) \\
             & = reverse(cons(x, \overleftarrow{L}))
\end{array}
\ee

Finally, we perform a reverse to turn $\overleftarrow{A}$ back to $A$.
Based on this idea, the algorithm can be improved one more step as the following.

\be
sort'(A, L, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  reverse(A) \cup B & L = \phi \\
  reverse(\{l_1\} \cup A) \cup B & |L| = 1 \\
  sort'(\{l_{min}\} \cup A, L'', \{l_{max}\} \cup B)
  \end{array}
\right.
\ee

This algorithm can be implemented by Haskell as below.

\lstset{language=Haskell}
\begin{lstlisting}
csort' xs = cocktail [] xs [] where
  cocktail as [] bs = reverse as ++ bs
  cocktail as [x] bs = reverse (x:as) ++ bs
  cocktail as xs bs = let (mi, xs', ma) = extractMinMax xs
                      in cocktail (mi:as) xs' (ma:bs)
\end{lstlisting}

\begin{Exercise}
  \begin{itemize}
    \item Realize the imperative basic selection sort algorithm, which can take a comparator as a parameter. Please try both dynamic typed language and static typed language. How to annotate the type of the comparator as general as possible in a static typed language?
   \item Implement Knuth's version of selection sort in your favorite programming language.
   \item An alternative to realize cock-tail sort is to assume the $i$-th element both the minimum and the maximum, after the inner loop, the minimum and maximum are found, then we can swap the the minimum to the $i$-th position, and
the maximum to position $|A|+1-i$. Implement this solution in your favorite imperative language. Please note that
there are several special edge cases should be handled correctly:
    \begin{itemize}
      \item $A = \{max, min, ...\}$;
      \item $A = \{..., max, min\}$;
      \item $A = \{max, ..., min\}$.
    \end{itemize}
    Please don't refer to the example source code along with this chapter before you try to solve this problem.
  \end{itemize}
\end{Exercise}

% ================================================================
% Improvement 2
% ================================================================

\section{Major improvement}

Although cock-tail sort halves the numbers of loop, the performance is still bound to quadratic time.
It means that, the method we developed so far handles big data poorly compare to other divide and conquer
sorting solutions.

To improve selection based sort essentially, we must analyze where is the bottle-neck. In order to sort
the elements by comparison, we must examine all the elements for ordering. Thus the outer loop of selection
sort is necessary. However, must it scan all the elements every time to select the minimum? Note that
when we pick the smallest one at the first time, we actually traverse the whole collection, so that
we know which ones are relative big, and which ones are relative small partially.

The problem is that, when we select the further minimum elements, instead of re-using the ordering information
we obtained previously, we drop them all, and blindly start a new traverse.

So the key point to improve selection based sort is to re-use the previous result. There are several approaches,
we'll adopt an intuitive idea inspired by football match in this chapter.

\subsection{Tournament knock out}
\index{Trounament knock out}
The football world cup is held every four years. There are 32 teams from different continent
play the final games. Before 1982, there were 16 teams compete for the tournament finals\cite{wiki-wc}.

For simplification purpose, let's go back to 1978 and imagine a way to determine the champion:
In the first round, the teams
are grouped into 8 pairs to play the game; After that, there will be 8 winner, and 8 teams will
be out. Then in the second round, these 8 teams are grouped into 4 pairs. This time there
will be 4 winners after the second round of games; Then the top 4 teams are divided into
2 pairs, so that there will be only two teams left for the final game.

The champion is determined after the total 4 rounds of games. And there are actually $8+4+2+1 = 16$
games. Now we have the world cup champion, however, the world cup game won't finish at this stage,
we need to determine which is the silver medal team.

Readers may argue that isn't the team beaten by the champion at the final game the second best?
This is true according to the real world cup rule. However, it isn't fair enough in some sense.

We often heard about the so called `group of death', Let's suppose that Brazil team is
grouped with Deutch team at the very beginning. Although both teams are quite strong, one of
them must be knocked out. It's quite possible that even the team loss that game can beat
all the other teams except for the champion. Figure \ref{fig:tournament-tree-1} illustrates such
case.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.25]{img/tournament-tree-1.ps}
  \caption{The element 15 is knocked out in the first round.}
  \label{fig:tournament-tree-1}
\end{figure}

Imagine that every team has a number. The bigger the number, the stronger the team. Suppose
that the stronger team always beats the team with smaller number, although this is not true
in real world. But this simplification is fair enough for us to develop the tournament knock
out solution. This maximum number which represents the champion is 16. Definitely, team with
number 14 isn't the second best according to our rules. It should be 15, which is knocked
out at the first round of comparison.

The key question here is to find an effective way to locate the second maximum number in this
tournament tree. After that, what we need is to apply the same method to select the third,
the fourth, ..., to accomplish the selection based sort.

One idea is to assign the champion a very small number (for instance, $-\infty$),
so that it won't be selected next time, and the second best one, becomes the new champion.
However, suppose there are $2^m$ teams for some natural number $m$, it still takes
$2^{m-1} + 2^{m-2} + ... + 2 + 1 = 2^m$ times of comparison to determine the new
champion. Which is as slow as the first time.

Actually, we needn't perform a bottom-up comparison at all since the tournament tree
stores plenty of ordering information. Observe that, the second best team must
be beaten by the champion at sometime, or it will be the final winner. So we
can track the path from the root of the tournament tree to the leaf of the
champion, examine all the teams along with this path to find the second best team.

In figure \ref{fig:tournament-tree-1}, this path is marked in gray color, the elements
to be examined are $\{14, 13, 7, 15\}$. Based on this idea, we refine the algorithm
like below.

\begin{enumerate}
\item Build a tournament tree from the elements to be sorted, so that the champion (the maximum) becomes the root;
\item Extract the root from the tree, perform a top-down pass and replace the maximum with $-\infty$;
\item Perform a bottom-up back-track along the path, determine the new champion and make it as the new root;
\item Repeat step 2 until all elements have been extracted.
\end{enumerate}

Figure \ref{fig:tournament-tree-2}, \ref{fig:tournament-tree-3}, and \ref{fig:tournament-tree-4}
show the steps of applying this strategy.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.25]{img/tournament-tree-2.ps}
  \caption{Extract 16, replace it with $-\infty$, 15 sifts up to root.}
  \label{fig:tournament-tree-2}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.25]{img/tournament-tree-3.ps}
  \caption{Extract 15, replace it with $-\infty$, 14 sifts up to root.}
  \label{fig:tournament-tree-3}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.25]{img/tournament-tree-4.ps}
  \caption{Extract 14, replace it with $-\infty$, 13 sifts up to root.}
  \label{fig:tournament-tree-4}
\end{figure}

We can reuse the binary tree definition given in the first chapter of this book to represent
tournament tree. In order to back-track from leaf to the root, every node should hold a reference
to its parent (concept of pointer in some environment such as ANSI C):

\lstset{language=C}
\begin{lstlisting}
struct Node {
  Key key;
  struct Node *left, *right, *parent;
};
\end{lstlisting}

To build a tournament tree from a list of elements (suppose the number of elements are $2^m$ for some $m$),
we can first wrap each element as a leaf, so that we obtain a list of binary trees. We take every two
trees from this list, compare their keys, and form a new binary tree with the bigger key as the root;
the two trees are set as the left and right children of this new binary tree. Repeat this operation
to build a new list of trees. The height of each tree is increased by 1. Note that the size of the tree
list halves after such a pass, so that we can keep reducing the list until there is only one tree left.
And this tree is the finally built tournament tree.

\begin{algorithmic}
\Function{Build-Tree}{$A$}
  \State $T \gets \phi$
  \For{each $x \in A$}
    \State $t \gets $ \Call{Create-Node}{}
    \State \Call{Key}{$t$} $\gets x$
    \State \Call{Append}{$T, t$}
  \EndFor
  \While{$|T| > 1$}
    \State $T' \gets \phi$
    \For{every $t_1, t_2 \in$ T}
      \State $t \gets $ \Call{Create-Node}{}
      \State \Call{Key}{$t$} $\gets$ \textproc{Max}(\Call{Key}{$t_1$}, \Call{Key}{$t_2$})
      \State \Call{Left}{$t$} $\gets t_1$
      \State \Call{Right}{$t$} $\gets t_2$
      \State \Call{Parent}{$t_1$} $\gets t$
      \State \Call{Parent}{$t_2$} $\gets t$
      \State \Call{Append}{$T', t$}
    \EndFor
    \State $T \gets T'$
  \EndWhile
  \State \Return $T[1]$
\EndFunction
\end{algorithmic}

Suppose the length of the list $A$ is $n$, this algorithm firstly traverses the list to build tree,
which is linear to $n$ time. Then it repeatedly compares pairs, which loops proportion to
$n + \frac{n}{2} + \frac{n}{4} + ... + 2 = 2n$. So the total performance is bound to $O(n)$ time.

The following ANSI C program implements this tournament tree building algorithm.

\lstset{language=C}
\begin{lstlisting}
struct Node* build(const Key* xs, int n) {
    int i;
    struct Node *t, **ts = (struct Node**) malloc(sizeof(struct Node*) * n);
    for (i = 0; i < n; ++i)
        ts[i] = leaf(xs[i]);
    for (; n > 1; n /= 2)
        for (i = 0; i < n; i += 2)
            ts[i/2] = branch(max(ts[i]->key, ts[i+1]->key), ts[i], ts[i+1]);
    t = ts[0];
    free(ts);
    return t;
}
\end{lstlisting}

The type of key can be defined somewhere, for example:

\lstset{language=C}
\begin{lstlisting}
typedef int Key;
\end{lstlisting}

Function \verb|leaf(x)| creats a leaf node, with value \verb|x| as key,
and sets all its fields, left, right and parent to \verb|NIL|.
While function \verb|branch(key, left, right)| creates a branch node, and links the new
created node as parent of its two children if they are not empty. For the sake of
brevity, we skip the detail of them. They are left as exercise to the reader, and
the complete program can be downloaded along with this book.

Some programming environments, such as Python provides tool to iterate every two elements
at a time, for example:

\lstset{language=Python}
\begin{lstlisting}
for x, y in zip(*[iter(ts)]*2):
\end{lstlisting}

We skip such language specific feature, readers can refer to the Python example program
along with this book for details.

When the maximum element is extracted from the tournament tree, we replace it with $-\infty$,
and repeatedly replace all these values from the root to the leaf. Next, we back-track
to root through the parent field, and determine the new maximum element.

\begin{algorithmic}
\Function{Extract-Max}{$T$}
  \State $m \gets$ \Call{Key}{$T$}
  \State \Call{Key}{$T$} $\gets -\infty$
  \While{$\lnot$ \Call{Leaf?}{$T$}}  \Comment{The top down pass}
    \If{\textproc{Key}(\Call{Left}{$T$}) $ = m$}
      \State $T \gets$ \Call{Left}{$T$}
    \Else
      \State $T \gets$ \Call{Right}{$T$}
    \EndIf
    \State \Call{Key}{$T$} $\gets -\infty$
  \EndWhile
  \While{\Call{Parent}{$T$} $\neq \phi$} \Comment{The bottom up pass}
    \State $T \gets$ \Call{Parent}{$T$}
    \State \Call{Key}{$T$} $\gets$ \textproc{Max}(\textproc{Key}(\Call{Left}{$T$}), \textproc{Key}(\Call{Right}{$T$}))
  \EndWhile
  \State \Return $m$
\EndFunction
\end{algorithmic}

This algorithm returns the extracted maximum element, and modifies the tournament tree in-place.
Because we can't represent $-\infty$ in real program by limited length of word, one approach is to define
a relative negative big number, which is less than all the elements in the tournament tree, for example,
suppose all the elements are greater than -65535, we can define negative infinity as below:

\lstset{language=C}
\begin{lstlisting}
#define N_INF -65535
\end{lstlisting}

We can implements this algorithm as the following ANSI C example program.

\lstset{language=C}
\begin{lstlisting}
Key pop(struct Node* t) {
    Key x = t->key;
    t->key = N_INF;
    while (!isleaf(t)) {
        t = t->left->key == x ? t->left : t->right;
        t->key = N_INF;
    }
    while (t->parent) {
        t = t->parent;
        t->key = max(t->left->key, t->right->key);
    }
    return x;
}
\end{lstlisting}

The behavior of \textproc{Extract-Max} is quite similar to the pop operation for some data structures,
such as queue, and heap, thus we name it as \verb|pop| in this code snippet.

Algorithm \textproc{Extract-Max} process the tree in tow passes, one is top-down, then a bottom-up along
the path that the `champion team wins the world cup'.
Because the tournament tree is well balanced,
the length of this path, which is the height of the tree, is bound to $O(\lg n)$,
where $n$ is the number of the elements to be sorted (which are equal to the number of leaves).
Thus the performance of this algorithm is $O(\lg n)$.

It's possible to realize the tournament knock out sort now. We build a tournament tree from the elements
to be sorted, then continuously extract the maximum. If we want to sort in monotonically increase order,
we put the first extracted one to the right most, then insert the further extracted elements one by one
to left; Otherwise if we want to sort in decrease order, we can just append the extracted elements
to the result. Below is the algorithm sorts elements in ascending order.

\begin{algorithmic}
\Procedure{Sort}{$A$}
  \State $T \gets$ \Call{Build-Tree}{$A$}
  \For{$i \gets |A|$ down to $1$}
    \State $A[i] \gets$ \Call{Extract-Max}{$T$}
  \EndFor
\EndProcedure
\end{algorithmic}

Translating it to ANSI C example program is straightforward.

\lstset{language=C}
\begin{lstlisting}
void tsort(Key* xs, int n) {
    struct Node* t = build(xs, n);
    while(n)
        xs[--n] = pop(t);
    release(t);
}
\end{lstlisting}

This algorithm firstly takes $O(n)$ time to build the tournament tree, then performs $n$ pops to select
the maximum elements so far left in the tree. Since each pop operation is bound to $O(\lg n)$, thus
the total performance of tournament knock out sorting is $O(n \lg n)$.

\subsubsection{Refine the tournament knock out}
\index{Tounament knock out!explict infinity}
It's possible to design the tournament knock out algorithm in purely functional approach. And we'll see
that the two passes (first top-down replace the champion with $-\infty$, then bottom-up determine the
new champion) in pop operation can be combined in recursive manner, so that we needn't the parent field
any more. We can re-use the functional binary tree definition as the following example Haskell code.

\lstset{language=Haskell}
\begin{lstlisting}
data Tr a = Empty | Br (Tr a) a (Tr a)
\end{lstlisting}

Thus a binary tree is either empty or a branch node contains a key, a left sub tree and a right sub tree.
Both children are again binary trees.

We've use hard coded big negative number to represents $-\infty$. However, this solution is ad-hoc, and
it forces all elements to be sorted are greater than this pre-defined magic number. Some programming
environments support algebraic type, so that we can define negative infinity explicitly. For instance,
the below Haskell program setups the concept of infinity \footnote{The order of the definition of `NegInf',
regular number, and `Inf' is significant if we want to derive the default, correct comparing behavior of `Ord'.
Anyway, it's possible to specify the detailed order by make it as an instance of `Ord'. However, this is
Language specific feature which is out of the scope of this book. Please refer to other textbook about Haskell.}.

\lstset{language=Haskell}
\begin{lstlisting}
data Infinite a = NegInf | Only a | Inf deriving (Eq, Ord)
\end{lstlisting}

From now on, we switch back to use the $min()$ function to determine the winner, so that the tournament selects the minimum
instead of the maximum as the champion.

Denote function $key(T)$ returns the key of the tree rooted at $T$. Function $wrap(x)$ wraps the element
$x$ into a leaf node. Function $tree(l, k, r)$ creates a branch node, with $k$ as the key, $l$ and $r$
as the two children respectively.

The knock out process, can be represented as comparing two trees, picking the smaller key as the new
key, and setting these two trees as children:

\be
branch(T_1, T_2) = tree(T_1, min(key(T_1), key(T_2)), T_2)
\ee

This can be implemented in Haskell word by word:

\lstset{language=Haskell}
\begin{lstlisting}
branch t1 t2 = Br t1 (min (key t1) (key t2)) t2
\end{lstlisting}

There is limitation in our tournament sorting algorithm so far. It only accepts collection of elements
with size of $2^m$, or we can't build a complete binary tree. This can be actually solved in the tree
building process. Remind that we pick two trees every time, compare and pick the winner. This is perfect
if there are always even number of trees. Considering a case in football match, that one team is absent
for some reason (sever flight delay or whatever), so that there left one team without a challenger.
One option is to make this team the winner, so that it will attend the further games. Actually, we can
use the similar approach.

To build the tournament tree from a list of elements, we wrap every element into a leaf, then start the
building process.

\be
build(L) = build'(\{wrap(x) | x \in L\})
\ee

The $build'(\mathbb{T})$ function terminates when there is only one tree left in $\mathbb{T}$, which
is the champion. This is the trivial edge case. Otherwise, it groups every two trees in a pair to determine
the winners. When there are odd numbers of trees, it just makes the last tree as the winner to attend the
next level of tournament and recursively repeats the building process.

\be
build'(\mathbb{T}) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \mathbb{T} & |\mathbb{T}| \leq 1 \\
  build'(pair(\mathbb{T})) & otherwise
  \end{array}
\right.
\ee

Note that this algorithm actually handles another special cases, that the list to be sort is empty.
The result is obviously empty.

Denote $\mathbb{T} = \{ T_1, T_2, ...\}$ if there are at least two trees, and $\mathbb{T}'$ represents
the left trees by removing the first two. Function $pair(\mathbb{T})$ is defined as the following.

\be
pair(\mathbb{T}) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ branch(T_1, T_2) \} \cup pair(\mathbb{T}') & |\mathbb{T}| \geq 2 \\
  \mathbb{T} & otherwise
  \end{array}
\right.
\ee

The complete tournament tree building algorithm can be implemented as the below example Haskell program.

\lstset{language=Haskell}
\begin{lstlisting}
fromList :: (Ord a) => [a] -> Tr (Infinite a)
fromList = build . (map wrap) where
  build [] = Empty
  build [t] = t
  build ts = build $ pair ts
  pair (t1:t2:ts) = (branch t1 t2):pair ts
  pair ts = ts
\end{lstlisting} %$

When extracting the champion (the minimum) from the tournament tree, we need examine either the left child
sub-tree or the right one has the same key, and recursively extract on that tree until arrive at the leaf
node. Denote the left sub-tree of $T$ as $L$, right sub-tree as $R$, and $K$ as its key. We can define this popping
algorithm as the following.

\be
pop(T) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  tree(\phi, \infty, \phi) & L = \phi \land R = \phi \\
  tree(L', min(key(L'), key(R)), R) & K = key(L), L' = pop(L) \\
  tree(L, min(key(L), key(R')), R') & K = key(R), R' = pop(R)
  \end{array}
\right.
\ee

It's straightforward to translate this algorithm into example Haskell code.

\lstset{language=Haskell}
\begin{lstlisting}
pop (Br Empty _ Empty) = Br Empty Inf Empty
pop (Br l k r) | k == key l = let l' = pop l in Br l' (min (key l') (key r)) r
               | k == key r = let r' = pop r in Br l (min (key l) (key r')) r'
\end{lstlisting}

Note that this algorithm only removes the current champion without returning it. So it's necessary to
define a function to get the champion at the root node.

\be
top(T) = key(T)
\ee

With these functions defined, tournament knock out sorting can be formalized by using them.

\be
sort(L) = sort'(build(L))
\ee

Where $sort'(T)$ continuously pops the minimum element to form a result list

\be
sort'(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & T = \phi \lor key(T) = \infty \\
  \{ top(T) \} \cup sort'(pop(T)) & otherwise
  \end{array}
\right.
\label{eq:tsort}
\ee

The rest of the Haskell code is given below to complete the implementation.

\lstset{language=Haskell}
\begin{lstlisting}
top = only . key

tsort :: (Ord a) => [a] -> [a]
tsort = sort' . fromList where
    sort' Empty = []
    sort' (Br _ Inf _) = []
    sort' t = (top t) : (sort' $ pop t)
\end{lstlisting} %$

And the auxiliary function \verb|only|, \verb|key|, \verb|wrap| accomplished with explicit infinity support are list
as the following.

\lstset{language=Haskell}
\begin{lstlisting}
only (Only x) = x
key (Br _ k _ ) = k
wrap x = Br Empty (Only x) Empty
\end{lstlisting}

\begin{Exercise}
  \begin{itemize}
    \item Implement the helper function \verb|leaf()|, \verb|branch|, \verb|max()| \verb|lsleaf()|, and \verb|release()| to complete the imperative tournament tree program.
    \item Implement the imperative tournament tree in a programming language support GC (garbage collection).
    \item Why can our tournament tree knock out sort algorithm handle duplicated elements (elements with same value)? We say a sorting algorithm stable, if it keeps the original order of elements with same value. Is the tournament tree knock out sorting stable?
    \item Design an imperative tournament tree knock out sort algorithm, which satisfies the following:
      \begin{itemize}
        \item Can handle arbitrary number of elements;
        \item Without using hard coded negative infinity, so that it can take elements with any value.
      \end{itemize}
    \item Compare the tournament tree knock out sort algorithm and binary tree sort algorithm, analyze efficiency both in time and space.
    \item Compare the heap sort algorithm and binary tree sort algorithm, and do same analysis for them.
  \end{itemize}
\end{Exercise}

\subsection{Final improvement by using heap sort}

We manage improving the performance of selection based sorting to $O(n \lg n)$ by using tournament knock out.
This is the limit of comparison based sort according to \cite{TAOCP}. However, there are still rooms for improvement.
After sorting, there lefts a complete binary tree with all leaves and branches hold useless infinite values.
This isn't space efficient at all. Can we release the nodes when popping?

Another observation is that if there are $n$ elements to be sorted, we actually allocate about $2n$ tree nodes.
$n$ for leaves and $n$ for branches. Is there any better way to halve the space usage?

The final sorting structure described in equation \ref{eq:tsort} can be easily uniformed to a more general
one if we treat the case that the tree is empty if its root holds infinity as key:

\be
sort'(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & T = \phi\\
  \{ top(T) \} \cup sort'(pop(T)) & otherwise
  \end{array}
\right.
\ee

This is exactly as same as the one of heap sort we gave in previous chapter.
Heap always keeps the minimum (or the maximum) on the top, and provides fast pop operation.
The binary heap by implicit array encodes the tree structure in array index, so there aren't
any extra spaces allocated except for the $N$ array cells. The functional heaps,
such as leftist heap and splay heap allocate $N$ nodes as well. We'll introduce more
heaps in next chapter which perform well in many aspects.

\section{Short summary}
In this chapter, we present the evolution process of selection based sort. selection
sort is easy and commonly used as example to teach students about embedded looping.
It has simple and straightforward structure, but the performance is quadratic.
In this chapter, we do see that there exists ways to improve
it not only by some fine tuning, but also fundamentally change the data
structure, which leads to tournament knock out and heap sort.

\begin{thebibliography}{99}

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)''. Addison-Wesley Professional; 2 edition (May 4, 1998) ISBN-10: 0201896850 ISBN-13: 978-0201896855

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein.
``Introduction to Algorithms, Second Edition''. ISBN:0262032937. The MIT Press. 2001

\bibitem{wiki-sweak-order}
Wikipedia. ``Strict weak order''. http://en.wikipedia.org/wiki/Strict\_weak\_order

\bibitem{wiki-wc}
Wikipedia. ``FIFA world cup''. http://en.wikipedia.org/wiki/FIFA\_World\_Cup

\end{thebibliography}

\ifx\wholebook\relax\else
\end{document}
\fi
