\ifx\wholebook\relax \else
% ------------------------

\documentclass[UTF8]{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
\input{../../../common-zh-cn.tex}

\setcounter{page}{1}

\begin{document}

%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{二叉堆}

\author{刘新宇
\thanks{{\bfseries 刘新宇 } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\maketitle
\fi

\markboth{二叉堆}{初等算法}

\ifx\wholebook\relax
\chapter{二叉堆}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{简介}
\label{introduction}
\index{二叉堆}

堆是被广泛应用的一种数据结构。堆可以用于解决很多实际问题，包括排序、带有优先级的调度，实现图算法等等\cite{wiki-heap}。

堆有很多不同的实现，其中最常见的一种通过数组来表示二叉树\cite{CLRS}，进而实现堆。例如C++标准库STL中的heap，和Python库中的heapq都是这样实现堆的。由R.W. Floyd给出的最高效的堆排序算法也是利用这个实现\cite{wiki-heapsort}\cite{rosetta-heapsort}。

堆是一种通用的概念，它也可以用数组以外的其他数据结构来实现。本章中，我们给出一些使用二叉树来实现的堆，包括左偏堆（Leftist Heap）、Skew堆、和Splay堆。它们非常适合纯函数式实现i\cite{okasaki-book}。

堆是一种满足如下性质的数据结构：
\begin{itemize}
\item 顶部（top）总是保存着最小（或最大）的元素；
\item 弹出（pop）操作将顶部元素移除，同时保持堆的性质，新的顶部元素仍然是剩余元素中的最小（或最大）值；
\item 将新元素插入到堆中仍然保持堆的性质，顶部元素还是所有元素中的最大（或最小）值；
\item 其他操作（例如将两个堆合并），都会保持堆的性质。
\end{itemize}

这一定义是递归的。它并没有限定实现堆的低层数据结构。

我们称顶部保存最小元素的堆为\underline{最小堆}，顶部保存最大元素的堆为\underline{最大堆}。

% ================================================================
%                 Implicit binary heap by array
% ================================================================
\section{用数组实现隐式二叉堆}
\label{ibheap}
\index{隐式二叉堆}

考虑堆的定义，我们可以用树来实现堆。一种直观的想法是将最小（或最大）元素保存在树的根节点。获取“顶部”元素时，我们可以直接返回根节点中的数据。执行“弹出”操作时，我们将根节点删除，然后从子节点中重建树。

我们称使用二叉树实现的堆为\underline{二叉堆}。本章介绍三种不同的二叉堆实现。

% ================================================================
%                 Definition
% ================================================================
\subsection{定义}

第一种实现称为隐式二叉树。考虑如何用数组来表示一棵完全二叉树（例如，有些编程语言中没有结构或记录等复合数据类型，只能使用数组来定义二叉树）。我们可以将全部元素自顶向下（从根节点开始，到叶子节点为止）压缩放入数组中。

图\ref{fig:tree-array-map}展示了一棵完全二叉树和它相应的数组表示形式。

\begin{figure}[htbp]
       \begin{center}
       	  \includegraphics[scale=0.5]{img/tree-array-map-tree.ps}
          \includegraphics[scale=0.5]{img/tree-array-map-array.ps}
        \caption{完全二叉树到数组的映射。} \label{fig:tree-array-map}
       \end{center}
\end{figure}

这一树和数组之间的映射可以定义如下（令数组的索引从1开始）：

\begin{algorithmic}[1]
\Function{Parent}{$i$}
  \State \Return $\lfloor \frac{i}{2} \rfloor$
\EndFunction
\Statex
\Function{Left}{$i$}
  \State \Return $2i$
\EndFunction
\Statex
\Function{Right}{$i$}
  \State \Return $2i+1$
\EndFunction
\end{algorithmic}

对数组中任意第$i$个元素代表的节点，由于二叉树是完全的，我们可以通过定位到第$\lfloor i/2 \rfloor$个元素找到它的父节点；它的左子树对应第$2i$个元素，而右子树对应第$2i+1$个元素。如果子节点的索引超出了数组的长度，说明这它不含有相应的子树（例如叶子节点）。

在实际的应用中，父节点和子树的访问可以通过位运算实现，例如下面的C代码。注意，代码中的索引从0开始。

\lstset{language=C}
\begin{lstlisting}
#define PARENT(i) ((((i) + 1) >> 1) - 1)

#define LEFT(i) (((i) << 1) + 1)

#define RIGHT(i) (((i) + 1) << 1)
\end{lstlisting}

% ================================================================
%                 Heapify
% ================================================================
\subsection{Heapify}
\index{二叉堆!Heapify}

堆算法中最重要的部份就是维护堆的性质：即顶部元素为最小（或最大）元素。

对于用数组表示的二叉堆，给定任何索引为$i$的节点，我们可以检查它的两个子节点是否都不小于父节点的元素。如果不满足，我们可以通过递归的交换，使得父节点保存最小值\cite{CLRS}。注意：这里我们假设$i$的两棵子树都是合法的堆。

下面的算法从给定的数组索引开始，迭代检查所有的子节点以保持最小堆性质。

\begin{algorithmic}[1]
\Function{Heapify}{$A, i$}
  \State $n \gets |A|$
  \Loop
    \State $l \gets$ \Call{Left}{$i$}
    \State $r \gets$ \Call{Right}{$i$}
    \State $smallest \gets i$
    \If{$l < n \land A[l] < A[i]$}
      \State $smallest \gets l$
    \EndIf
    \If{$r < n \land A[r] < A[smallest]$}
      \State $smallest \gets r$
    \EndIf
    \If{$smallest \neq i$}
      \State \textproc{Exchange} $A[i] \leftrightarrow A[smallest]$
      \State $i \gets smallest$
    \Else
      \State \Return
    \EndIf
  \EndLoop
\EndFunction
\end{algorithmic}

算法接受一个数组$A$和一个索引$i$，$A[i]$的两个子节点都不应比它小。否则，我选出最小的元素保存在$A[i]$，并将较大的元素交换至子树，然后算法自顶向下检查并修复堆的性质直到叶子节点或者没有发现任何违反堆性质的情况。

\textproc{Heapify}的时间复杂度为$O(\lg n)$，其中$n$是元素的总数。这是因为上述算法中的循环次数和完全二叉树的高度成正比。

在具体的实现中，元素之间的比较运算可以用参数的形式传入，这样同一实现就即可以支持最小堆，也支持最大堆。下面的C例子程序实现了这一算法。

\begin{lstlisting}
typedef int (*Less)(Key, Key);
int less(Key x, Key y) { return x < y; }
int notless(Key x, Key y) { return !less(x, y); }

void heapify(Key* a, int i, int n, Less lt) {
    int l, r, m;
    while (1) {
        l = LEFT(i);
        r = RIGHT(i);
        m = i;
        if (l < n && lt(a[l], a[i]))
            m = l;
        if (r < n && lt(a[r], a[m]))
            m = r;
        if (m != i) {
            swap(a, i, m);
            i = m;
        } else
            break;
    }
}
\end{lstlisting}

图\ref{fig:heapify}描述了\textproc{Heapify}从索引2开始，以最大堆处理数组$\{16, 4, 10, 14, 7, 9, 3, 2, 8, 1\}$过程中的各个步骤。数组最终变换为$\{16, 14, 10, 8, 7, 9, 3, 2, 4, 1\}$。

\begin{figure}[htbp]
    \centering
    \subfloat[步骤1：4、14和7中的最大元素是14。将4和左侧子节点交换；]{\includegraphics[scale=0.5]{img/heapify-1.ps}} \hspace{0.01\textwidth}
    \subfloat[步骤2：2、4和8中的最大元素是8。将4和右侧子节点交换；]{\includegraphics[scale=0.5]{img/heapify-2.ps}} \\
    \subfloat[4为叶子节点。过程结束。]{\includegraphics[scale=0.5]{img/heapify-3.ps}}
    \caption{Heapify的例子，堆为最大堆} \label{fig:heapify}
\end{figure}


% ================================================================
%                 Build a heap
% ================================================================
\subsection{构造堆}
\index{二叉堆!构造堆}

使用\textproc{Heapify}算法，我们可以很方便地从任意数组构造堆。观察完全二叉树各层的节点数：

$1, 2, 4, 8, ..., 2^i, ...$.

唯一例外是最后一层，由于树并不一定是满的（完全二叉树不等同于满），最后一层最多含有$2^{p-1}$个节点，其中$2^p \leq n$，$n$是数组的长度。

\textproc{Heapify}算法对于叶子节点不起任何作用，这是由于所有的叶子节点都已经满足堆性质了。我们可以跳过所有的叶子节点，从第一个分支节点开始执行\textproc{Heapify}。显然第一个分支节点的索引不大于$\lfloor n/2 \rfloor$。

根据这一分析，我们可以设计出如下的堆构造算法（以最小堆为例）：

\begin{algorithmic}[1]
\Function{Build-Heap}{$A$}
  \State $n \gets |A|$
  \For{$i \gets \lfloor n/2 \rfloor$ down to $1$}
    \State \Call{Heapify}{$A, i$}
  \EndFor
\EndFunction
\end{algorithmic}

虽然\textproc{Heapify}算法的复杂度为$O(\lg n)$，但是\textproc{Build-Heap}的复杂度不是$O(n \lg n)$，而是线性时间$O(n)$的。我们跳过了所有的叶子节点，最多有$1/4$一半的节点被比较并向下移动一次；最多有$1/8$的节点被比较并向下移动两次；最多有$1/16$的节点被比较并向下移动三次……总共比较和移动次数的上限为：

\be
S = n (\frac{1}{4} + 2 \frac{1}{8} + 3 \frac{1}{16} + ...)
\label{eq:build-heap-1}
\ee

将两侧都乘以2：

\be
2S = n (\frac{1}{2} + 2 \frac{1}{4} + 3 \frac{1}{8} + ...)
\label{eq:build-heap-2}
\ee

用式（\ref{eq:build-heap-2}）减去式（\ref{eq:build-heap-1}），我们有：

\[
S = n (\frac{1}{2} + \frac{1}{4} + \frac{1}{8} + ...) = n
\]

下面的C语言例子程序实现了堆构造算法：

\lstset{language=C}
\begin{lstlisting}
void build_heap(Key* a, int n, Less lt) {
    int i;
    for (i = (n-1) >> 1; i >= 0; --i)
        heapify(a, i, n, lt);
}
\end{lstlisting}

图\ref{fig:build-heap-1}、\ref{fig:build-heap-2}和\ref{fig:build-heap-3}描述了从数组$\{4, 1, 3, 2, 16, 9, 10, 14, 8, 7\}$构造一个最大堆的各个步骤。黑色节点表示执行\textproc{Heapify}时开始的节点；灰色节点表示为了维持堆性质进行交换的节点。

\begin{figure}[htbp]
    \centering
    \subfloat[开始构造前的一个无序数组。]{\includegraphics[scale=0.5]{img/build-heap-array.ps}} \\
    \subfloat[第一步：数组被映射为二叉树。第一个分支节点的值是16。]{\includegraphics[scale=0.5]{img/build-heap-1.ps}} \\
    \subfloat[第二步：16是当前子树中的最大元素，接下来检查元素2所在的节点。]{\includegraphics[scale=0.5]{img/build-heap-2.ps}}
    \caption{从任意数组构造堆。灰色表示每步中进行交换的节点，黑色表示下一步需要检查的节点。} \label{fig:build-heap-1}
\end{figure}

\begin{figure}[htbp]
    \centering
    \subfloat[第三步：14是子树中的最大元素，交换2和14；接下来检查元素为3的节点。]{\includegraphics[scale=0.5]{img/build-heap-3.ps}} \\
    \subfloat[第四步：10是子树中的最大元素，交换10和3；接下来检查元素为1的节点。]{\includegraphics[scale=0.5]{img/build-heap-4.ps}}
    \caption{从任意数组构造堆。灰色表示每步中进行交换的节点，黑色表示下一步需要检查的节点。} \label{fig:build-heap-2}
\end{figure}

\begin{figure}[htbp]
    \centering
    \subfloat[第五步：16是子树中的最大元素，先交换16和1，接下来交换1和7；此后检查元素为4的根节点。]{\includegraphics[scale=0.5]{img/build-heap-5.ps}} \\
    \subfloat[第六步：交换4和14，然后交换4和8；堆构造结束。]{\includegraphics[scale=0.5]{img/build-heap-6.ps}}
    \caption{从任意数组构造堆。灰色表示每步中进行交换的节点，黑色表示下一步需要检查的节点。} \label{fig:build-heap-3}
\end{figure}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{堆的基本操作}

堆的通用定义要求我们提供一些基本操作使得用户可以获取或者改变数据。

最重要的操作包括获取顶部元素（查找最小或最大元素），弹出顶部元素，寻找最小（或最大）的前$k$个元素，减小某一元素的值（此操作对应最小堆，最大堆的相应操作是增加某一元素的值），以及插入新元素。

对于用完全二叉树实现的堆，大部份操作的复杂度在最差情况下都是$O(\lg n)$的。有些操作，例如获取顶部元素，仅仅需要常数时间（$O(1)$）。

\subsubsection{获取顶部元素}
\index{二叉堆!top}

在用二叉树实现的堆中，根节点保存了最小（或最大）元素。根节点对应数组的第一个值。

\begin{algorithmic}[1]
\Function{Top}{$A$}
  \State \Return $A[1]$
\EndFunction
\end{algorithmic}

这一简单操作是常数时间$O(1)$的。我们这里省略了对于空堆的错误处理。如果堆为空，我们可以返回一个错误。

\subsubsection{弹出堆顶元素}
\index{二叉堆!pop}

弹出操作比获取顶部元素要复杂一些。我们需要在移除顶部元素后，通过执行\textproc{Heapify}算法检查并恢复堆的性质。我们可以简单地给出下面的实现，但是它的性能并不好。

\begin{algorithmic}[1]
\Function{Pop-Slow}{$A$}
  \State $x \gets$ \Call{Top}{$A$}
  \State \Call{Remove}{$A$, 1}
  \If{$A$ is not empty}
    \State \Call{Heapify}{$A$, 1}
  \EndIf
  \State \Return $x$
\EndFunction
\end{algorithmic}

这一算法首先用$x$记录下顶部元素，然后将数组中的第一个元素删除，数组的长度减一。如果此后数组不为空，就从新的第一个元素开始执行一次\textproc{Heapify}。

从长度为$n$的数组中删除第一个元素需要线性时间$O(n)$。这是因为我们需要将所有剩余的元素依次向前移动一位。这一操作成为了整个算法的瓶颈，使得算法的复杂度升高了。

为了解决这一问题，我们可以交换数组中的第一个和最后一个元素，然后将数组的长度减一。

\begin{algorithmic}[1]
\Function{Pop}{$A$}
  \State $x \gets$ \Call{Top}{$A$}
  \State $n \gets$ \Call{Heap-Size}{$A$}
  \State \textproc{Exchange} $A[1] \leftrightarrow A[n]$
  \State \Call{Remove}{$A, n$}
  \If{$A$ is not empty}
    \State \Call{Heapify}{$A$, 1}
  \EndIf
  \State \Return $x$
\EndFunction
\end{algorithmic}

从数组的末尾删除最后一个元素仅需要常数时间$O(1)$，而\textproc{Heapify}算法的时间是$O(\lg n)$的。这样整体上弹出操作算法的复杂度为对数时间$O(\lg n)$。下面的C例子程序实现了这一算法\footnote{此程序并未删除最后一个元素，而是复用数组的最后一个cell来存储弹出的结果。}。

\lstset{language=C}
\begin{lstlisting}
Key pop(Key* a, int n, Less lt) {
    swap(a, 0, --n);
    heapify(a, 0, n, lt);
    return a[n];
}
\end{lstlisting}

\subsubsection{寻找top $k$个元素}
\index{二叉堆!top-k}

使用pop，可以很方便地找出一组值中的前$k$大个（或前$k$小）。我们可以构建一个最大堆，然后重复执行$k$次pop操作。

\begin{algorithmic}[1]
\Function{Top-k}{$A, k$}
  \State $R \gets \Phi$
  \State \Call{Build-Heap}{$A$}
  \For{$i \gets 1$ to \textproc{Min}(k, |$A$|)}
    \State \textproc{Append}($R$, \Call{Pop}{$A$})
  \EndFor
  \State \Return $R$
\EndFunction
\end{algorithmic}

如果$k$超过了数组的长度，我们返回整个数组作为结果。因此上述实现中，我们使用最小值\textproc{Min}函数来决定循环的次数。

下面的Python例子程序实现了top-$k$算法：

\lstset{language=Python}
\begin{lstlisting}
def top_k(x, k, less_p = MIN_HEAP):
    build_heap(x, less_p)
    return [heap_pop(x, less_p) for _ in range(min(k, len(x)))]
\end{lstlisting}

\subsubsection{减小key值}
\index{二叉堆!decrease key}

堆可以用来实现带有优先级的队列。因此我们需要提供方法来更改堆中的key值。例如，我们在实际应用中，会增加一个任务的优先级来使得它能尽早执行。

这里我们给出在最小堆中减小key的结果，最大堆的相应操作为增加其中的key。图\ref{fig:decrease-key-1}和\ref{fig:decrease-key-2}描述了将最大堆中第9个节点从4增加到15的步骤。

\begin{figure}[htbp]
    \centering
    \subfloat[第9个节点的值为4。]{\includegraphics[scale=0.5]{img/decrease-key-a.ps}} \\
    \subfloat[将4增加到15，大于其父节点的值。]{\includegraphics[scale=0.5]{img/decrease-key-b.ps}} \\
    \subfloat[根据最大堆的性质，交换8和15。]{\includegraphics[scale=0.5]{img/decrease-key-c.ps}}
    \caption{增大最大堆中某个值的例子过程} \label{fig:decrease-key-1}
\end{figure}

\begin{figure}[htbp]
    \centering
    \subfloat[因为15大于父节点的值14，它们进行交换。此后因为15小于16，处理过程结束。]{\includegraphics[scale=0.5]{img/decrease-key-d.ps}}
    \caption{增大最大堆中某个值的例子过程} \label{fig:decrease-key-2}
\end{figure}

当最小堆中的某个值减小时，可能会违反堆的性质，新的key可能比它的祖先小。我们可以定义如下算法来恢复堆的性质。

\begin{algorithmic}[1]
\Function{Heap-Fix}{$A, i$}
  \While{$i>1 \land A[i] < A[$ \Call{Parent}{$i$} $]$}
    \State \textproc{Exchange} $A[i] \leftrightarrow A[$ \Call{Parent}{$i$} $]$
    \State $i \gets$  \Call{Parent}{$i$}
  \EndWhile
\EndFunction
\end{algorithmic}

这一算法不断比较当前节点和父节点的值，如果父节点较小，就进行交换。算法自底向上进行检查，直到到达根节点，或者发现父节点的值较小。

使用这一辅助算法，我们可以实现减小最小堆中key的操作。

\begin{algorithmic}[1]
\Function{Decrease-Key}{$A, i, k$}
  \If{$k < A[i]$}
    \State $A[i] \gets k$
    \State \Call{Heap-Fix}{$A, i$}
  \EndIf
\EndFunction
\end{algorithmic}

这一算法仅仅在新key比此前的值小时才有效。算法的性能是$O(\lg n)$的。下面的C例子程序实现了此算法。

\lstset{language=C}
\begin{lstlisting}
void heap_fix(Key* a, int i, Less lt) {
    while (i > 0 && lt(a[i], a[PARENT(i)])) {
        swap(a, i, PARENT(i));
        i = PARENT(i);
    }
}

void decrease_key(Key* a, int i, Key k, Less lt) {
    if (lt(k, a[i])) {
        a[i] = k;
        heap_fix(a, i, lt);
    }
}
\end{lstlisting}

\subsubsection{插入}
\index{二叉堆!插入}
\index{二叉堆!heap push}

In some materials like \cite{CLRS}, insertion is implemented by using \textproc{Decrease-Key}.
A new node with $\infty$ as key is created. According
to the min-heap property, it should be the last element
in the under ground array. After that, the key is decreased to
the value to be inserted, and \textproc{Decrease-Key} is called
to fix any violation to the heap property.

Alternatively, we can reuse \textproc{Heap-Fix} to implement
insertion. The new key is directly appended at the end of the array,
and the \textproc{Heap-Fix} is applied to this new node.

\begin{algorithmic}[1]
\Function{Heap-Push}{$A, k$}
  \State \Call{Append}{$A, k$}
  \State \Call{Heap-Fix}{$A, |A|$}
\EndFunction
\end{algorithmic}

The following example Python program implements the heap insertion
algorithm.

\lstset{language=Python}
\begin{lstlisting}
def heap_insert(x, key, less_p = MIN_HEAP):
    i = len(x)
    x.append(key)
    heap_fix(x, i, less_p)
\end{lstlisting}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort}
\label{heap-sort}
\index{Heap sort}

Heap sort is interesting application of heap. According
to the heap property, the min(max) element can be easily accessed
by from the top of the heap. A straightforward way to sort a list
of values is to build a heap from them, then continuously
pop the smallest element till the heap is empty.

The algorithm based on this idea can be defined like below.

\begin{algorithmic}[1]
\Function{Heap-Sort}{$A$}
  \State $R \gets \Phi$
  \State \Call{Build-Heap}{$A$}
  \While{$A \neq \Phi$}
    \State \textproc{Append}($R$, \Call{Heap-Pop}{$A$})
  \EndWhile
  \State \Return $R$
\EndFunction
\end{algorithmic}

The following Python example program implements this definition.

\lstset{language=Python}
\begin{lstlisting}
def heap_sort(x, less_p = MIN_HEAP):
    res = []
    build_heap(x, less_p)
    while x!=[]:
        res.append(heap_pop(x, less_p))
    return res
\end{lstlisting}

When sort $n$ elements, the \textproc{Build-Heap} is bound to $O(n)$.
Since pop is $O(\lg n)$, and it
is called $n$ times, so the overall sorting takes $O(n \lg n)$
time to run. Because we use another list to hold the result,
the space requirement is $O(n)$.

Robert. W. Floyd found a fast implementation of heap sort.
The idea is to build a max-heap instead of min-heap, so the first
element is the biggest one. Then the biggest element is swapped
with the last element in the array, so that it is in the right
position after sorting. As the last element becomes the new top,
it may violate the heap property. We can shrink the heap size
by one and perform
\textproc{Heapify} to resume the heap property.
This process
is repeated till there is only one element left in the heap.

\begin{algorithmic}[1]
\Function{Heap-Sort}{$A$}
  \State \Call{Build-Max-Heap}{$A$}
  \While{$|A| > 1$}
    \State \textproc{Exchange} $A[1] \leftrightarrow A[n]$
    \State $|A| \gets |A| - 1$
    \State \Call{Heapify}{$A, 1$}
  \EndWhile
\EndFunction
\end{algorithmic}

This is in-place algorithm, it needn't any extra spaces to hold
the result. The following. The following ANSI C example code
implements this algorithm.

\lstset{language=C}
\begin{lstlisting}
void heap_sort(Key* a, int n) {
    build_heap(a, n, notless);
    while(n > 1) {
        swap(a, 0, --n);
        heapify(a, 0, n, notless);
    }
}
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item Somebody considers one alternative to realize in-place heap sort. Take
sorting the array in ascending order as example, the first step is to build
the array as a minimum heap $A$, but not the maximum heap like the Floyd's method.
After that the first element $a_1$ is in the correct place. Next, treat
the rest $\{a_2, a_3, ..., a_n\}$ as a new heap, and perform
\textproc{Heapify} to them from $a_2$ for these $n-1$ elements. Repeating this
advance and \textproc{Heapify} step from left to right would sort the array.
The following example ANSI C code illustrates this idea.
Is this solution correct? If yes, prove it; if not, why?
\lstset{language=C}
\begin{lstlisting}
void heap_sort(Key* a, int n) {
    build_heap(a, n, less);
    while(--n)
        heapify(++a, 0, n, less);
}
\end{lstlisting}

\item Because of the same reason, can we perform \textproc{Heapify} from
left to right $k$ times to realize in-place top-$k$ algorithm like below
ANSI C code?
\lstset{language=C}
\begin{lstlisting}
int tops(int k, Key* a, int n, Less lt) {
    build_heap(a, n, lt);
    for (k = MIN(k, n) - 1; k; --k)
        heapify(++a, 0, --n, lt);
    return k;
}
\end{lstlisting}
\end{itemize}
\end{Exercise}

% ================================================================
%                 Explicit binary heap
% ================================================================
\section{Leftist heap and Skew heap, the explicit binary heaps}
\label{ebheap}

Instead of using implicit binary tree by array, it is natural to
consider why we can't use explicit binary tree to realize heap?

There are some problems must be solved if we turn into explicit
binary tree as the under ground data structure.

The first problem is about the \textproc{Heap-Pop} or \textproc{Delete-Min} operation.
Consider the binary tree is represented in form of left, key, and right as
$(L, k, R)$, which is shown in figure \ref{fig:lvr}

\begin{figure}[htbp]
       \begin{center}
       	  \includegraphics[scale=0.8]{img/lvr.ps}
        \caption{A binary tree, all elements in children are smaller than $k$.} \label{fig:lvr}
       \end{center}
\end{figure}

If $k$ is the top element, all elements in left and right children are less
than $k$. After $k$ is popped, only left and right children are left.
They have to be merged to a new tree. Since heap property should be maintained
after merge, the new root is still the smallest element.

Because both left and right children are binary trees conforming heap property,
the two trivial cases can be defined immediately.

\[
merge(H_1, H_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  H_2 & H_1 = \Phi \\
  H_1 & H_2 = \Phi \\
  ? & otherwise
  \end{array}
\right.
\]

Where $\Phi$ means empty heap.

If neither left nor right child is empty, because they all fit
heap property, the top elements of them are all the minimum respectively.
We can compare these two roots,
and select the smaller as the new root of the merged heap.

For instance, let $L = (A, x, B)$ and $R = (A', y, B')$, where $A$, $A'$, $B$,
and $B'$ are all sub trees. If $x < y$, $x$ will be the new root.
We can either keep $A$, and recursively merge $B$ and $R$; or
keep $B$, and merge $A$ and $R$, so the new heap can be one
of the following.

\begin{itemize}
\item $(merge(A, R), x, B)$
\item $(A, x, merge(B, R))$
\end{itemize}

Both are correct. One simplified solution is to only merge the right
sub tree. {\em Leftist} tree provides a systematically approach based on this
idea.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}
\index{Leftist heap}

The heap implemented by Leftist tree is called Leftist heap. Leftist
tree is first introduced by C. A. Crane in 1972\cite{wiki-leftist-tree}.

\subsubsection{Rank (S-value)}
\index{Leftist heap!rank}
\index{Leftist heap!S-value}

In Leftist tree, a rank value (or $S$ value) is defined for each node.
Rank is the distance to the nearest external node. Where external node
is the NIL concept extended from the leaf node.

For example, in figure \ref{fig:rank}, the rank of NIL
is defined 0, consider the root node 4, The nearest leaf node is
the child of node 8. So the rank of root node 4 is 2. Because node
6 and node 8 both only contain NIL, so their rank values are 1.
Although node 5 has non-NIL left child, However, since the right
child is NIL, so the rank value, which is the minimum distance
to leaf is still 1.

\begin{figure}[htbp]
   \begin{center}
     \includegraphics[scale=0.5]{img/rank.ps}
     \caption{$rank(4) = 2$, $rank(6) = rank(8) = rank(5) = 1$.} \label{fig:rank}
   \end{center}
\end{figure}

\subsubsection{Leftist property}

With rank defined, we can create a strategy when merging.

\begin{itemize}
\item Every time when merging, we always merge to right child; Denote the rank
of the new right sub tree as $r_r$;
\item Compare the ranks of the left and right children, if the rank of
left sub tree is $r_l$ and $r_l < r_r$, we swap the left and the right children.
\end{itemize}

We call this `Leftist property'. In general, a Leftist tree always
has the shortest path to some external node on the right.

Leftist tree tends to be very unbalanced, However, it ensures important
property as specified in the following theorem.

\begin{theorem}
If a Leftist tree $T$ contains $n$ internal nodes, the path from root to the
rightmost external node contains at most $\lfloor \log (n+1) \rfloor$ nodes.
\end{theorem}

We skip the proof here, readers can refer to \cite{brono-book} and \cite{TAOCP}
for more information. With this theorem, algorithms operate along this path are
all bound to $O(\lg n)$.

We can reuse the binary tree definition, and augment with a rank field to
define the Leftist tree. For example in form of $(r, k, L, R)$ for non-empty
case. The below Haskell defines Leftist tree.

\lstset{language=Haskell}
\begin{lstlisting}
data LHeap a = E -- Empty
             | Node Int a (LHeap a) (LHeap a) -- rank, element, left, right
\end{lstlisting}

For empty tree, the rank is defined as zero. Otherwise, it's the value
of the augmented field. A $rank(H)$ function can be
given to cover both cases.

\be
rank(H) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & H = \Phi \\
  r & otherwise, H = (r, k, L, R)
  \end{array}
\right.
\ee

Here is the example Haskell rank function.

\lstset{language=Haskell}
\begin{lstlisting}
rank E = 0
rank (Node r _ _ _) = r
\end{lstlisting}

In the rest of this section, we denote $rank(H)$ as $r_H$

% ================================================================
%                 Merge
% ================================================================
\subsection{Merge}
\index{Leftist heap!merge}

In order to realize `merge', we need develop the auxiliary algorithm
to compare the ranks and swap the children if necessary.

\be
mk(k, A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (r_A + 1, k, B, A) & r_A < r_B \\
  (r_B + 1, k, A, B) & otherwise
  \end{array}
\right.
\ee

This function takes three arguments, a key and two sub trees $A$, and $B$.
if the rank of $A$ is smaller, it builds a bigger tree with $B$ as the left child,
and $A$ as the right child. It increment the rank of $A$ by 1 as the
rank of the new tree; Otherwise if $B$ holds the smaller rank, then $A$ is
set as the left child, and $B$ becomes the right. The resulting rank
is $r_b + 1$.

The reason why rank need be increased by one is because there
is a new key added on top of the tree. It causes the rank
increasing.

Denote the key, the left and right children for $H_1$ and $H_2$ as
$k_1, L_1, R_1$, and $k_2, L_2, R_2$ respectively.
The $merge(H_1, H_2)$ function can be completed by using this auxiliary
tool as below

\be
merge(H_1, H_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  H_2 & H_1 = \Phi \\
  H_1 & H_2 = \Phi \\
  mk(k_1, L_1, merge(R_1, H_2)) & k_1 < k_2 \\
  mk(k_2, L_2, merge(H_1, R_2)) & otherwise
  \end{array}
\right.
\ee

The $merge$ function is always recursively called on the right side,
and the Leftist property is maintained. These facts ensure the performance
being bound to $O(\lg n)$.

The following Haskell example code implements the merge program.

\lstset{language=Haskell}
\begin{lstlisting}
merge E h = h
merge h E = h
merge h1@(Node _ x l r) h2@(Node _ y l' r') =
    if x < y then makeNode x l (merge r h2)
    else makeNode y l' (merge h1 r')

makeNode x a b = if rank a < rank b then Node (rank a + 1) x b a
                 else Node (rank b + 1) x a b
\end{lstlisting}

\subsubsection{Merge operation in implicit binary heap by array}
\index{Binary heap!merge}

Implicit binary heap by array performs very fast in most cases, and
it fits modern computer with cache technology well. However, merge
is the algorithm bounds to $O(n)$ time. The typical realization is to
concatenate two arrays together and make a heap for this array \cite{NIST}.

\begin{algorithmic}[1]
\Function{Merge-Heap}{$A, B$}
  \State $C \gets$ \Call{Concat}{$A, B$}
  \State \Call{Build-Heap}{$C$}
\EndFunction
\end{algorithmic}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

Most of the basic heap operations can be implemented with $merge$
algorithm defined above.

\subsubsection{Top and pop}
\index{Leftist heap!top}
\index{Leftist heap!pop}
Because the smallest element is always held in root, it's trivial
to find the minimum value. It's constant $O(1)$ operation. Below
equation extracts the root from non-empty heap $H = (r, k, L, R)$.
The error handling for empty case is skipped here.

\be
top(H) = k
\ee

For pop operation, firstly, the top element is removed, then
left and right children are merged to a new heap.

\be
pop(H) = merge(L, R)
\ee

Because it calls $merge$ directly, the pop operation on Leftist heap is bound
to $O(\lg n)$.

\subsubsection{Insertion}
\index{Leftist heap!insertion}

To insert a new element, one solution is to create a single
leaf node with the element, and then merge this leaf node to
the existing Leftist tree.

\be
insert(H, k) = merge(H, (1, k, \Phi, \Phi))
\ee

It is $O(\lg n)$ algorithm since insertion also calls $merge$ directly.

There is a convenient way to build the Leftist heap from
a list. We can continuously insert the elements one by one
to the empty heap. This can be realized by folding.

\be
build(L) = fold(insert, \Phi, L)
\ee

Figure \ref{fig:leftist-tree} shows one example Leftist tree
built in this way.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/leftist-tree.ps}
    \caption{A Leftist tree built from list $\{9, 4, 16, 7, 10, 2, 14, 3, 8, 1\}$.}
    \label{fig:leftist-tree}
   \end{center}
\end{figure}

The following example Haskell code gives reference implementation
for the Leftist tree operations.

\lstset{language=Haskell}
\begin{lstlisting}
insert h x = merge (Node 1 x E E) h

findMin (Node _ x _ _) = x

deleteMin (Node _ _ l r) = merge l r

fromList = foldl insert E
\end{lstlisting}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort by Leftist Heap}
\index{Leftist heap!heap sort}

With all the basic operations defined, it's straightforward to
implement heap sort. We can firstly turn the list into a Leftist
heap, then continuously extract the minimum
element from it.

\be
sort(L) = heapSort(build(L))
\ee

\be
heapSort(H) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & H = \Phi \\
  \{top(H)\} \cup heapSort(pop(H)) & otherwise
  \end{array}
\right.
\ee

Because pop is logarithm operation, and it is recursively called $n$ times,
this algorithm takes $O(n \lg n)$ time in total. The following Haskell
example program implements heap sort with Leftist tree.

\lstset{language=Haskell}
\begin{lstlisting}
heapSort = hsort . fromList where
    hsort E = []
    hsort h = (findMin h):(hsort $ deleteMin h)
\end{lstlisting} %$



% ================================================================
%                 Skew Heap
% ================================================================

\subsection{Skew heaps}
\label{skew-heap}
\index{Skew heap}

Leftist heap performs poor in some cases. Figure \ref{fig:unbalanced-leftist-tree}
shows one example. The Leftist tree is built by folding on
list $\{16, 14, 10, 8, 7, 9, 3, 2, 4, 1\}$.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.3]{img/unbalanced-leftist-tree.ps}
    \caption{A very unbalanced Leftist tree build from list $\{16, 14, 10, 8, 7, 9, 3, 2, 4, 1\}$.}
    \label{fig:unbalanced-leftist-tree}
   \end{center}
\end{figure}

The binary tree almost turns to a linked-list. The worst case
happens when feed the ordered list to build Leftist tree. Because the
tree downgrades to linked-list, the performance drops from $O(\lg n)$
to $O(n)$.

Skew heap (or {\em self-adjusting heap}) simplifies Leftist heap realization
and can solve the performance issue\cite{wiki-skew-heap} \cite{self-adjusting-heaps}.

When construct the Leftist heap, we swap the left and right children during merge
if the rank on left side is less than the right side. This comparison-and-swap strategy
doesn't work when either sub tree has only one child. Because
in such case, the rank of the sub tree is always 1 no matter how
big it is. A `Brute-force' approach is to swap the left and right children
every time when merge. This idea leads to Skew heap.

\subsubsection{Definition of Skew heap}

Skew heap is the heap realized with Skew tree. Skew tree is a special
binary tree. The minimum element is stored in root. Every sub tree is
also a skew tree.

It needn't keep the rank (or $S$-value) field. We can reuse the
binary tree definition for Skew heap. The tree is either empty,
or in a pre-order form $(k, L, R)$. Below Haskell code defines
Skew heap like this.

\lstset{language=Haskell}
\begin{lstlisting}
data SHeap a = E -- Empty
             | Node a (SHeap a) (SHeap a) -- element, left, right
\end{lstlisting}

\subsubsection{Merge}
\index{Skew heap!merge}
\index{Skew heap!insertion}
\index{Skew heap!top}
\index{Skew heap!pop}

The merge algorithm tends to be very simple.
When merge two non-empty Skew
trees, we compare the roots, and pick the smaller
one as the new root, then the other tree contains the bigger
element is merged onto one sub tree, finally,
the tow children are swapped. Denote $H_1 = (k_1, L_1, R_1)$
and $H_2 =(k_2, L_2, R_2)$ if they are not empty.
if $k_1 < k_2$ for instance, select $k_1$ as the new root. We can
either merge $H_2$ to $L_1$, or merge $H_2$ to $R_1$.
Without loss of generality, let's merge to $R_1$.
And after swapping the two children, the final result
is $(k_1, merge(R_1, H_2), L_1)$. Take account of
edge cases, the merge algorithm is defined as the
following.

\be
merge(H_1, H_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  H_1 & H_2 = \Phi \\
  H_2 & H_1 = \Phi \\
  (k_1, merge(R_1, H_2), L_1) & k_1 < k_2 \\
  (k_2, merge(H_1, R_2), L_2) & otherwise
  \end{array}
\right.
\ee

All the rest operations, including insert, top and pop are all
realized as same as the Leftist heap by using merge, except that
we needn't the rank any more.

Translating the above algorithm into Haskell yields the following
example program.

\lstset{language=Haskell}
\begin{lstlisting}
merge E h = h
merge h E = h
merge h1@(Node x l r) h2@(Node y l' r') =
    if x < y then Node x (merge r h2) l
    else Node y (merge h1 r') l'

insert h x = merge (Node x E E) h

findMin (Node x _ _) = x

deleteMin (Node _ l r) = merge l r
\end{lstlisting}

Different from the Leftist heap, if we feed ordered list to Skew heap, it can build a
fairly balanced binary tree as illustrated in figure \ref{fig:skew-tree}.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/skew-tree.ps}
    \caption{Skew tree is still balanced even the input is an ordered list $\{1, 2, ..., 10\}$.}
    \label{fig:skew-tree}
   \end{center}
\end{figure}


% ================================================================
%                 Splay Heap
% ================================================================

\section{Splay heap}
\label{splayheap}
\index{Splay heap}

The Leftist heap and Skew heap show the fact that it's quite possible to realize
heap data structure with explicit binary tree.
Skew heap gives one method to solve the tree balance problem. Splay heap
on the other hand, use another method to keep the tree balanced.

The binary trees used in Leftist heap and Skew heap
are not Binary Search tree (BST). If we turn the underground
data structure to binary search tree, the minimum(or maximum)
element is not root any more. It takes $O(\lg n)$ time
to find the minimum(or maximum) element.

Binary search tree becomes inefficient if it isn't well
balanced. Most operations degrade to $O(n)$ in the worst case.
Although red-black tree can be used to realize
binary heap, it's overkill. Splay tree provides a light weight
implementation with acceptable dynamic balancing result.


% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

Splay tree uses cache-like approach. It keeps rotating the current
access node close to the top, so that the node can be accessed fast
next time. It defines such kinds of operation as ``Splay''. For the
unbalanced binary search tree, after several splay operations, the
tree tends to be more and more balanced. Most basic operations of
Splay tree perform in amortized $O(\lg n)$ time. Splay tree was invented
by Daniel Dominic Sleator and Robert Endre Tarjan in 1985\cite{wiki-splay-tree}
\cite{self-adjusting-trees}.

\subsubsection{Splaying}
\index{Splay heap!splaying}

There are two methods to do splaying. The first one need deal
with many different cases, but can be implemented fairly easy with
pattern matching. The second one has a uniformed form, but the implementation
is complex.

Denote the node currently being accessed as $X$, the parent node as $P$,
and the grand parent node as $G$ (If there are).  There are 3 steps for
splaying. Each step contains 2 symmetric cases. For illustration
purpose, only one case is shown for each step.

\begin{itemize}
\item {\em Zig-zig step.} As shown in figure \ref{fig:zig-zig}, in this case,
$X$ and $P$ are children on the same side, either both on the left or on the right. By
rotating 2 times, $X$ becomes the new root.

\begin{figure}[htbp]
  \centering
  \subfloat[$X$ and $P$ are either left or right children.]{\includegraphics[scale=0.4]{img/zig-zig-a.ps}}
  \subfloat[$X$ becomes new root after rotating 2 times.]{\includegraphics[scale=0.4]{img/zig-zig-b.ps}}
  \caption{Zig-zig case.} \label{fig:zig-zig}
\end{figure}

\item {\em Zig-zag step.} As shown in figure \ref{fig:zig-zag}, in this
case, $X$ and $P$ are children on different sides. $X$ is on the left,
$P$ is on the right. Or $X$ is on the right, $P$ is on the left.
After rotation, $X$ becomes the new root, $P$ and $G$ are siblings.

\begin{figure}[htbp]
  \centering
  \subfloat[$X$ and $P$ are children on different sides.]{\includegraphics[scale=0.4]{img/zig-zag-a.ps}}
  \subfloat[$X$ becomes new root. $P$ and $G$ are siblings.]{\includegraphics[scale=0.4]{img/zig-zag-b.ps}}
  \caption{Zig-zag case.} \label{fig:zig-zag}
\end{figure}

\item {\em Zig step.} As shown in figure \ref{fig:zig}, in this case,
$P$ is the root, we rotate the tree, so that $X$ becomes new root.
This is the last step in splay operation.

\begin{figure}[htbp]
  \centering
  \subfloat[$P$ is the root.]{\includegraphics[scale=0.4]{img/zig-a.ps}}
  \subfloat[Rotate the tree to make $X$ be new root.]{\includegraphics[scale=0.4]{img/zig-b.ps}}
  \caption{Zig case.} \label{fig:zig}
\end{figure}

\end{itemize}

Although there are 6 different cases, they can be handled in the
environments support pattern matching. Denote the binary tree
in form $(L, k, R)$. When access key $Y$ in tree $T$, the splay
operation can be defined as below.

\be
splay(T, X) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (a, X, (b, P, (c, G, d))) & T = (((a, X, b), P, c), G, d), X = Y \\
  (((a, G, b), P, c), X, d) & T= (a, G, (b, P, (c, X, d))), X = Y \\
  ((a, P, b), X, (c, G, d)) & T = (a, P, (b, X, c), G, d), X = Y \\
  ((a, G, b), X, (c, P, d)) & T = (a, G, ((b, X, c), P, d)), X = Y \\
  (a, X, (b, P, c)) & T = ((a, X, b), P, c), X = Y \\
  ((a, P, b), X, c) & T = (a, P, (b, X, c)), X = Y \\
  T &  otherwise
  \end{array}
\right.
\ee

The first two clauses handle the 'zig-zig' cases; the next two
clauses handle the 'zig-zag' cases; the last two clauses handle
the zig cases. The tree aren't changed for all other situations.

The following Haskell program implements this splay function.

\lstset{language=Haskell}
\begin{lstlisting}
data STree a = E -- Empty
             | Node (STree a) a (STree a) -- left, key, right

-- zig-zig
splay t@(Node (Node (Node a x b) p c) g d) y =
    if x == y then Node a x (Node b p (Node c g d)) else t
splay t@(Node a g (Node b p (Node c x d))) y =
    if x == y then Node (Node (Node a g b) p c) x d else t
-- zig-zag
splay t@(Node (Node a p (Node b x c)) g d) y =
    if x == y then Node (Node a p b) x (Node c g d) else t
splay t@(Node a g (Node (Node b x c) p d)) y =
    if x == y then Node (Node a g b) x (Node c p d) else t
-- zig
splay t@(Node (Node a x b) p c) y = if x == y then Node a x (Node b p c) else t
splay t@(Node a p (Node b x c)) y = if x == y then Node (Node a p b) x c else t
-- otherwise
splay t _ = t
\end{lstlisting}

With splay operation defined, every time when insert a new key,
we call the splay function to adjust the tree.
If the tree is empty, the result is a leaf; otherwise we compare this key
with the root, if it is less than the root, we recursively insert it into
the left child, and perform splaying after that; else the key is inserted
into the right child.

\be
insert(T, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x, \Phi) & T = \Phi \\
  splay((insert(L, x), k, R), x) & T = (L, k, R), x < k \\
  splay(L, k, insert(R, x)) & otherwise
  \end{array}
  \right.
\ee

The following Haskell program implements this insertion algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
insert E y = Node E y E
insert (Node l x r) y
    | x > y     = splay (Node (insert l y) x r) y
    | otherwise = splay (Node l x (insert r y)) y
\end{lstlisting}

Figure \ref{fig:splay-result} shows the result of using this function.
It inserts the ordered elements $\{1, 2, ..., 10\}$
one by one to the empty tree. This would build a very poor result
which downgrade to linked-list with normal binary search tree.
The splay method creates more balanced result.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{img/splay-tree.ps}
  \caption{Splaying helps improving the balance.}
  \label{fig:splay-result}
\end{figure}

Okasaki found a simple rule for Splaying \cite{okasaki-book}.
Whenever we follow
two left branches, or two right branches continuously, we rotate
the two nodes.

Based on this rule, splaying can be realized in such a way.
When we access node for a key $x$ (can be during the process of
inserting a node, or looking up a node, or deleting a node), if
we traverse two left branches or two right branches, we
partition the tree in two parts $L$ and $R$, where $L$ contains all
nodes smaller than $x$, and $R$ contains all the rest.
We can then create a new tree (for instance in insertion),
with $x$ as the root, $L$ as the left child, and $R$ being the right child.
The partition process is recursive, because it will splay
its children as well.

\be
partition(T, p) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, \Phi) & T = \Phi \\
  (T, \Phi) & T = (L, k, R) \land R = \Phi \\
  ((L, k, L'), k', A, B) & \begin{array}{l} \\
                             T = (L, k, (L', k', R')) \\
                             k < p, k' < p \\
                             (A, B) = partition(R', p)
                           \end{array} \\
  ((L, k, A), (B, k', R')) & \begin{array}{l} \\
                               T = (L, K, (L', k', R')) \\
                               k < p \leq k' \\
                               (A, B) = partition(L', p) \\ \\
                             \end{array} \\
  (\Phi, T) & T = (L, k, R) \land L = \Phi \\
  (A, (L', k', (R', k, R)) & \begin{array}{l} \\
                               T = ((L', k', R'), k, R) \\
                               p \leq k, p \leq k' \\
                               (A, B) = partition(L', p)
                             \end{array} \\
  ((L', k', A), (B, k, R)) & \begin{array}{l} \\
                               T = ((L', k', R'), k, R) \\
                               k' \leq p \leq k \\
                               (A, B) = partition(R', p)
                             \end{array}
  \end{array}
  \right.
\ee

Function $partition(T, p)$ takes a tree $T$, and a pivot $p$ as arguments.
The first clause is edge case. The partition result for empty is
a pair of empty left and right trees. Otherwise, denote the tree
as $(L, k, R)$. we need compare the pivot $p$ and the root $k$.
If $k < p$, there are two sub-cases. one is trivial case that
$R$ is empty. According to the property of binary search tree,
All elements are less than $p$, so the result pair is $(T, \Phi)$;
For the other case, $R = (L', k', R')$, we need further compare
$k'$ with the pivot $p$. If $k' < p$ is also true, we recursively
partition $R'$ with the pivot, all the elements less than $p$ in
$R'$ is held in tree $A$, and the rest is in tree $B$. The
result pair can be composed with two trees, one is $((L, k, L'), k', A)$;
the other is $B$. If the key of the right sub tree is not less than
the pivot. we recursively partition $L'$ with the pivot to give
the intermediate pair $(A, B)$, the final pair trees can be
composed with $(L, k, A)$ and $(B, k', R')$. There is a symmetric
cases for $p \leq k$. They are handled in the last three clauses.

Translating the above algorithm into Haskell yields the following partition
program.

\begin{lstlisting}
partition E _ = (E, E)
partition t@(Node l x r) y
    | x < y =
        case r of
          E -> (t, E)
          Node l' x' r' ->
              if x' < y then
                  let (small, big) = partition r' y in
                  (Node (Node l x l') x' small, big)
              else
                  let (small, big) = partition l' y in
                  (Node l x small, Node big x' r')
    | otherwise =
        case l of
          E -> (E, t)
          Node l' x' r' ->
              if y < x' then
                  let (small, big) = partition l' y in
                  (small, Node l' x' (Node r' x r))
              else
                  let (small, big) = partition r' y in
                  (Node l' x' small, Node big x r)
\end{lstlisting}

% ================================================================
%                 Basic heap operations
% ================================================================
\index{Splay heap!insertion}
Alternatively, insertion can be realized with $partition$ algorithm.
When insert a new element $k$ into
the splay heap $T$, we can first partition the heap into two trees, $L$ and $R$. Where
$L$ contains all nodes smaller than $k$, and $R$ contains the rest.
We then construct a new node, with $k$ as the root and $L$, $R$ as the children.

\be
insert(T, k) = (L, k, R), (L, R) = partition(T, k)
\ee

The corresponding Haskell example program is as the following.

\lstset{language=Haskell}
\begin{lstlisting}
insert t x = Node small x big where (small, big) = partition t x
\end{lstlisting}

\subsubsection{Top and pop}
\index{Splay heap!top}
\index{Splay heap!pop}
Since splay tree is just a special binary search tree, the minimum
element is stored in the left most node. We need keep traversing
the left child to realize the top operation. Denote the none empty
tree $T=(L, k, R)$, the $top(T)$ function can be defined as below.

\be
top(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  k & L = \Phi \\
  top(L) & otherwise
  \end{array}
  \right.
\ee

This is exactly the $min(T)$ algorithm for binary search tree.

For pop operation, the algorithm need remove the minimum element from the
tree. Whenever there
are two left nodes traversed, the splaying operation should be performed.

\be
pop(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  R & T = (\Phi, k, R) \\
  (R', k, R) & T = ((\Phi, k', R') k, R) \\
  (pop(L'), k', (R', k, R)) & T = ((L', k', R'), k, R)
  \end{array}
  \right.
\ee

Note that the third clause performs splaying without explicitly call
the $partition$ function. It utilizes the property of binary
search tree directly.

Both the top and pop algorithms are bound to $O(\lg n)$ time because
the splay tree is balanced.

The following Haskell example programs implement the top and pop
operations.

\lstset{language=Haskell}
\begin{lstlisting}
findMin (Node E x _) = x
findMin (Node l x _) = findMin l

deleteMin (Node E x r) = r
deleteMin (Node (Node E x' r') x r) = Node r' x r
deleteMin (Node (Node l' x' r') x r) = Node (deleteMin l') x' (Node r' x r)
\end{lstlisting}

\subsubsection{Merge}
\index{Splay heap!merge}
Merge is another basic operation for heaps as it is widely used in Graph algorithms. By using the $partition$ algorithm, merge can be realized in $O(\lg n)$ time.

When merging two splay trees, for non-trivial case, we can take the root of the first tree as the new root, then partition the second tree with this new root as the pivot. After that we recursively merge
the children of the first tree to the partition result. This algorithm is defined as the following.

\be
merge(T_1, T_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  T_2 & T_1 = \Phi \\
  (merge(L, A), k, merge(R, B)) & T_1 = (L, k, R), (A, B) = partition(T_2, k)
  \end{array}
  \right.
\ee

If the first heap is empty, the result is definitely the second heap. Otherwise,
denote the first splay heap as $(L, k, R)$, we partition $T_2$ with $k$ as the
pivot to yield $(A, B)$, where $A$ contains all the elements in $T_2$ which are
less than $k$, and $B$ holds the rest. We next recursively merge $A$ with $L$;
and merge $B$ with $R$ as the new children for $T_1$.

Translating the definition to Haskell gives the following example program.

\lstset{language=Haskell}
\begin{lstlisting}
merge E t = t
merge (Node l x r) t = Node (merge l l') x (merge r r')
    where (l', r') = partition t x
\end{lstlisting}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort}

Since the internal implementation of the Splay heap is completely
transparent to the heap interface, the heap sort algorithm can
be reused. It means that the heap sort algorithm is generic no
matter what the underground data structure is.

% ================================================================
%                 Short summary
% ================================================================
\section{Notes and short summary}

In this chapter, we define binary heap more general
so that as long as the heap property is maintained, all binary
representation of data structures can be used to implement binary heap.

This definition doesn't limit to the popular array based binary
heap, but also extends to the explicit binary heaps including Leftist
heap, Skew heap and Splay heap. The array based binary heap
is particularly convenient for the imperative implementation
because it intensely uses random index access which can be mapped to
a completely binary tree. It's hard to find directly functional
counterpart in this way.

However, by using explicit binary tree, functional implementation
can be achieved, most of them have $O(\lg n)$ worst case
performance, and some of them even reach $O(1)$ amortize time.
Okasaki in \cite{okasaki-book} shows detailed analysis of these data
structures.

In this chapter, only purely functional realization for Leftist heap,
Skew heap, and Splay heap are explained, they can all be realized
in imperative approaches.

It's very natural to extend the concept from binary tree to
$k$-ary ($k$-way) tree, which leads to other useful heaps such as
Binomial heap, Fibonacci heap and pairing heap. They are introduced
in the next chapter.

% ================================================================
%                 Exercise
% ================================================================
\begin{Exercise}
\begin{itemize}
\item Realize the imperative Leftist heap, Skew heap, and Splay heap.
\end{itemize}
\end{Exercise}


\begin{thebibliography}{99}

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. ``Introduction to Algorithms, Second Edition''. The MIT Press, 2001. ISBN: 0262032937.

\bibitem{wiki-heap}
Heap (data structure), Wikipedia. http://en.wikipedia.org/wiki/Heap\_(data\_structure)

\bibitem{wiki-heapsort}
Heapsort, Wikipedia. http://en.wikipedia.org/wiki/Heapsort

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{rosetta-heapsort}
Sorting algorithms/Heapsort. Rosetta Code. http://rosettacode.org/wiki/Sorting\_algorithms/Heapsort

\bibitem{wiki-leftist-tree}
Leftist Tree, Wikipedia. http://en.wikipedia.org/wiki/Leftist\_tree

\bibitem{brono-book}
Bruno R. Preiss. Data Structures and Algorithms with Object-Oriented Design Patterns in Java. http://www.brpreiss.com/books/opus5/index.html

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming. Volume 3: Sorting and Searching.''. Addison-Wesley Professional;
2nd Edition (October 15, 1998). ISBN-13: 978-0201485417. Section 5.2.3 and 6.2.3

\bibitem{wiki-skew-heap}
Skew heap, Wikipedia. http://en.wikipedia.org/wiki/Skew\_heap

\bibitem{self-adjusting-heaps}
Sleator, Daniel Dominic; Jarjan, Robert Endre. ``Self-adjusting heaps'' SIAM Journal on Computing 15(1):52-69. doi:10.1137/0215004 ISSN 00975397 (1986)

\bibitem{wiki-splay-tree}
Splay tree, Wikipedia. http://en.wikipedia.org/wiki/Splay\_tree

\bibitem{self-adjusting-trees}
Sleator, Daniel D.; Tarjan, Robert E. (1985), ``Self-Adjusting Binary Search Trees'', Journal of the ACM 32(3):652 - 686, doi: 10.1145/3828.3835

\bibitem{NIST}
NIST, ``binary heap''. http://xw2k.nist.gov/dads//HTML/binaryheap.html

\end{thebibliography}

\ifx\wholebook\relax \else
\end{document}
\fi
